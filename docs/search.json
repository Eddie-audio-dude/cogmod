[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Cognitive Modeling",
    "section": "",
    "text": "Preface\nThis is a collection of notes and materials for an introductory course in computational cognitive modeling. It will be updated and, arguably, improved over the course of the semester.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Some functions of models\nUnderstanding the mind is ultimately about understanding a dynamic system comprised of a large set of elements that interact in complex ways. If we had designed the mind, things would be easier: Like a car, a computer, or some other piece of modern machinery, we would have built different parts for different functions and stuck them together in a way that would make it easy for us to repair and reconfigure the system. In reality, things are not so simple. Because the mind is not human-engineered (or really “engineered” at all!), for us mere humans to understand it requires us to reverse-engineer the mind. In other words, we have to see what the mind does in some particular situation, build a particular type of machine—a model—that we hypothesize operates like the mind does, and then put the machine in that same scenario and see whether it acts like a human mind. To the extent that our model acts like a human in that scenario, that gives us reason to believe that the model is operating in some sense “like” a human mind does in that scenario.\nIn the early days of scientific psychology, our tools for building models of complex dynamical systems were quite limited. We could build physical models, but these are constrained by the physical properties of the materials we use (nonetheless, it is worth noting that mechanical model of attention by Broadbent (1957) is designed in such a way that the physical properties of the model capture important psychological constraints). Purely mathematical models may not have been so strongly constrained by physicality, but when considering situations with many interacting elements or when stochastic noise is involved, mathematical models quickly become intractable (consider the insoluble “three body problem”—even with a complete model of such a system, we cannot derive its future behavior analytically). As a result, early psychology was dominated by behaviorism, which eschewed the development of theories of the mind and contented itself merely with observing and cataloging behavior.\nIt was not until the middle of the twentieth century, when modern computers began to become of use, that the possibility of “reverse-engineering the mind” became a reality. This was the time of the “cognitive revolution” (Neisser, 1967). The revolution came about for both technical and conceptual reasons.\nFrom a conceptual perspective, computers offered a productive metaphor for helping us understand how the mind works. A computer uses the same physical substrate to perform different functions, similar to how the same brain lets us both speak and play piano. A computer’s adaptivity comes from the fact that the computer can run different “programs” on its hardware. A program is a set of procedures that take a set of “inputs” and transform them into “outputs”. This is analogous to how a living organism decides to act in a certain way (its “outputs”) depending on its goals and on the environment it happens to be in (its “inputs”). Meanwhile, the procedures that transform a computer’s inputs into outputs often involve intermediate steps that do not themselves produce observable behavior but which are nonetheless represented by changes in the internal state of the computer. These internal states are analogous to thoughts or beliefs in that they may not be externally observable, but they are critical steps on the path toward taking an action. The computer metaphor thus enables us to understand cognition in terms of how internal states of mind represent aspects of an organism’s environment, goals, and thoughts in such a way that these representations can be processed to yield behavior that is appropriate to the situation the organism is in.\nFrom a technical perspective, computers offer a way to derive predictions from complex models that would not have been tractable otherwise. As we shall see, this is particularly valuable for two applications: First, we can use the computer to simulate what a model would do and thereby understand the distribution of possible actions it can take. This obviates the need to derive predictions through mathematical analysis or logic, which though powerful, can only be applied to simple models. Second, we can use the computer to fit a model to data. Almost all models have parameters which can be thought of as “knobs” or “settings” that adjust the kind of behavior the model produces. To “fit” a model means to find the parameters for that model that get it to generate behavior that is as close as possible to the behavior recorded in a dataset. Except for very simple models, it is impossible (or at least very impractical) to try to fit them to data without a computer. But as we shall see, fitting a model is useful because we can infer from the best-fitting parameters something about the person who produced the data to which the model was fit.\nIn summary, computers made it feasible for cognitive psychologists to “reverse-engineer” the mind because they (a) provided a valuable conceptual metaphor that allowed theories of cognition to be posed in the form of computational models comprised of internal representations and processes applied to those representations; (b) enabled predictions to be derived for models that were complex and/or had stochastic elements; and (c) enabled those same kinds of models to be “fit” to data so that model parameters can give insight into how a person performed the task for which data was recorded.\nThere is an important difference between “reverse-engineering” a natural system, like the mind, from reverse-engineering a human-designed system like a car. Because a natural system was not “engineered”, the models we devise are not guaranteed to work the same way as a natural system, even if the model accurately mimicks the behavior of the natural system in the cases we study. The purpose in “reverse-engineering” the mind is not to build a duplicate mind, it is instead to “translate” a complex system into a form that enables us to understand it better. The model is a deliberate simplification which we expect to deviate from reality in many ways. What we hope is that we arrive at a model that helps us understand the key features of a natural system well enough for us to understand why it acts the way it does in specific situations (for further discussion of the purposes of models in psychology, see Cox & Shiffrin, 2024; Singmann et al., 2022).\nAs Cox & Shiffrin (2024) describe, a computational cognitive model falls on the “causal” end of the spectrum in the graph shown at the top. They enumerate a couple of goals that such a model might serve:",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#some-functions-of-models",
    "href": "intro.html#some-functions-of-models",
    "title": "1  Introduction",
    "section": "",
    "text": "All models are wrong, but causal models are useful when they capture and approximate the causal processes at work and when they generalize to other settings. When modeling complex human behavior, all the models are at best crude approximations to reality, but nonetheless they can prove useful by describing the processes that are most important for explaining behavior.\nModeling allows precise communication among scientists. When theories and hypotheses are proposed verbally and heuristically, their precise meaning is not generally known even to the proposers, and they are often misinterpreted and misunderstood by others. When theoretical constructs are clearly linked to elements of a model and the internal workings of the model are described with mathematical or computational precision (e.g., by including the code running the model simulation), other scientists can use, generalize, and test the model because its application in at least its original domain is well specified.\nModels make clear the links between assumptions and predictions. Different investigators cannot dispute the predictions of the model that are produced by a given set of parameters (the parameters generally index different quantitative variants of a given class of models).\nThe formal specification of the model clarifies the nature of proposed internal cognitive processes. A poor modeler may fail to demonstrate that linkage. A good modeler will explore the parameter space and show how the parameter values change the predictions and how narrow or broad a range of outcomes can be predicted by changes in the parameter values. A good modeler will also explore alterations in the model, perhaps by deleting some processes or by adding others, or by fixing some parameter values at certain values, thus providing diagnostic information about the qualitative features of the outcome space that are primarily due to a process controlled by particular parameters. A bad modeler might claim a fit to data provides support for an underlying causal theory when in fact the fit is primarily due to some parameter or theory not conceptually related to the claim.\nConstructing a model can act as an “intuition pump” (cf. Dennett, 1980). Many modelers try to infer underlying cognitive processes from complex data sets that involve multiple measures (e.g., accuracy and response time) and many conditions which may be difficult or impossible to summarize adequately. Modelers form hypotheses about the processes involved based on the data and their prior knowledge. It is often the case that intuiting predictions for different combinations of processes is not possible due to the limitations of human cognition. Thus, different combinations of processes are instantiated as formal models, enabling a modeler to observe and test the predictions of their hypotheses. In an iterative model building process, the failures in each iteration indicate the way forward to an appropriate account.\nModeling allows different hypothesis classes to be compared, both because the predictions of each are well specified and because model comparison techniques take into account model complexity. The issue of complexity is itself quite complex.\n\nOne issue is due to statistical noise produced by limited data. A flexible model with many parameters can produce what seems to be a good fit with parameter values that explain the noise in the data rather than the underlying processes. The best model comparison techniques penalize models appropriately for extra complexity. However, models are in most cases developed after observing the data, and they are used to explain the patterns observed. To do so, they often incorporate assumptions that are critical but not explicitly parameterized. It thus becomes a difficult and subtle matter to assess complexity adequately. A secondary problem with using fit to compare models is the fact that the most principled ways to control for complexity, such as Bayesian model selection and minimum description length, are difficult to implement computationally and are often replaced by approximations such as the Akaike or Bayes/Schwartz information criteria that often fail to account for key aspects of model complexity.\nSimpler models are also favored for other reasons. A chief one is limited human reasoning: As a model becomes more complex, it becomes more difficult for a human to understand how it works. Simpler models are also favored when analytic predictions can be derived (thereby greatly reducing computational costs) and for qualitative reasons such as “elegance”.\nSimpler models are particularly favored when their core processes generalize well across many tasks.\nThere exists a danger that the modeler will mistake small quantitative differences in “fit” for the important differences among models—differences that generally lie in qualitative patterns of predictions. Knowing one model provides a better fit to a limited set of observations from a narrow experimental setting is not often useful. For example, consider a model that correctly predicts the relative levels of recall accuracy observed across conditions in a given experiment but quantitatively overpredicts the accuracy in a single condition. Meanwhile, another model perfectly predicts the accuracy in that condition but fails to predict the right qualitative pattern of accuracy across conditions. We argue that the qualitatively correct prediction is a reason to favor the first model, even though it might yield a worse quantitative fit. Correct qualitative predictions across conditions are often a sign that a model captures an important and generalizable causal process.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-importance-of-simulation",
    "href": "intro.html#the-importance-of-simulation",
    "title": "1  Introduction",
    "section": "1.2 The importance of simulation",
    "text": "1.2 The importance of simulation\nThe models that we will be covering are models that simulate behavioral (and maybe even neural) outcomes. Because we are building models of a complex system—namely, the mind—our models can also become complex. Therefore, understanding what kind of behavior a model produces may require us to simulate behavior from that model. This will also help us to understand the relationship between the model’s parameters and its behavior. By simulating how behavior changes as one or more parameters change, we can understand the role played by the theoretical construct represented by that parameter.\nFor example, we might have a parameter that represents the quality or precision with which an event is stored in memory. In a model where this memory representation is used to yield behavior, we can systematically adjust the value of the quality/precision parameter and observe the effect this has on the model’s simulated behavior. Again, because we are dealing with models that can grow quite complex, we may even be surprised by the behavior the model produces!\nTwo analogies may help give some intuition about the value of simulation: If we are cooking, often the only way to know how a particular combination of spices will taste is to actually combine them and taste. Model parameters are like the different amounts of each spice, with the final taste being analogous to the model’s simulated behavior. Alternatively, you can think of model parameters as being like characters in an improv sketch. The characters have different backgrounds and goals which dictate how they will interact and how the story will develop. The background and goals of the characters are like the parameters of a model, with the resulting performance analogous to the model’s predicted behavior.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-importance-of-fitting-data",
    "href": "intro.html#the-importance-of-fitting-data",
    "title": "1  Introduction",
    "section": "1.3 The importance of fitting data",
    "text": "1.3 The importance of fitting data\n“Fitting” a model to data means finding the combination of parameter values for which the model produces behavior that most closely resembles that produced by a participant in some task. The value of doing this is that it helps us understand why that participant acted the way they did.\nFor example, we might want to know whether someone was fast because they were able to quickly accumulate the evidence they needed, because they were uncautious, because they were biased, or because they could execute motor actions quickly. We can address that question by finding the values of the parameters associated with each construct that best fit their observed performance.\n\n\n\n\nBroadbent, D. E. (1957). A mechanical model for human attention and immediate memory. Psychological Review, 64(3), 205–215.\n\n\nCox, G. E., & Shiffrin, R. M. (2024). Computational models of event memory. In M. J. Kahana & A. Wagner (Eds.), Oxford handbook of human memory. Oxford University Press.\n\n\nDennett, D. (1980). The milk of human intentionality. Behavioral and Brain Sciences, 3(3), 428–430. https://doi.org/10.1017/s0140525x0000580x\n\n\nNeisser, U. (1967). Cognitive psychology. Appleton-Century-Crofts.\n\n\nSingmann, H., Kellen, D., Cox, G. E., Chandramouli, S. H., Davis-Stober, C. P., Dunn, J. C., Gronau, Q. F., Kalish, M. L., McMullin, S. D., Navarro, D. J., & Shiffrin, R. M. (2022). Statistics in the service of science: Don’t let the tail wag the dog. Computational Brain & Behavior.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "r_coding.html",
    "href": "r_coding.html",
    "title": "2  Programming with R",
    "section": "",
    "text": "2.1 Tips and strategies",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming with R</span>"
    ]
  },
  {
    "objectID": "r_coding.html#tips-and-strategies",
    "href": "r_coding.html#tips-and-strategies",
    "title": "2  Programming with R",
    "section": "",
    "text": "When trying to write code to accomplish a particular task—or when trying to understand code written by someone else—try to break the task into individual steps that are accomplished in sequence in order to yield the final result.\nIf you are unsure what a particular bit of code will do—for example, if you want to figure out how to code one of the steps you’ve identified above—try to construct a minimal working example. This example should be simple enough that you can figure out what the result should be without doing any code. Then you can try out the code and verify whether the result matches what you expected.\nThe same principles that underlie producing good code also underlie debugging code. This is covered well in the corresponding chapter of Wickham’s Advanced R book, but essentially debugging involves (a) isolating where the problem arises; (b) understanding the conditions that resulted in the problem; (c) revising the code in an attempt to correct the problem or to prevent problematic circumstances from arising; and (d) testing to ensure the solution in fact addresses the problem. Working on small bits of code at a time makes all of the essential steps of debugging easier.\nR supports vectorization of many operations. While vectorization allows for code to run more efficiently, the resulting code can sometimes be harder to understand and debug. As a result, you may want to write a less efficient but easier to read version of your code first, so that you can verify that it works the way you expect. You can then see where you might want to try to make your code more efficient, using your original easy-to-read code to verify that any changes you make don’t alter the expected result.\nUsing named vectors/matrices/arrays can often be quite handy when you want to index values by using a string that describes their meaning or interpretation, rather than a numerical index. Not only can this make your code more interpretable, it avoids issues when you may not know ahead of time which numerical index contains a particular desired value.\nR has a tendency to recycle things in ways you may not expect! For example, when doing any operation involving multiple vectors, if one vector is shorter than the other, R will sometimes “recycle” the elements of the shorter vector to create a new vector that is the same length as the longer one. The rules that govern how R “recycles” are not consistently applied and can be hard to predict, therefore it is important to ensure that your code will not produce this kind of ambiguous situation. You may want to include error checks to ensure that vectors are the same length. Alternatively, if you want to recycle, you can do so explicitly so there is no ambiguity (e.g., x_recycled &lt;- rep(x, length(y))[1:length(y)]).\nAlways remember the drop option when subsetting an array, matrix, or data frame! If the subsetting procedure selects only a single element, unless you use drop = FALSE, the result will be a length-one vector that “throws out” the other dimensions of your data structure. This can result in bugs if your code assumes that the result of the subset will have a consistent number of dimensions.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming with R</span>"
    ]
  },
  {
    "objectID": "r_coding.html#exercise-fibonnacci",
    "href": "r_coding.html#exercise-fibonnacci",
    "title": "2  Programming with R",
    "section": "2.2 Exercise: Fibonnacci",
    "text": "2.2 Exercise: Fibonnacci\nThe following set of R coding exercises are meant to prepare you for the kind of coding that will be involved in writing our first cognitive model simulations. It is not exhaustive of all the things that you can do with R, but it addresses many of the essentials. It also exemplifies the workflow involved in building a model:\n\nImplement the basic processes involved in a simple case where you know what the correct result should be, so you can ensure you have implemented these basics appropriately.\nBuild a function that generalizes the code you wrote in step 1 so that you can apply it to different parameter settings.\nUse your function to simulate different results using different parameter settings.\nExplore the range of results your function produces across parameter settings.\nOptionally, consider ways that you can generalize your model even further by incorporating additional parameters.\n\nThese exercises are based on everyone’s favorite sequence, the Fibonnacci sequence. The sequence is defined by a simple rule: the next value in the sequence is the sum of the previous two values. Written in Math, that’s: \\[\nf[i] = f[i - 2] + f[i - 1]\n\\] where \\(f[\\cdot]\\) is a value in the Fibonnacci sequence and \\(i\\) is the index of the next value. To get this sequence going, we need to know the first two values, \\(f[1]\\) and \\(f[2]\\). Typically, these are both set to 1. As a result, the beginning of the Fibonnacci sequence goes like this: \\[\n1, 1, 2, 3, 5, 8, 13, \\ldots\n\\]\nAnyway, let’s begin!\n\n2.2.1 Exercise 1\nWrite two versions of a chunk of code that will create a vector called fib that contains the first 20 values in the Fibonnacci sequence. Assume that the first two values in the sequence are 1 and 1. Write one version of the code that creates the vector by appending each new value to the end of fib. Write another version that assigns values to the corresponding entries in fib directly using the appropriate index (for this second version, you may want to use the rep function to create the fib vector).\n\n\n2.2.2 Exericse 2\nBased on the code you wrote for the previous exercise, write a function that returns a vector containing the first N terms of the Fibonnacci sequence. Your function should take two arguments, the value N and a vector called init that contains the first two values in the sequence. Give those arguments sensible default values. The chunk below gives a sense of the overall structure your function should have:\n\n\nCode\nfib_func &lt;- function(N = ___, init = ___) {\n    ...\n    return(fib)\n}\n\n\n\n\n2.2.3 Exercise 3\nWrite code that calls the function you wrote in the previous exercise several times, each time using a different value for the second value in the init argument (but the same value for N and for init[1]). Collect the results from each function call in a single data frame or tibble. The data frame or tibble should have a column that stores the second initial value, a column for the vector returned from the function, and a third column that is the value’s index within the sequence. An example of the kind of result you’re looking for is given below:\n\n\n# A tibble: 8 × 3\n  init2   fib     i\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     1     1     1\n2     1     1     2\n3     1     2     3\n4     1     3     4\n5     4     1     1\n6     4     4     2\n7     4     5     3\n8     4     9     4\n\n\n\n\n2.2.4 Exercise 4\nWrite code that uses the ggplot2 library to plot the values of the Fibonnacci sequence on the y-axis against their position in the sequence on the x-axis. Distinguish between different init2 values by using different colored lines. The result should look something like the plot below.\n\n\n\n\n\n\n\n\n\n\n\n2.2.5 Extension exercise\nWrite a new function that takes a third argument, n_back, which specifies how many of the previous values to add up to create the next value in the sequence. For the Fibonnacci sequence, n_back = 2, but in principle we could define other kinds of sequences too. Adapt the code you wrote for your previous exercises to explore what happens with different values of n_back. You may also want to include some code at the beginning of your function that checks to ensure that the number of initial values provided in the init argument is sufficient!",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming with R</span>"
    ]
  },
  {
    "objectID": "random_walk.html",
    "href": "random_walk.html",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "",
    "text": "3.1 Out for a random walk\nYou are in a bind. There are two piles of hay in front of you. They look pretty similar in size. You are hungry. Which of the two piles do you walk toward? I should also mention that you are a donkey, so this is pretty important to you! (The deliberating donkey shown above, Simon, is the resident donkey at Indian Ladder Farms near Albany, NY.)\nThe situation described above is a version of the parable of Buridan’s ass. In the parable, the unfortunate ass is unable to come to a decision because the two piles of hay are equally large, meaning he has no basis for choosing between them. As a result, the donkey starves to death.\nIn this chapter, we will build a model of two-choice behavior that rescues Buridan’s donkey from this dire fate. This model is called a random walk. As we will see, this model can be applied to any situation involving a choice between two options. The random walk model instantiates four core constructs that together comprise a theory of how people and other animals decide between two options. It is also a great example of the general structure of a computational cognitive model, in that it describes how an observable action (a choice) arises by applying a process of accumulation to a representation of the balance of evidence between the two options. As we shall see, it will be fairly straightforward to extend the random walk model into a more general form.\nThe random walk model is designed to account for two aspects of behavior: choice and response time (RT). These and similar models are applied in situations where a person (or other organism!) has to decide between a small number of possible alternatives, often just two. Such situations abound in experimental psychology, including lexical decision, recognition memory, detection tasks, search tasks, categorization tasks, etc. The models are designed to help us understand two things:",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#out-for-a-random-walk",
    "href": "random_walk.html#out-for-a-random-walk",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "",
    "text": "Why did a participant make the choices they made?\nWhy did it take the participant a certain amount of time to make the choices they made?\n\n\n3.1.1 Key theoretical constructs\nThe vast majority of models of choice and RT, including random walk and diffusion models, address the two questions above by invoking four basic theoretical constructs:\nEvidence accumulation: Choosing from among a set of options is assumed to require accumulating “evidence” that weighs either in favor of or against each option. This evidence may come from perception (e.g., deciding which of two stimuli is brighter), from memory (e.g., deciding whether an item was or was not on a previously-studied list), or from deliberation (e.g., deciding which of two products to buy). As such, the idea that decisions are made by accumulating evidence helps explain not only which choice was made (it was the option that was most favored by the accumulated evidence) and how long it took to made the choice (the time needed to accumulate sufficient evidence to commit to a decision).\nResponse caution: If decisions are made by accumulating evidence, there must be a policy that terminates the accumulation process, otherwise someone would keep accumulating evidence forever—and this is exactly what Buridan’s ass needs to avoid! The construct of “response caution” refers to the idea that, depending on the situation, a decision maker may adopt a policy of deciding quickly on the basis of very little evidence (low response caution) or deciding slowly by waiting for more evidence to accumulate (high response caution). Thus, this construct is directly related to the idea of speed-accuracy trade-off.\nResponse bias: It may be that a decision maker is willing to commit to some options more readily than others; in that case, we say they are “biased” in favor of those responses. Typically, this bias is modeled by assuming lower response caution for some options than others. In other words, a participant may be willing to commit to some decisions on the basis of less accumulated evidence than others.\nResidual time: The time needed to accumulate sufficient evidence to make a decision is not the only thing that contributes to observed response times. After all, it takes time to realize that a trial of a task has actually begun. It may also take time to retrieve relevant information from memory, to focus attention on relevant features in the environment, or to evaluate a potential outcome. Finally, it takes some time to execute the motor actions associated with the chosen option (e.g., to press a button, move a lever, etc.). The time for all of these additional processes is often called non-decision time (NDT) or encoding and response execution time (\\(T_{ER}\\)). However, I prefer to call it simply “residual time” because that is what it is—it is the time “left over” besides the time needed for evidence accumulation.\n\n\n3.1.2 Representing the current state of evidence\nThe random walk model assumes that, at any given time, a decision maker represents the current balance of evidence between two options as a number. We will creatively refer to this representation as \\(x(t)\\), where \\(x\\) stands for evidence and \\((t)\\) stands for the fact that it is the evidence at a specific time \\(t\\). The sign and magnitude of \\(x(t)\\) represents the extent to which the current value of evidence favors one option over the other.\nIf \\(x(t)\\) equals zero, then the evidence at time \\(t\\) does not favor either option. This is akin to the situation when Buridan’s ass first encounters the two piles of hay. If \\(x(t) &gt; 0\\), then the evidence favors one of the two options. For Buridan’s ass, perhaps positive values of evidence represent evidence in favor of going toward the pile of hay on the right. If \\(x(t) &lt; 0\\), then the evidence favors the other option. For Buridan’s ass, maybe negative values of evidence represent evidence in favor of going toward the pile of hay on the left. Notice that we could just as easily do it the other way around: positive evidence favors going left while negative evidence favors going right. The important thing is just that the two options are associated with opposite signs of evidence.\nIn a cognitive task, the two choices might be “word” and “non-word” in a lexical decision task, “old” and “new” in a recognition memory task, “present” and “absent” in a visual search task, “same” and “different” in a change detection task, “category A” and “category B” in a categorization task, etc. Again, the point is that, at any given time, the degree to which the decision maker’s accumulated evidence at time \\(t\\) favors one option or the other is represented by the value of a number \\(x(t)\\), with each option associated with opposite signs.\n\n\n3.1.3 Accumulating evidence\nThe value of \\(x(t)\\) represents the evidence that has been accumulated by time \\(t\\). But what does it mean to “accumulate” evidence? And what is the “evidence” that is accumulated?\nIn a random walk model, we assume that at regular time intervals (each interval has duration \\(\\Delta t\\)), the decision maker receives a “sample” of evidence, which we will label \\(\\Delta x(t)\\). This sample can take one of two values, \\(+1\\) or \\(-1\\). If it is \\(+1\\), the sample favors the option associated with positive evidence values (e.g., the pile of hay on the right) and if it is \\(-1\\), the sample favors the option associated with negative evidence values (e.g., the pile of hay on the left). To accumulate evidence means to add the new sample \\(\\Delta x(t)\\) to the current value of the accumulated evidence, i.e.: \\[\n\\overbrace{x(t + \\Delta t)}^{\\text{Updated evidence}} = \\overbrace{x(t)}^{\\text{Current accumulated evidence}} + \\overbrace{\\Delta x(t)}^{\\text{Current sample of evidence}}\n\\] Thus, the accumulated evidence \\(x(t)\\) is the sum of all the samples of evidence that were obtained by time \\(t\\).\n\n\n3.1.4 What is evidence?\nAt this point, it would be reasonable to ask where these samples of evidence come from. There is no single answer to this question because the random walk model, like most of the models of choice and RT we will consider, treats evidence in a very abstract sense. Later in the course, we will encounter models that instantiate specific theories of the “evidence” a decision maker may use in a specific context.\nTo return to Buridan’s ass, the evidence might be perceptual in nature: For an interval of time, the donkey looks at both piles of hay. Even though both piles are, by assumption, equally big, that may not always be visually apparent. During any finite interval of time, one pile might happen to look ever so slightly larger than the other, perhaps due to a quirk of the light, a sheaf fluttering in the breeze, the donkey’s visual acuity, etc. If the pile on the right happened to look a bit bigger than the one on the left during one of those intervals, then the sample of evidence for that interval would be \\(+1\\). Otherwise, it would be \\(-1\\). Because these minute differences are due to essentially chance factors, and they are equally likely to favor either pile, we can say that the probability of getting a sample that is either \\(+1\\) or \\(-1\\) is \\(0.5\\). While the evidence might not favor one pile over the other in the long run, it will favor one option over a finite interval of time, which is all any real decision maker has at their disposal. As we shall see shortly, this is the key to saving Buridan’s ass.\nTreating evidence as due, at least in part, to chance factors is why this model is called a “random” walk. It also highlights the fact that the evidence samples need not occur with equal frequency. Perhaps samples come up \\(+1\\) with probability \\(p\\) and otherwise come up \\(-1\\), like the proverbial biased coin flip. If the evidence consistently favors one option, that means that \\(p\\) is close to either 1 or 0. To the extent that chance factors influence the evidence, \\(p\\) will be closer to \\(0.5\\). We have now been introduced to the first parameter of the random walk model: \\(p\\), the probability of getting a sample of evidence that favors the option associated with positive evidence.\nThe figure below illustrates different ways that evidence might accumulate over time. Each step up or down is driven by the sample of evidence that was obtained at that time, which is assumed to be random with probability \\(p\\). The figure also illustrates why this model is called a random “walk”, because each trajectory kind of looks like a path that someone might have walked.\n\n\nCode\nexpand_grid(p = c(0.2, 0.5, 0.8), sim_index = 1:5, t = 1:20) %&gt;%\n    mutate(x_sample = 2 * rbinom(n = n(), size = 1, prob = p) - 1) %&gt;%\n    group_by(p, sim_index) %&gt;%\n    mutate(x_accum = cumsum(x_sample)) %&gt;%\n    ggplot(aes(x = t, y = x_accum, color = factor(p), group = interaction(p, sim_index))) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_step(alpha = 0.5) +\n    labs(x = \"Time interval\", y = \"Accumulated evidence\", color = \"p\")\n\n\n\n\n\n\n\n\n\nWhat about evidence in cognitive tasks? Buridan’s ass relies on the same kind of sensory evidence as one needs to do, for example, psychophysical tasks like picking which stimulus is brighter, more leftward-oriented, etc. Evidence derived from memory can also be noisy—perhaps when retrieving an event, you sometimes recall the color of an object as blue and sometimes as green. When deciding between different gambles or products, we may also shift attention to different features of those options, leading us to judge them as better or worse depending on which features we attend to (Busemeyer & Townsend, 1993; Diederich, 1997).\n\n\n3.1.5 Doing some code\nHaving now familiarized ourselves with how the random walk model represents a decision maker’s evidence and how it processes that evidence via accumulation, let’s see how we would write that model in code. Specifically, we will be writing code that simulates different possible random walks. The way we will do this is more of an intellectual exercise, since we will not be striving for efficiency (later on, we will use special-purposes R packages for that). Rather, the point here is to see how the conceptual aspects of a model can be implemented in code. We will add on to this code as we go.\nFor now, we know that we will have a line that looks something like the accumulation equation above:\n\n\nCode\nx &lt;- x + x_sample\n\n\nHere, x stands for the value of the accumulated evidence and x_sample stands for the current sample of evidence (which is either 1 or -1). The &lt;- evaluates the expression on the right and assigns it to the thing on the left, so the code above says “take the current value of x, add the new sample x_sample, and put it back as the new value of x”.\nWith the code above as the core of the model, we now need to do three things: first, specify how to get x_sample; second, obtain many such samples; third, keep a record of how the accumulated evidence changes over time.\n\n3.1.5.1 Sampling evidence\nTo get a value for x_sample, we will use R’s rbinom function, which generates a random sample from a binomial distribution. Specifically, the line rbinom(n = 1, size = 1, prob = 0.5) will generate a sample that equals 1 with probability 0.5, otherwise it equals zero. It is perhaps easiest to think of it in terms of a coin flip: The n parameter of the rbinom function says how many sets of coins to flip, size says how many coins we flip in each set, and prob is the probability that any single flip comes up heads. The number that rbinom gives is the number of heads in a particular set of flips.\nFor Buridan’s ass, the sample of evidence favors each pile equally often, so prob = 0.5 makes sense. Note that because rbinom returns either a 0 or a 1, we need to do some math to turn the result into \\(+1\\) or \\(-1\\). This is shown below.\n\n\nCode\nx_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\nx &lt;- x + x_sample\n\n\n\n\n3.1.5.2 Obtaining many samples\nThere are a few ways we can write code that will obtain many samples. To anticipate what we will be doing later, we will use the while control structure. We can use it to specify a condition such that, so long as the condition is met, a block of code will continue to be executed in a loop.\nOur condition will depend on the current time. Remember that, in the random walk, each sample of evidence arrives at fixed intervals of time. We will therefore need to keep track of the current time as well as the accumulated evidence. Similar to how we updated the evidence, we will need to keep track of t, the current time. We will also need to specify dt, the duration of each interval (otherwise known as \\(\\Delta t\\)), and t_max, the amount of time to keep accumulating evidence.\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n}\n\n\nNotice that we specified values for t_max and dt outside the while loop. We can specify initial values for x and t the same way:\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nx &lt;- 0\nt &lt;- 0\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n}\n\n\n\n\n3.1.5.3 Keeping a record\nThe chunk of code above will work just fine! But unfortunately it does not leave a record of accumulated evidence over time that we can then examine, like we did with the graph above. In the chunk below, we use a fun trick to keep a record of each value of x and t: We create two vectors x_record and t_record and use the c function to concatenate the current values of x and t to these vectors:\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nx &lt;- 0\nt &lt;- 0\n\nx_record &lt;- x\nt_record &lt;- t\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n    x_record &lt;- c(x_record, x)\n    t_record &lt;- c(t_record, t)\n}\n\n\n\n\n3.1.5.4 Visualizing the record\nNow that we are keeping a record of evidence over time, let’s visualize it! The code below uses base R for that purpose, although the graph above uses ggplot2 which we will use again later. The type = \"s\" setting in the plot function at the end give the “step-like” plot.\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nx &lt;- 0\nt &lt;- 0\n\nx_record &lt;- x\nt_record &lt;- t\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n    x_record &lt;- c(x_record, x)\n    t_record &lt;- c(t_record, t)\n}\n\nplot(t_record, x_record, type = \"s\", xlab = \"Time\", ylab = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nTry copy-pasting the code above and running it yourself a few times to see what it looks like!\n\n\n3.1.5.5 Making a function\nIf we have a chunk of code that we want to re-run many times, we would do better to write a function that we can call instead of having to re-run the whole chunk. Writing a function also makes it easier to deal with parameters that can have different settings, like dt and t_max. We will also make the probability \\(p\\) a parameter too. Finally, the values for these three parameters in the function line are defaults.\n\n\nCode\nrw_sim &lt;- function(p = 0.5, dt = 0.05, t_max = 5) {\n    x &lt;- 0\n    t &lt;- 0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (t &lt; t_max) {\n        x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = p) - 1\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(tibble(t = t_record, x = x_record))\n}\n\n\nNow we can call the function rw_sim with different settings to simulate different random walks. Note that, because the function returns t_record and x_record as different columns of a tibble, we can easily use ggplot2 to plot the results, as in the examples below.\n\n\nCode\nsim_result1 &lt;- rw_sim(p = 0.5, dt = 0.05, t_max = 5)\n\nsim_result1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.5\")\n\n\n\n\n\n\n\n\n\nCode\nsim_result2 &lt;- rw_sim(p = 0.2, dt = 0.05, t_max = 5)\n\nsim_result2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.2\")\n\n\n\n\n\n\n\n\n\nCode\nsim_result3 &lt;- rw_sim(p = 0.8, dt = 0.05, t_max = 5)\n\nsim_result3 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.8\")\n\n\n\n\n\n\n\n\n\nGo ahead, try it out yourself with different values of p, dt, and/or t_max. It’s fun! And if you don’t think the step graphs are too interesting, just imagine that each of those steps is Simon the donkey trying to decide between his two piles of hay.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#making-a-decision",
    "href": "random_walk.html#making-a-decision",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "3.2 Making a decision",
    "text": "3.2 Making a decision\nSo far, we have built a simple model of evidence accumulation. In this model, samples of “evidence” arrive at regular intervals, with the sample supporting either one option (\\(+1\\)) or the other (\\(-1\\)) with probability \\(p\\), and the decision maker accumulates these samples by summation. The resulting accumulated evidence thus starts at zero and takes a “random walk” that can drift upward (if \\(p &gt; 0.5\\)), downward (if \\(p &lt; 0.5\\)), or in no particular direction (if \\(p = 0.5\\)).\n\n3.2.1 Response boundaries\nWhat we have not done is say how the decision maker uses this accumulated evidence to decide between their two options. According to the random walk model, the decision maker sets two values prior to accumulating evidence. These values are called thresholds, criteria, or boundaries (these terms are often used interchangeably). There is one positive boundary and one negative boundary. If and when the accumulated evidence crosses one of these boundaries, the decision maker selects the corresponding option.\nFor example, say that Buridan’s ass will pick the pile on the right if his accumulated evidence ever gets greater than \\(+5\\) and he will pick the pile on the left if his accumulated evidence ever gets less than \\(-5\\). We can visualize this situation by overlaying lines at those two boundaries on the “random walk” of accumulating evidence:\n\n\nCode\nburidan_sim1 &lt;- rw_sim(p = 0.5)\n\nburidan_sim1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nHere’s another one:\n\n\nCode\nburidan_sim2 &lt;- rw_sim(p = 0.5)\n\nburidan_sim2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nGo ahead and try it out yourself!\nThe point is that we can read from these graphs which option the donkey ends up picking by seeing which boundary gets crossed first. We can also see when the donkey makes his decision based on how long it took for that first boundary-crossing to occur. This is how the random walk model ultimately produces both a choice (which boundary was crossed first) and an RT (how long it took). It is also why the random walk saves Buridan’s ass: Even if the evidence in the long run does not favor either option, by chance the accumulated evidence will at some point cross one of the boundaries, enabling the donkey to make a decision.\n\n\n3.2.2 Response bias\nIn the examples above, Buridan’s ass set his response boundaries to be of equal distance from the initial evidence value of zero. Burdian’s ass might be more willing to go to the leftward pile than the rightward one—maybe it is more aesthetically appealing or the donkey has a limp that makes it easier for him to walk left than right. This would amount to a bias in favor of one option (going left) over the other (going right).\nWe can instantiate this bias in the random walk model via the donkey’s response boundaries. For example, the donkey may go to the left if the accumulated evidence ever gets less than \\(-4\\) but would only be willing to go to the right if the accumulated evidence ever gets greater than \\(+6\\). The following two graphs illustrate these biased response boundaries.\n\n\nCode\nburidan_bias_sim1 &lt;- rw_sim(p = 0.5)\n\nburidan_bias_sim1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"Response bias, simulation 1\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nburidan_bias_sim2 &lt;- rw_sim(p = 0.5)\n\nburidan_bias_sim2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"Response bias, simulation 2\")\n\n\n\n\n\n\n\n\n\nIntuitively, it seems reasonable to expect that, if one boundary is closer to the start than the other, that two things will happen: First, the option associated with the closer boundary will be picked more often (at least if the evidence against that option is not too strong). Second, the decision maker will tend to be faster to pick the option associated with the closer boundary. We will verify these intuitions later, but for now you can rest assured that these intuitions are correct.\n\n\n3.2.3 Revising our function\nNow that we have gotten acquainted with the notion of response boundaries and how they can be biased, let’s incorporate them into our random walk simulation function from earlier. This will involve two things: First, we will need to add two parameters to the function, one for each boundary. Second, we will need to change the condition in the while loop so that the random walk stops when it reaches a boundary. As a corrollary to this second step, we will keep the t_max condition but adjust the default value of t_max.\nThe revised function is shown below, with some additional explanation following:\n\n\nCode\nrw_sim &lt;- function(p = 0.5, b_upper = 5, b_lower = -5, dt = 0.05, t_max = Inf) {\n    x &lt;- 0\n    t &lt;- 0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = p) - 1\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(tibble(t = t_record, x = x_record))\n}\n\n\nThe key changes we made to the rw_sim function are:\n\nAdding parameters b_upper and b_lower for the upper and lower response boundaries, respectively.\nChanging the default value of t_max to Inf for “infinity”. This means that, by default, reaching a boundary is the only way the random walk will stop. However, by leaving t_max as a parameter, it means that we can set it to some real number like 5 or 10 to force the random walk to stop eventually.\nChanging the condition in the while loop. Now the walk will continue so long as the evidence x is below the upper boundary (x &lt; b_upper) and above the lower boundary (x &gt; b_lower) and so long as the maximum time hasn’t been reached (t &lt; t_max). Note that the & is a “logical and” operator.\n\nHere are a few simulation runs—try it out yourself!\n\n\nCode\nboundary_sim_result1 &lt;- rw_sim(p = 0.5, b_upper = 5, b_lower = -5)\n\nboundary_sim_result1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.5, b_upper = 5, b_lower = -5\")\n\n\n\n\n\n\n\n\n\nCode\nboundary_sim_result2 &lt;- rw_sim(p = 0.5, b_upper = 6, b_lower = -4)\n\nboundary_sim_result2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.5, b_upper = 6, b_lower = -4\")\n\n\n\n\n\n\n\n\n\nCode\nboundary_sim_result3 &lt;- rw_sim(p = 0.7, b_upper = 6, b_lower = -4)\n\nboundary_sim_result3 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.7, b_upper = 6, b_lower = -4\")",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#residual-time",
    "href": "random_walk.html#residual-time",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "3.3 Residual time",
    "text": "3.3 Residual time\nWe are nearly done with our simulation model! We can model accumulating evidence and making a decision. The final ingredient arises from the fact that, while a decision maker might select one option at a particular time, we can only observe the behavioral consequences of that decision. Those behavioral consequences might be hitting a key, clicking a button, pressing a lever, or walking toward a pile of hay. Executing that behavior takes time in addition to the time needed to accumulate evidence and reach a response boundary. That additional time goes by many names, often “non-decision time” (NDT) or “encoding and responding” time (\\(T_{ER}\\)), but I prefer to simply call it residual time.\nFor now, we will adopt a simple assumption that this residual time is constant. Therefore, the observed response time will be the sum of the time needed for the random walk to reach a boundary plus the residual time associated with all the other processes that are involved in taking an action but which our model doesn’t explicitly enumerate.\nTo make this concrete, let’s introduce a parameter called t0 that will stand for residual time. While I cannot speak to what a plausible value of t0 would be for Buridan’s ass, in many cognitive tasks, it tends to be around 0.2 or 0.3 seconds, to account for the time needed to execute a simple motor action like hitting a button.\n\n\nCode\nrw_sim &lt;- function(p = 0.5, b_upper = 5, b_lower = -5, t0 = 0.2, dt = 0.05, t_max = Inf) {\n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = p) - 1\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(tibble(t = t_record, x = x_record))\n}\n\n\nNote that the main change to our rw_sim function is that the initial value for the time t is no longer 0 but t0, i.e., the value of the residual time parameter.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#simulating-many-trials",
    "href": "random_walk.html#simulating-many-trials",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "3.4 Simulating many trials",
    "text": "3.4 Simulating many trials\nOur rw_sim function can now simulate single realizations of a random walk decision process. As we have seen, though, each realization of this process is different because the samples of evidence are random. If we want to get a sense of the kind of behavior the model tends to produce, we need to simulate many realizations of the decision and examine the distribution of choices and RT’s produced by the model. This is the same reason why, in a typical cognitive task, we collect multiple trials from each participant in each condition. With a real participant, we are limited by the time and energy that a participant is willing to commit. With a model, we are still limited by time and energy, but they are our time and the computer’s energy. Nonetheless, it is worth keeping in mind that all the techniques below for visualizing choices and RT’s apply to observed data as well as they apply to simulated data.\n\n3.4.1 Running and saving many simulation results\nWe will need to write some code that repeatedly calls our rw_sim function a large number of times and saves the results so we can examine them later. What follows is not necessarily the most efficient way of accomplishing those goals, but it is conceptually transparent and introduces the for loop. The comments (following the # marks) explain what is going on with the line below.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- rw_sim(p = 0.5, b_upper = 5, b_lower = -5, t0 = 0.2, dt = 0.05)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Get a quick sense of what the results look like\nglimpse(sim_results)\n\n\nRows: 25,884\nColumns: 3\n$ t         &lt;dbl&gt; 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, …\n$ x         &lt;dbl&gt; 0, -1, -2, -3, -2, -3, -4, -3, -2, -3, -4, -3, -4, -3, -2, -…\n$ sim_index &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, …\n\n\n\n\n3.4.2 Visualizing the random walks\nWhat we are about to do may be a bit silly but helps build some intuitions about what is going on in the model. We are going to make a plot that overlays all 1000 simulated random walks on top of each other. The point is to get a sense of how much variability there is from one realization to the next.\n\n\nCode\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x, group = sim_index)) +\n    geom_step(alpha = 0.1) +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nOkay, maybe it is a bit silly after all. But it is possible to see that things “thin out” at longer times as more and more random walks end by hitting a boundary. If you check out the code that generates the plot, note how group = sim_index was used to make sure each individual simulation, indexed by sim_index, got its own step-line on the graph. Also note the use of alpha = 0.1 to make each line semi-transparent so they could be overlayed on one another.\nLet’s try a different approach to visualize the same thing, using a heatmap that indicates the relative frequency with which the accumulated evidence takes different values at different times:\n\n\nCode\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\")\n\n\n\n\n\n\n\n\n\nAgain, what is important to see above is that all the random walks start at the same time and evidence value (the yellow region) and then “fan out” over time.\n\n\n3.4.3 Joint distributions of choice and RT\nWhat we visualized in the previous section are the internal states of the model, that is, how the model represents the decision maker’s current balance of evidence between their two options. Remember, though, that the model is ultimately judged on its externally-observable behavior, since that is all we have to compare it against. We are finally going to visualize the choices and response times produced by the model. As we shall see, however, there are a few ways to do this!\n\n3.4.3.1 Extracting choices and RT’s\nFor each simulation, the RT is the final value of t, since that is the time (plus residual time) at which the first boundary was crossed. Meanwhile, the choice is whether the evidence x is positive or negative. The chunk of code below takes our simulation results and extracts the final choices and RT from each simulation.\n\n\nCode\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\nglimpse(choice_rt)\n\n\nRows: 1,000\nColumns: 3\n$ sim_index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ choice    &lt;fct&gt; lower, lower, lower, upper, upper, lower, upper, lower, uppe…\n$ rt        &lt;dbl&gt; 1.05, 2.35, 0.95, 0.75, 0.55, 2.45, 1.15, 0.45, 1.35, 3.25, …\n\n\n\n\n3.4.3.2 Joint frequency plot\nThe code below plots the frequency with which each choice (upper or lower) is made at different times. This kind of plot is not terribly common, but is a quick way to get a sense of both how often each choice is made as well as the shape of the distributions of RT’s.\n\n\nCode\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_freqpoly(binwidth = 0.2) +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\")\n\n\n\n\n\n\n\n\n\n\n\n3.4.3.3 Conditional RT density\nThe code below plots the conditional density of the RT’s for each choice. This kind of plot is much more common, but doesn’t convey any information about the relative frequency with which different choices are made. Nonetheless, it illustrates how the random walk produces distributions of RT’s with a pronounced right skew, similar to RT distributions that are actually observed in choice tasks. Note that the conditional RT distributions for each choice are pretty similar to one another too.\n\n\nCode\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\")\n\n\n\n\n\n\n\n\n\n\n\n3.4.3.4 Quantile-probability plots\nIn the choice-RT modeling world, it is common to make “quantile-probability plots”, sometimes abbreviated to QP plots. These plots can be a bit confusing at first, but are useful because they convey information about choice proportions and RT distributions in a single graph.\nThe horizontal axis of a QP plot corresponds to the probability of having made a particular choice. In this case, that is the proportion of simulations that resulted in each choice. We can get that information in numerical form from our choice_rt tibble:\n\n\nCode\nchoice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\n\n# A tibble: 2 × 3\n  choice     n p_resp\n  &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 upper    507  0.507\n2 lower    493  0.493\n\n\nThe vertical axis of a QP plot corresponds to different quantiles of the conditional RT distributions for each choice. Typically, those quantiles are the RT’s at the 10th, 30th, 50th, 70th, and 90th percentiles of the distribution. The reason for all of these quantiles is that they convey information about different aspects of the distribution: The 50th percentile, otherwise known as the median, conveys the central tendency. The 30th and 70th percentiles indicate where the “bulk” of the RT’s tend to fall. Finally, the 10th and 90th percentiles convey information about the lower and upper tails of the distribution, respectively. We can obtain those quantiles numerically like so:\n\n\nCode\nchoice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\n\n# A tibble: 10 × 2\n   choice  rt_q\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 upper   0.55\n 2 upper   0.75\n 3 upper   1.05\n 4 upper   1.55\n 5 upper   2.65\n 6 lower   0.55\n 7 lower   0.85\n 8 lower   1.25\n 9 lower   1.75\n10 lower   2.75\n\n\nTo make a QP plot, we need to “join” together the response proportions and RT quantiles into the same tibble:\n\n\nCode\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q)\n\n\nJoining with `by = join_by(choice)`\n\n\n# A tibble: 10 × 4\n   choice     n p_resp  rt_q\n   &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 upper    507  0.507  0.55\n 2 upper    507  0.507  0.75\n 3 upper    507  0.507  1.05\n 4 upper    507  0.507  1.55\n 5 upper    507  0.507  2.65\n 6 lower    493  0.493  0.55\n 7 lower    493  0.493  0.85\n 8 lower    493  0.493  1.25\n 9 lower    493  0.493  1.75\n10 lower    493  0.493  2.75\n\n\nThat joined tibble can then be used as the basis for our QP plot:\n\n\nCode\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#putting-it-all-together",
    "href": "random_walk.html#putting-it-all-together",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "3.5 Putting it all together",
    "text": "3.5 Putting it all together\nWe have used R to build a random walk model of decision making, implemented via a function called rw_sim, that accumulates samples of evidence until the accumulated evidence reaches either an upper or lower boundary. This model depends on several parameters, of which the most theoretically important are:\n\np: The probability that any given sample of evidence favors the option associated with the upper response boundary.\nb_upper: The upper response boundary.\nb_lower: The lower response boundary.\nt0: Residual time.\n\nWe also saw different ways that we can visualize both the internal states and external behavior of the model. It may be useful at this point to put together everything we have done so far into a single chunk of code. This will make your own explorations of this model easier.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- rw_sim(p = 0.5, b_upper = 5, b_lower = -5, t0 = 0.2, dt = 0.05)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#exercises",
    "href": "random_walk.html#exercises",
    "title": "3  Building a random walk model to simulate choice and RT",
    "section": "3.6 Exercises",
    "text": "3.6 Exercises\n\nSet the p parameter to something other than 0.5, so that the evidence tends to favor one option over the other. Do one set of simulations in which the response boundaries are equidistant from the starting value of 0 (you may need to play around to find values that you like). Do another set of simulations in which you keep the boundaries equidistant but make them closer to the starting point. What is the effect on the model’s choices and RT’s of having boundaries that are closer to the starting point?\nRun one set of simulations with the p parameter to 0.6 and the response boundaries equidistant from the starting point. Run another set of simulations keeping the response boundaries the same but increasing the p parameter to 0.8. What is the effect of increasing the p parameter on the RT distributions for making the “upper” choice? What is the effect of increasing the p parameter on the RT distributions for making the “lower” choice?\nImagine that, instead of each sample of evidence equalling either \\(+1\\) or \\(-1\\), the evidence could also equal \\(0\\). Write code to simulate this model and use your simulations to see how this model might differ from the random walk model we developed in this chapter.\n\nYou will need to introduce a new parameter to the model that represents the probability of getting a sample that equals zero. What ways can you think of to implement this aspect of the model? Which method did you pick and why?\nHow does the shape of the predicted RT distributions differ, if at all, from that predicted by the original random walk model? (Hint: you may want to explore settings in which there is zero probability of taking a step either up or down. It may also help to visualize the random walks themselves too.)\nWhat cognitive tasks might be better modeled by allowing for evidence to have a value of zero?\n\nTry implementing a model in which the residual time can vary randomly according to some distribution. Since residual time must be non-negative, you might consider distributions like the Gamma distribution or a uniform distribution between two positive values.\n\nHow did you implement random residual times?\nHow does random residual time affect the shape of the predicted RT distributions?\nWhat psychological factors might contribute to variability in residual time?\n\n\n\n\n\n\nBusemeyer, J. R., & Townsend, J. T. (1993). Decision field theory: A dynamic–cognitive approach to decision making in an uncertain environment. Psychological Review, 100(3), 432–459.\n\n\nDiederich, A. (1997). Dynamic stochastic models for decision making under time constraints. Journal of Mathematical Psychology, 41(3), 260–274. https://doi.org/10.1006/jmps.1997.1167",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html",
    "href": "diffusion_sim.html",
    "title": "4  From random walk to diffusion",
    "section": "",
    "text": "4.1 Discrete to continuous evidence\nIn the previous chapter, we built a random walk model of how someone might decide between two options. In this chapter, we turn the random walk model into the more widely applied diffusion model, introduced to psychology by Ratcliff (1978). As we shall see, conceptually, the transition from a random walk to a diffusion model is not very large.\nLike most models of choice and RT, both random walk and diffusion models are premised on the idea that making a decision requires accumulating samples of “evidence” until the accumulated evidence reaches a response boundary. The “evidence” in these models is deliberately abstract because these models are meant to be applied in a variety of situations. The important thing is that “evidence” can be represented in these models as a number, where a sample of evidence supporting one option takes a positive value while a sample supporting the other option takes a negative value. A diffusion model differs from a random walk model in two aspects regarding the nature of the evidence that is accumulated:\nIn other words, the random walk model treats evidence as taking discrete values that are sampled at discrete intervals, whereas a diffusion model treats evidence as taking continuous values that are sampled continuously in time.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#discrete-to-continuous-evidence",
    "href": "diffusion_sim.html#discrete-to-continuous-evidence",
    "title": "4  From random walk to diffusion",
    "section": "",
    "text": "Samples of evidence take continuous values, rather than discrete values.\nSamples of evidence arrive continually, rather than at regular intervals.\n\n\n\n4.1.1 Evidence sampled from a normal distribution\nIn the random walk model, the magnitude of each sample of evidence was always equal to one. Each sample was either \\(+1\\) or \\(-1\\). In a diffusion model, evidence can take any real value, such that its magnitude is now important. Conceptually, this has some intuitive appeal. Some samples of evidence strongly favor one option, some samples only weakly support one option, and some are equivocal.\nIn a diffusion model, samples of evidence are specifically assumed to come from a normal distribution. The standard deviation of this distribution is typically fixed to some value like 0.1 or 1. Here, we will fix it to the value of 1. The reason for fixing this value is that “evidence” is abstract and therefore has no natural scale. We could multiply or divide all the evidence samples by a constant amount without changing their underlying meaning.\nThe mean of the evidence distribution represents how strongly the evidence tends to favor one option over the other, similar in meaning to the \\(p\\) parameter in the random walk model. The mean of the evidence distribution in a diffusion model is termed the drift rate, as it reflects the tendency for accumulated evidence to “drift” either upward or downward over time. As illustrated in the graph below, the mean of the evidence distribution governs the degree to which samples support one option versus the other.\n\n\nCode\nexpand_grid(v = c(-2, -1, 0, 1, 2), x = seq(-4, 4, length.out = 201)) %&gt;%\n    mutate(d = dnorm(x, mean = v, sd = 1)) %&gt;%\n    ggplot(aes(x = x, y = d, color = v, group = v)) +\n    geom_vline(xintercept = 0, linetype = \"dashed\") +\n    geom_line() +\n    scale_color_gradient2(mid = \"#444444\", midpoint = 0) +\n    labs(x = \"Value of evidence sample\", y = \"Relative frequency\", color = \"Mean of evidence\\ndistribution\")\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Evidence sampled continuously in time\nHere we come to a bit of a subtle issue: In the random walk, evidence arrived in discrete units at regular intervals, but the duration of the interval was not related to the magnitude of the evidence. In a diffusion model, we assume that evidence arrives continuously in time. One way to think about this—indeed, the way that we will simulate this—is that evidence is sampled in many very short intervals of time, each of which has duration \\(\\Delta t\\). When \\(\\Delta t\\) is small enough, those many little intervals will look like one continuous span of time. This principle is illustrated in the graph below.\n\n\nCode\ndiffusion_sim &lt;- expand_grid(dt = c(0.1, 0.01, 0.001)) %&gt;%\n    group_by(dt) %&gt;%\n    reframe(t = seq(0, 3, by = dt)) %&gt;%\n    ungroup() %&gt;%\n    mutate(x_sample = rnorm(n = n(), mean = 0, sd = 1 * sqrt(dt))) %&gt;%\n    group_by(dt) %&gt;%\n    mutate(x = cumsum(x_sample))\n\nscaled_plot &lt;- diffusion_sim %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    facet_wrap(\"dt\", labeller = label_bquote(Delta * t == .(dt))) +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = expression(\"Evidence scaled by \" * Delta * t))\n\nunscaled_plot &lt;- diffusion_sim %&gt;%\n    ggplot(aes(x = t, y = x / sqrt(dt))) +\n    geom_step() +\n    facet_wrap(\"dt\", labeller = label_bquote(Delta * t == .(dt))) +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"Evidence not scaled\")\n\nscaled_plot / unscaled_plot\n\n\n\n\n\n\n\n\n\nThe top set of graphs above show how, when \\(\\Delta t\\) is sufficiently small, the trajectory of accumulated evidence looks essentially continuous—you can no longer see the “jumps” from one interval to the next.\nThe bottom set of graphs illustrate the subtlety I mentioned earlier. If we divide time into many small intervals but leave the mean and standard deviation of the evidence distribution the same, then we are essentially getting many more samples of evidence. As a result, accumulated evidence has a much larger scale than it would have if we had picked a smaller \\(\\Delta t\\). From a theoretical standpoint, this doesn’t make sense—the rate at which evidence accumulates for a decision should not be affected by the modeler’s arbitrary choice of \\(\\Delta t\\).\nSo what we do is scale the evidence samples by \\(\\Delta t\\). That’s what was done in the top set of graphs, but not the bottom set. The idea is that if you have very small time intervals, you shouldn’t be able to get as large of a sample of evidence. Again, this makes theoretical sense, if evidence is something that takes time to accumulate.\nSpecifically, a diffusion model assumes that each sample of evidence is drawn from a normal distribution with a mean of \\(v \\times \\Delta t\\), where \\(v\\) is the drift rate parameter, and a standard deviation of \\(\\sqrt{\\Delta t}\\). Why \\(\\sqrt{\\Delta t}\\) instead of just \\(\\Delta t\\)? Because it is the mean and variance that need to be scaled by \\(\\Delta t\\).\n\n\n4.1.3 A new simulation function\nLet’s take our rw_sim function from the previous chapter and turn it into a diffusion model. To do this, we make two modifications: First, we swap out the p parameter representing the probability of getting a positive sample for a parameter called v which is the drift rate. Second, instead of getting each evidence sample x_sample from a binomial distribution, we will get it from a normal distribution using R’s rnorm function. These changes are illustrated below.\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, b_upper = 1, b_lower = -1, t0 = 0.2, dt = 0.01, t_max = Inf) {\n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = v * dt, sd = sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nIn the code above, I also took the liberty of adjusting the default values of b_upper, b_lower, and dt so that the simulated choices and RT’s would look a bit more like those observed in cognitive tasks, but of course you may feel free to adjust those yourself as you like.\n\n\n4.1.4 Putting it all together—again\nAt the end of the last chapter, I included a chunk of code that simulated a random walk and produced some visualizations to help us understand both its internal states and its overt behavior (choices and RT). By swapping out rw_sim with the appropriately adjusted diffusion_sim line, we can apply the same chunk of code to the diffusion model! In the chunk below, I picked some arbitrary but reasonable values for the parameters.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, b_upper = 1, b_lower = -1)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\nYou may or may not be surprised to see that the RT distributions produced by the diffusion model closely resemble those produced by the random walk! The diffusion model also demonstrates an interesting feature of a random walk, namely, that the conditional RT distribution depends on the boundaries but not on the drift rate. In the example above, I set \\(v = 0.5\\), such that evidence would tend to favor the positive option. Even though the model ends up choosing that option more often, it does not do so any faster or slower than it chooses the negative option. This is something we will return to at the end of this chapter.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#response-caution-and-response-bias",
    "href": "diffusion_sim.html#response-caution-and-response-bias",
    "title": "4  From random walk to diffusion",
    "section": "4.2 Response caution and response bias",
    "text": "4.2 Response caution and response bias\nBefore confronting the issue of invariant RT distributions, it behooves us to consider a different way of specifying the response boundaries in our model. So far, we have specified those boundaries directly. We can speak of response caution in terms of how far those boundaries are from the starting point and response bias in terms of whether the boundaries are equidistant from the starting point.\nSpecifically, we could define a term \\(A\\) that is the total distance between the starting point (zero) and the two boundaries. If \\(B_{\\text{Upper}}\\) and \\(B_{\\text{Lower}}\\) are the upper and lower boundaries, respectively, then \\[\nA = B_{\\text{Upper}} - B_{\\text{Lower}}\n\\] In other words, \\(A\\) is how far apart the two boundaries are, called boundary separation. The term \\(A\\) can be seen to operationalize the construct of response caution in that a decision maker who wants to wait to accumulate evidence would put their response boundaries far apart.\nWe can also operationalize the construct of response bias by defining a term \\(w\\). This term will be a number between 0 and 1 that represents the degree to which response boundaries favor one choice over the other. Specifically, let \\[\nw = \\frac{-B_{\\text{Lower}}}{B_{\\text{Upper}} - B_{\\text{Lower}}}\n\\] As shown in the graph below, \\(w = 0.5\\) when the boundaries are equidistant from zero, \\(w &lt; 0.5\\) when the boundaries are biased in favor of the negative option, and \\(w &gt; 0.5\\) when the boundaries are biased in favor of the positive option.\n\n\nCode\nexpand_grid(b_upper = seq(1, 5), b_lower = seq(-1, -5)) %&gt;%\n    mutate(A = b_upper - b_lower) %&gt;%\n    mutate(w = -b_lower / A) %&gt;%\n    pivot_longer(c(A, w), names_to = \"par\", values_to = \"val\") %&gt;%\n    mutate(par = factor(par, levels = c(\"A\", \"w\"), labels = c(\"Response caution (A)\", \"Response bias (w)\"))) %&gt;%\n    ggplot(aes(x = b_upper, y = val, color = b_lower, group = b_lower)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(\"par\", scales = \"free_y\", strip.position = \"left\") +\n    labs(x = expression(B[\"Upper\"]), y = NULL, color = expression(B[\"Lower\"])) +\n    theme(strip.placement = \"outside\", strip.background = element_blank())\n\n\n\n\n\n\n\n\n\nHaving defined \\(A\\) and \\(w\\) as ways of operationalizing response caution and response bias, respectively, why not treat these values as parameters instead of the boundaries themselves? The value in doing so is that we can then specify these constructs directly, rather than having to work backwards from the boundaries. Specifically, if we pick values of \\(A\\) and \\(w\\) we can immediately compute what the upper and lower boundaries should be: \\[\\begin{align}\nB_{\\text{Upper}} & = w A \\\\\nB_{\\text{Lower}} & = -\\left(1 - w \\right) A \\\\\n\\end{align}\\]\nAnd we can adjust our diffusion_sim code accordingly to have a and w as parameters instead of b_upper and b_lower, which now get calculated in the function itself:\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf) {\n    b_upper &lt;- (1 - w) * a\n    b_lower &lt;- -w * a\n    \n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#trial-by-trial-variability",
    "href": "diffusion_sim.html#trial-by-trial-variability",
    "title": "4  From random walk to diffusion",
    "section": "4.3 Trial-by-trial variability",
    "text": "4.3 Trial-by-trial variability\nRecall that both the random walk and the diffusion model have the following property: The response times they produce depend on the distance between the starting point and the response boundary, not on the drift rate \\(v\\) or the step probability \\(p\\). To see why this might be problematic from a psychological perspective, imagine that the upper boundary corresponds to making a correct response while the lower boundary corresponds to making an error. Assume that \\(v &gt; 0\\), such that the evidence tends to favor making a correct response. The fact that response times do not depend on drift rates means that the model predicts that correct and error responses will be made in the same amount of time. To be more precise, the distribution of RT’s conditional on accuracy are the same.\nOften, errors are either faster or slower than correct responses. For example, it may be that errors occur more often when the decision maker happens to get poor evidence. In that case, we might expect errors to be slow because they result from the decision maker deliberating longer in the face of this poor evidence. On the other hand, maybe a decision maker tends to be correct when they take their time, but will sometimes “jump the gun” and pick the wrong option, in which case we would expect errors to be faster than correct responses.\nThe critical factor that Ratcliff (1978) introduced to the diffusion model that has made it into such a useful tool is that the drift rate is not the same on every trial, but varies randomly from trial to trial. On some trials, you happen to get a drift rate in the high positive tail of the distribution of drift rates, in which case you would probably make a fast correct response. On other trials, you happen to get a drift rate that is close to zero or even falls below zero by chance, in which case you would be more likely to make an error and would tend to do so more slowly. Thus, trial-by-trial variability in drift rates accounts for slow errors.\nWhat about fast errors? Ratcliff & Rouder (1998) showed that these can result if your response boundaries are not always fixed, but can also vary randomly from trial to trial. Sometimes, they happen to be very close to the starting point such that it takes very little evidence to commit to a response. Such rapid responses would be more likely to be errors, since they don’t give much time to accumulate evidence. Thus, trial-by-trial variability in boundaries (or, equivalently, in starting point) accounts for fast errors.\nThere is a final thing that can vary from trial to trial, and that is residual time. After all, if the time needed to accumulate evidence can vary between trials, so can the time needed to accomplish all the other processes involved in any given decision task. Trial-by-trial variability in residual time does not, of course, affect the probability of choosing either option, but it does affect the form of the RT distributions.\n\n4.3.1 Adding variability to our simulation code\nTo model each of these kinds of trial-by-trial variability, we need to decide how each of the values above (drift rate, boundaries, and residual time) can vary. This will also inform us as to what new parameters we will need to add to our model to specify that variability. In what follows, we will adopt common assumptions in the literature that are also implemented in the model-fitting functions we will use in later chapters. Check out the exercises (or explore on your own) to consider other forms of trial-by-trial variability!\n\n4.3.1.1 Trial-by-trial variability in drift rates\nOur model already has a parameter called \\(v\\) that stands for the “drift rate”. Let us instead treat \\(v\\) as the mean of a normal distribution of drift rates, which has standard deviation \\(s_v\\). If \\(s_v = 0\\), then we have our original diffusion model with the same drift rate on every trial. On the other hand, if \\(s_v &gt; 0\\), then the drift rate on any given trial will sometimes be greater or less than \\(v\\), even if the average drift rate across all trials is \\(v\\).\nTo implement this in our simulation code, we need to\n\nAdd a new parameter sv.\nAdd a line that randomly samples the drift rate (called trial_v) from a normal distribution with mean v and standard deviation sv.\nReplace v when drawing samples of evidence with trial_v.\n\nThese changes are reflected in the following adjusted code:\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf, sv = 0) {\n    trial_v &lt;- rnorm(n = 1, mean = v, sd = sv)\n    \n    b_upper &lt;- (1 - w) * a\n    b_lower &lt;- -w * a\n    \n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = trial_v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nThe chunk of code below has the same settings as that shown above, only now sv = 1. As you can see, responses on the lower boundary have a different RT distribution, which tends to be slower, than responses on the upper boundary. (Note, too, that I am using our revised code that uses a and w to define the response boundaries.)\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0.5)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\n\n\n4.3.1.2 Trial-by-trial variability in boundaries/starting point\nThere are a number of ways that we could introduce variability in the starting point and/or boundaries. To be consistent with the model-fitting we will do later, we will assume that the bias parameter \\(w\\) is not fixed, but is sampled from a uniform distribution that goes from \\(w - \\frac{s_w}{2}\\) to \\(w + \\frac{s_w}{2}\\). Thus, the average bias is still \\(w\\) but has a range defined by parameter \\(s_w\\). As above, we need to add this new parameter and randomly sample a trial_w value at the top of our code. Note that the line that samples trial_w does some checking using the min and max functions to make sure that \\(w\\) never falls below 0 or greater than 1.\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf, sv = 0, sw = 0) {\n    trial_v &lt;- rnorm(n = 1, mean = v, sd = sv)\n    trial_w &lt;- runif(n = 1, min = max(0, w - 0.5 * sw), max = min(1, w + 0.5 * sw))\n    \n    b_upper &lt;- (1 - trial_w) * a\n    b_lower &lt;- -trial_w * a\n    \n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = trial_v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nThe simulations below set \\(s_v = 0\\) and \\(s_w = 0.9\\), while \\(v = 0.5\\). In the simulations below, when the model picks the “incorrect” option associated with the lower boundary, it is predicted to do so faster than when it responds by choosing the “correct” option associated with the upper boundary.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0, sw = 0.9)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\n\n\n4.3.1.3 Trial-by-trial variability in residual time\nOur final amendment to our diffusion simulation code involves adding variability to the residual time. Again, there are many ways we could do this, but we will adopt the same conventional approach used in our model-fitting later: We will assume that the residual time on any given trial is sampled from a uniform distribution that ranges from \\(t_0\\) to \\(t_0 + s_{t_0}\\). The code below adds the new st0 parameter and samples a residual time trial_t0 from a uniform distribution at the beginning of the simulation:\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf, sv = 0, sw = 0, st0 = 0) {\n    trial_v &lt;- rnorm(n = 1, mean = v, sd = sv)\n    trial_w &lt;- runif(n = 1, min = max(0, w - 0.5 * sw), max = min(1, w + 0.5 * sw))\n    trial_t0 &lt;- runif(n = 1, min = t0, max = t0 + st0)\n    \n    b_upper &lt;- (1 - trial_w) * a\n    b_lower &lt;- -trial_w * a\n    \n    x &lt;- 0\n    t &lt;- trial_t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = trial_v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nThe simulations below again assume that \\(v = 0.5\\) and set \\(s_v = s_w = 0\\) while \\(s_t = 0.5\\). Note that the resulting RT distributions end up having a longer early tail, reflecting greater variability in the fastest RT’s due to variability in residual time.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0, sw = 0, st0 = 0.6)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 Putting it all together (finally!)\nFinally, for completeness, let’s collect our code to run a set of simulations that allows for all three kinds of variability. This code is not executed here, but is included so it can serve as the basis for your own simulations and explorations.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0.5, sw = 0.2, st0 = 0.4)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#shiny-app",
    "href": "diffusion_sim.html#shiny-app",
    "title": "4  From random walk to diffusion",
    "section": "4.4 Shiny App",
    "text": "4.4 Shiny App\nTo have a good deal of fun exploring how the different parameters of a diffusion model influence its predicted choice and RT distributions, download this Shiny app and run it from RStudio. You will also need to download this R script into the same directory as the Shiny app. The Shiny app has some additional functionality that we will see more of in the next chapter.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#exercises",
    "href": "diffusion_sim.html#exercises",
    "title": "4  From random walk to diffusion",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\n\nUnder what circumstances do you think it is more likely for errors to be faster than correct responses? What about circumstances in which errors are more likely to be slower than correct responses? What do you think about the psychological implications of how the diffusion model produces either fast or slow errors?\nWrite a new diffusion simulation that uses a different distribution of drift rates from trial to trial—you might try distributions that are skewed (like an ExGaussian) or have heavy tails (like the T distribution with few degrees of freedom).\n\nDescribe the distribution you picked and whether it corresponds to a theory or hypothesis about how evidence may vary from trial-to-trial in a particular cognitive task.\nDescribe any differences you find between the model you wrote and the model in the chapter that assumes a normal distribution of drift rates across trials.\n\n\n\n\n\n\nRatcliff, R. (1978). A theory of memory retrieval. Psychological Review, 85(2), 59–108.\n\n\nRatcliff, R., & Rouder, J. N. (1998). Modeling response times for two-choice decisions. Psychological Science, 9(5), 347–356.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html",
    "href": "diffusion_fit.html",
    "title": "5  Fitting a diffusion model to data",
    "section": "",
    "text": "5.1 Fitting a model\nIn the previous chapter, we adapted our random walk model into a diffusion model. We saw how the different parameters of the diffusion model produce different patterns of choice and RT behavior. This is what makes the diffusion model a useful cognitive model in so many cases: It has parameters that correspond to meaningful, if unobservable, psychological constructs (evidence accumulation, response caution, response bias) and we can see how differences in those constructs manifest in observable behavior.\nIn this chapter, we turn the model around: Instead of taking parameters and using them to produce behavior, we are taking behavior and inferring what values of the model parameters most likely produced that behavior. In doing so, we are performing a kind of “mind reading”—we are learning about unobservable psychological constructs via how those constructs manifest in behavior.\nThe foundational material puts us in a position to do three things in the next chapter:\nFitting a model to data means finding the values of that model’s parameters that assign the highest likelihood to the observed data. Given a set of parameter values, we can use the model to compute how likely it would be to have seen each observation in our dataset.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#fitting-a-model",
    "href": "diffusion_fit.html#fitting-a-model",
    "title": "5  Fitting a diffusion model to data",
    "section": "",
    "text": "5.1.1 Parameters, likelihoods, and log-likelihoods\nUsing \\(\\theta\\) to stand for a set of parameter values and \\(x_i\\) to stand for a particular observation, we can use \\(f \\left( x \\mid \\theta \\right)\\) to stand for the likelihood of the datum \\(x_i\\) given the parameter values \\(\\theta\\) and a model with a likelihood function \\(f\\).\nConsider the familiar normal distribution. This distribution has two parameters, a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\). So for the normal distribution, \\(\\theta\\) is actually a vector with two elements: \\(\\theta = \\left[ \\mu, \\sigma \\right]\\). The normal distribution is a simple model in that it says that observed values fall in a particular shape around the mean \\(\\mu\\) with a spread described by \\(\\sigma\\). The likelihood function for the normal distribution, written below, indicates how likely it would be to observe datum \\(x_i\\) given specific values of \\(\\mu\\) and \\(\\sigma\\): \\[\nf_{\\text{Normal}} \\left( x_i \\mid \\mu, \\sigma \\right) = \\frac{1}{ \\sigma \\sqrt{2 \\pi}} \\exp \\left[ - \\frac{\\left(x_i - \\mu \\right)^2}{2 \\sigma^2} \\right]\n\\]\nThe graph below assumes that we have just a single observation, \\(x_i = 0\\). The plots show how the likelihood assigned to that datum depends on both parameters of the normal distribution, \\(\\mu\\) and \\(\\sigma\\).\n\n\nCode\nnorm_like_df &lt;- expand_grid(mu = seq(-3, 3, by = 0.1), sigma = seq(0, 3, by = 0.1), x = 0) %&gt;%\n    mutate(d = dnorm(x, mean = mu, sd = sigma))\n\nmu_plot &lt;- norm_like_df %&gt;%\n    filter(sigma == 1) %&gt;%\n    ggplot(aes(x = mu, y = d)) +\n    geom_line() +\n    labs(x = expression(mu), y = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, 1), \")\")), title = expression(\"Assuming \" * sigma == 1))\n\nsigma_plot &lt;- norm_like_df %&gt;%\n    filter(mu == 1) %&gt;%\n    ggplot(aes(x = sigma, y = d)) +\n    geom_line() +\n    labs(x = expression(sigma), y = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(1, sigma), \")\")), title = expression(\"Assuming \" * mu == 1))\n\njoint_plot &lt;- norm_like_df %&gt;%\n    ggplot(aes(x = mu, y = sigma, fill = d)) +\n    geom_raster(interpolate = TRUE) +\n    geom_vline(xintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    geom_hline(yintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    scale_fill_viridis_c() +\n    coord_equal() +\n    labs(x = expression(mu), y = expression(sigma), fill = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, sigma), \")\")), title = \"Joint likelihood\")\n\nmu_plot + sigma_plot + joint_plot + plot_layout(design = \"12\\n33\", guides = \"keep\", heights = c(1, 2))\n\n\n\n\n\nData: (0)\n\n\n\n\nWhen we have more than one observation, the likelihood of all observations is the product of the likelihoods of each individual observation: \\[\nf \\left( \\mathbf{x} \\mid \\theta \\right) = \\prod_{i = 1}^N f \\left( x_i \\mid \\theta \\right)\n\\] This situation is illustrated in the graphs below. These graphs again assume a normal distribution as the model, but now we have observed the values \\(\\mathbf{x} = \\left[ 0, 1 \\right]\\).\n\n\nCode\nnorm_like_df &lt;- expand_grid(mu = seq(-3, 3, by = 0.1), sigma = seq(0, 3, by = 0.1), x = c(0, 1)) %&gt;%\n    mutate(d = dnorm(x, mean = mu, sd = sigma)) %&gt;%\n    group_by(mu, sigma) %&gt;%\n    summarize(d = prod(d), .groups = \"keep\")\n\nmu_plot &lt;- norm_like_df %&gt;%\n    filter(sigma == 1) %&gt;%\n    ggplot(aes(x = mu, y = d)) +\n    geom_line() +\n    labs(x = expression(mu), y = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, 1), \")\")), title = expression(\"Assuming \" * sigma == 1))\n\nsigma_plot &lt;- norm_like_df %&gt;%\n    filter(mu == 1) %&gt;%\n    ggplot(aes(x = sigma, y = d)) +\n    geom_line() +\n    labs(x = expression(sigma), y = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(1, sigma), \")\")), title = expression(\"Assuming \" * mu == 1))\n\njoint_plot &lt;- norm_like_df %&gt;%\n    ggplot(aes(x = mu, y = sigma, fill = d)) +\n    geom_raster(interpolate = TRUE) +\n    geom_vline(xintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    geom_hline(yintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    scale_fill_viridis_c() +\n    coord_equal() +\n    labs(x = expression(mu), y = expression(sigma), fill = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, sigma), \")\")), title = \"Joint likelihood\")\n\nmu_plot + sigma_plot + joint_plot + plot_layout(design = \"12\\n33\", guides = \"keep\", heights = c(1, 2))\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\nData: (0, 1)\n\n\n\n\n\n\n5.1.2 Log-likelihood\nYou may have noticed that the scale on the bottom graph in the second set of plots (where the data were \\(0, 1\\)) has a smaller maximum than it did in the bottom graph in the first set of plots (where the data were just \\(0\\)). This is because the likelihood of the whole dataset is a product of many small numbers, so the result also tends to be small. Unfortunately, this can lead to technical issues when we have many observations because the resulting likelihood may be too small for the computer to accurately calculate.\nAs a result, we often work with the natural logarithm of the likelihood function. Because multiplication turns into addition on the log-scale, this saves the computer from needing to work with very tiny numbers. We often write \\(LL\\) to stand for this “log-likelihood”: \\[\nLL \\left( \\mathbf{x} \\mid \\theta \\right) = \\log \\left[ \\prod_{i = 1}^N f \\left( x_i \\mid \\theta \\right) \\right] = \\sum_{i = 1}^N \\log f \\left( x_i \\mid \\theta \\right)\n\\]\nFortunately, since the logarithm is a monotonic transformation, the parameters that maximize the likelihood are the very same parameters that maximize the log-likelihood. To get a sense of this, the graphs below replicate the graph above with observed values \\(\\mathbf{x} = \\left[ 0, 1 \\right]\\), but showing the log-likelihood instead of the likelihood.\n\n\nCode\nnorm_like_df &lt;- expand_grid(mu = seq(-3, 3, by = 0.1), sigma = seq(0, 3, by = 0.1), x = c(0, 1)) %&gt;%\n    mutate(ll = dnorm(x, mean = mu, sd = sigma, log = TRUE)) %&gt;%\n    group_by(mu, sigma) %&gt;%\n    summarize(ll = sum(ll), .groups = \"keep\")\n\nmu_plot &lt;- norm_like_df %&gt;%\n    filter(sigma == 1) %&gt;%\n    ggplot(aes(x = mu, y = ll)) +\n    geom_line() +\n    labs(x = expression(mu), y = expression(LL[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, 1), \")\")), title = expression(\"Assuming \" * sigma == 1))\n\nsigma_plot &lt;- norm_like_df %&gt;%\n    filter(mu == 1) %&gt;%\n    ggplot(aes(x = sigma, y = ll)) +\n    geom_line() +\n    labs(x = expression(sigma), y = expression(LL[Normal] * bgroup(\"(\", x[i] * \"|\" * list(1, sigma), \")\")), title = expression(\"Assuming \" * mu == 1))\n\njoint_plot &lt;- norm_like_df %&gt;%\n    ggplot(aes(x = mu, y = sigma, fill = ll)) +\n    geom_raster(interpolate = TRUE) +\n    geom_vline(xintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    geom_hline(yintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    scale_fill_viridis_c() +\n    coord_equal() +\n    labs(x = expression(mu), y = expression(sigma), fill = expression(LL[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, sigma), \")\")), title = \"Total log-likelihood\")\n\nmu_plot + sigma_plot + joint_plot + plot_layout(design = \"12\\n33\", guides = \"keep\", heights = c(1, 2))\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\nData: (0, 1)\n\n\n\n\n\n\n5.1.3 Maximizing log-likelihood\nThe combination of parameter values that assigns the highest total log-likelihood to all the data is where the bright spot is in the bottom graph of each set of plots above. For a normal model, the values of \\(\\mu\\) and \\(\\sigma\\) that assign the highest total log-likelihood to the data can be computed directly: they are the sample mean and the population standard deviation computed on the same (i.e., where you divide by \\(N\\) instead of \\(N - 1\\)).\nFor a more complex model, like our diffusion model, we will not be able to calculate these “optimal” parameter values directly. Instead, we will need to use the computer to search for these values. The topic of parameter optimization is a large one that we cannot fully address here. However, the functions supplied with this tutorial make use of two search methods: the Nelder-Mead simplex algorithm and the ucminf function from the ucminf R package.\nThe essence of these parameter optimization algorithms is this: They begin with an initial “guess” of the parameter values and compute the log-likelihood of the data given that initial guess. They then “explore” by calculating the log-likelihood of the data using slightly adjusted parameter values. If this exploration finds a set of adjusted parameter values that assign a higher log-likelihood to the data than the original guess, then these adjusted values are considered a “better guess”. The search process then begins again starting from that better guess. As a result, the “guess” gets gradually better on each step of the algorithm until it can no longer find any way to adjust the parameters to find a higher log-likelihood.\nOne can think of this parameter search as like trying to find a mountain peak in a snowstorm: The snowstorm obscures your visibility so you can only search areas in your immediate vicinity (similar to how the parameter optimizer tries to adjust values of the parameters starting from an initial guess). However, you can still figure out which direction is “uphill” from your current position (similar to how the algorithm finds adjusted parameters that yield a higher log-likelihood than the current best guess). Eventually, you will reach a point where you can’t go uphill any more.\nThe metaphor in the previous paragraph serves to highlight the fact that parameter optimizers are not necessarily guaranteed to find the “global optimum”—the best possible set of parameter values. It is possible that they instead find a “local optimum”—a set of values for which no small adjustment yields an improvement, but which is not the best you could possibly do. To return to the metaphor, the tallest peak may be a mile away from the one you found, but you’ll never know that because you can’t see it.\nThe utility functions we will use in this tutorial are generally pretty good at finding global optima, but it is not guaranteed! This is why it is important to develop some intuitions about how model parameters manifest in behavior, so you can detect when you might have reached a local optimum instead of a global one.\n\n\n5.1.4 Minimizing negative log-likelihood\nDue to the vagaries of history, algorithms that search for optimal parameter values are often written not to maximize something, but to minimize it instead. As a result, what a parameter optimization algorithm actually does is minimize the negative log-likelihood. Basically, it flips the “maximization” problem over, although it does not change anything conceptually. However, it does mean that a more appropriate metaphor is finding the lowest point in a valley rather than the highest peak of a mountain range.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#fitting-a-diffusion-model",
    "href": "diffusion_fit.html#fitting-a-diffusion-model",
    "title": "5  Fitting a diffusion model to data",
    "section": "5.2 Fitting a diffusion model",
    "text": "5.2 Fitting a diffusion model\nTo find the set of parameters for a diffusion model that assign the highest likelihood to the data, we need to be able to compute the likelihood of making a specific choice at a specific time, given a set of diffusion model parameters. This computation is not something we can do by hand—we have to rely on the computer. Fortunately, many folks have contributed to the development of efficient means of doing this computation (e.g., Blurton et al., 2012; Gondan et al., 2014; Hartmann & Klauer, 2021; Navarro & Fuss, 2009; Tuerlincx, 2004).\nWe will make use specifically of an R package that implements these methods called WienR (Hartmann & Klauer, 2023). The name comes from the fact that Norbert Wiener was associated with the development of the “bare-bones” diffusion model without any of the trial-by-trial variability introduced by Ratcliff (1978). That said, the WienR package allows for all of those additional forms of variability as described in the previous chapter.\nBe sure you have the WienR package installed and loaded!\n\n\nCode\nlibrary(WienR)\n\n\n\n5.2.1 Diffusion model likelihood\nThe WienR package provides a function called WienerPDF which returns both the likelihood and the log-likelihood of a set of responses, given a set of diffusion model parameters.\nUnlike with the normal distribution model, where an observation was characterized by a single number, an observation for a diffusion model is characterized by a choice and a response time. Thus, if we want to compute the likelihood of responding at the upper boundary in 1 second given a drift rate of \\(v = 0.5\\), response caution of \\(a = 2\\), response bias of \\(w = 0.5\\), and residual time \\(t_0 = 0.2\\), we use\n\n\nCode\nWienerPDF(t = 1, response = \"upper\", a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\n\n\nFirst-passage time PDF\n\n[1] 0.4362052\n\n---------------------------\n\nLog of first-passage time PDF\n\n[1] -0.8296426\n\n---------------------------\n\n\nThe “First-passage time PDF” is the likelihood of having made that response at that time.\nLet’s imagine that we observed a few more trials, one in which a decision-maker responded at the upper boundary in 1.5 seconds and one in which the decision-maker responded at the lower boundary in 2 seconds. We can compute the likelihood and log-likelihood of all of those trials:\n\n\nCode\nWienerPDF(t = c(1, 1.5, 2), response = c(\"upper\", \"upper\", \"lower\"), a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\n\n\nFirst-passage time PDF\n\n[1] 0.43620517 0.22137809 0.04128626\n\n---------------------------\n\nLog of first-passage time PDF\n\n[1] -0.8296426 -1.5078832 -3.1872255\n\n---------------------------\n\n\nNote two things: First, t and response are allowed to be vectors, where the \\(i\\)th entry in the t vector corresponds to the \\(i\\)th entry in the response vector. Second, we get the likelihood and log-likelihood for each trial individually. So if we want the total log-likelihood, we have to do that ourselves:\n\n\nCode\nresult &lt;- WienerPDF(t = c(1, 1.5, 2), response = c(\"upper\", \"upper\", \"lower\"), a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\n(total_log_likelihood &lt;- sum(result$logvalue))\n\n\n[1] -5.524751\n\n\nThe upshot of this is that WienerPDF gives us a way to evaluate the log-likelihood of a set of choices and response times given a set of values for the diffusion model’s parameters. WienerPDF also allows us to include variability in drift rates via the sv parameter, variability in boundaries via the sw parameter, and variability in residual time via the st0 parameter, like so:\n\n\nCode\nWienerPDF(t = c(1, 1.5, 2), response = c(\"upper\", \"upper\", \"lower\"), a = 2, v = 0.5, w = 0.5, t0 = 0.2, sv = 0.5, sw = 0.1, st0 = 0.4)\n\n\n\nFirst-passage time PDF\n\n[1] 0.56113640 0.26562047 0.06165999\n\n---------------------------\n\nLog of first-passage time PDF\n\n[1] -0.5777913 -1.3256868 -2.7861200\n\n---------------------------\n\n\nNotice that the likelihoods and log-likelihoods changed when we included those three new parameters (which, by default, are set to zero).\n\n\n5.2.2 Finding the best-fitting parameters\nTo find the diffusion model parameters that assign the highest log-likelihood to a given set of choices and RT’s, I have provided a helpful function called fit_wienr. To use this function, download the wienr_fit_utils.r R script to your working directory and run\n\n\nCode\nsource(\"wienr_fit_utils.r\")\n\n\nWe shall see later that the fit_wienr function has some useful bells-and-whistles, but for now let’s just see how it works in its basic form. To do this, let’s use another function from the WienR package called sampWiener which simulates a sample of data from a diffusion model, just like we did in the last chapter (actually, you could even use the diffusion_sim function you built for that purpose!):\n\n\nCode\n(sim_data &lt;- sampWiener(N = 50, a = 2, v = 0.5, w = 0.5, t0 = 0.2))\n\n\n$q\n [1] 1.0145267 0.7785001 0.4830922 0.7476763 0.6889558 0.3996333 0.7652827\n [8] 1.0678441 1.0589824 1.1778599 1.1916915 2.0070563 0.4737943 0.5687575\n[15] 0.9735880 0.7182124 0.7907633 1.5717658 0.6528604 0.8988441 0.6722051\n[22] 0.3394198 0.8275921 2.0176933 0.6527308 1.3267482 1.0114098 0.7283250\n[29] 1.6299191 1.8353343 1.2566600 1.2643379 2.0130723 1.5419533 1.0620593\n[36] 0.5713716 2.5825087 0.7561459 1.1029754 0.6381128 0.5454809 0.6359990\n[43] 3.0614166 1.6769220 0.8270179 2.9357647 1.0710890 0.3431315 0.4737332\n[50] 3.6870701\n\n$response\n [1] \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\"\n[10] \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\"\n[19] \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\"\n[28] \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\"\n[37] \"upper\" \"upper\" \"upper\" \"lower\" \"lower\" \"lower\" \"lower\" \"lower\" \"lower\"\n[46] \"lower\" \"lower\" \"lower\" \"lower\" \"lower\"\n\n$call\nsampWiener(N = 50, a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\nattr(,\"class\")\n[1] \"Diffusion_samp\"\n\n\nThe simulated choices are in sim_data$response and the simulated RT’s are in sim_data$q.\nBecause we are using the diffusion model to simulate data, it’s worth asking how likely the data are given the parameter values we actually used for the simulation:\n\n\nCode\noriginal_likelihood &lt;- WienerPDF(t = sim_data$q, response = sim_data$response, a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\nsum(original_likelihood$logvalue)\n\n\n[1] -66.06148\n\n\nRemember, though, that the optimization algorithm will actually be minimizing the negative log-likelihood. The negative log-likelihood given the original values of the parameters is\n\n\nCode\n-sum(original_likelihood$logvalue)\n\n\n[1] 66.06148\n\n\nIt may be that the “best-fitting” parameters differ from those used to simulate the data, just due to sampling variability. Thus, what we are about to do is a form of parameter recovery, in that we are seeing how well we can “recover” the original model parameters, despite this sampling variability.\nThe following code uses the fit_wienr function to find the best-fitting diffusion model parameter values for the simulated data above:\n\n\nCode\n(fit &lt;- fit_wienr(rt = sim_data$q, response = sim_data$response))\n\n\n$par\n     a[1]      v[1]      w[1]     t0[1] \n2.0672467 0.7208941 0.4525921 0.2034778 \n\n$value\n[1] 65.09442\n\n$convergence\n[1] 1\n\n$message\n[1] \"Stopped by small gradient (grtol).\"\n\n$invhessian.lt\n [1]  0.0233247552  0.0059470005 -0.0003678730 -0.0039030749  0.0343417878\n [6] -0.0053634481 -0.0010979615  0.0025483504  0.0004887555  0.0017438879\n\n$info\n maxgradient     laststep      stepmax        neval \n3.672338e-08 2.012289e-07 1.500625e-02 1.700000e+01 \n\n\nThe best negative log-likelihood that the algorithm was able to find is fit$value—as we can see it is slightly lower than the negative log-likelihood associated with the original simulating parameters, suggesting that the best-fitting parameters really do a slightly better job of “fitting” the data. Those parameter estimates reside in fit$par, where we can compare them against the values used to actually simulate the data.\n\n\n5.2.3 Visualizing model fit\nMinimizing the negative log-likelihood is well and good, but it doesn’t tell us much about how well the model actually “fits” the data. Does it predict similar choice and RT patterns?\nTo address this, it is helpful to visually inspect the fit of the model. There are many ways to do this, but here we will make use of the quantile-probability plots we introduced in previous chapters. We will overlay the quantile-probability plots of the original data with those that would be predicted by the best-fitting diffusion model parameters. To the extent that the model and data are close to one another, we can feel confident that the model is accurately reproducing the major features of the data.\nThe qp_fit function in the wienr_fit_utils.r script calculates the response probabilities and RT quantiles for both the observed data and the fitted model, like so:\n\n\nCode\n(obs_fit_data &lt;- qp_fit(rt = sim_data$q, response = sim_data$response, par = fit$par))\n\n\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response)`\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response, p_resp, rt_p, rt_q, source)`\n\n\n# A tibble: 20 × 12\n# Groups:   drift_index, bound_index, resid_index, sv_index, sw_index,\n#   st0_index [1]\n   drift_index bound_index resid_index sv_index sw_index st0_index response\n         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   \n 1           1           1           1        1        1         1 lower   \n 2           1           1           1        1        1         1 lower   \n 3           1           1           1        1        1         1 lower   \n 4           1           1           1        1        1         1 lower   \n 5           1           1           1        1        1         1 lower   \n 6           1           1           1        1        1         1 upper   \n 7           1           1           1        1        1         1 upper   \n 8           1           1           1        1        1         1 upper   \n 9           1           1           1        1        1         1 upper   \n10           1           1           1        1        1         1 upper   \n11           1           1           1        1        1         1 upper   \n12           1           1           1        1        1         1 upper   \n13           1           1           1        1        1         1 upper   \n14           1           1           1        1        1         1 upper   \n15           1           1           1        1        1         1 upper   \n16           1           1           1        1        1         1 lower   \n17           1           1           1        1        1         1 lower   \n18           1           1           1        1        1         1 lower   \n19           1           1           1        1        1         1 lower   \n20           1           1           1        1        1         1 lower   \n# ℹ 5 more variables: n_resp &lt;int&gt;, p_resp &lt;dbl&gt;, rt_p &lt;dbl&gt;, rt_q &lt;dbl&gt;,\n#   source &lt;chr&gt;\n\n\nWe can then use the result to overlay the quantile-probabilities from the fitted model over those computed from the observed data:\n\n\nCode\nobs_fit_data %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = source, shape = response)) +\n    geom_line(aes(group = interaction(rt_p, source), linetype = source), alpha = 0.5) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response probability\", y = \"RT Quantile (s)\", color = NULL, linetype = NULL, shape = \"Response\", title = \"50 simulated trials\")\n\n\n\n\n\n\n\n\n\nThis visualization highlights a few things: While the diffusion model fits the response probabilities and central tendency of the RT’s quite well, it doesn’t do as good a job with the error RT’s nor with the tails of the RT distributions. This is not because of a qualitative difference between the model and data—after all, we used the diffusion model to simulate these data! Rather, this apparent misfit is due to sampling error: With a small sample, it is harder to estimate RT’s for rare responses (like errors) and it is harder to estimate the tails of the RT distributions (since, by definition, we have fewer observations in the tails).\nSo let’s see what happens if we simulate 1000 trials instead of just 50.\n\n\nCode\nsim_data_large &lt;- sampWiener(N = 1000, a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\n(fit_large &lt;- fit_wienr(rt = sim_data_large$q, response = sim_data_large$response))\n\n\n$par\n     a[1]      v[1]      w[1]     t0[1] \n2.0145409 0.4891320 0.4895752 0.1936399 \n\n$value\n[1] 1394.557\n\n$convergence\n[1] 4\n\n$message\n[1] \"Stopped by zero step from line search\"\n\n$invhessian.lt\n [1] 1 0 0 0 1 0 0 1 0 1\n\n$info\nmaxgradient    laststep     stepmax       neval \n  0.3267429   0.0000000   1.0000000   1.0000000 \n\n\nCode\nobs_fit_data_large &lt;- qp_fit(rt = sim_data_large$q, response = sim_data_large$response, par = fit_large$par)\n\n\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response)`\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response, p_resp, rt_p, rt_q, source)`\n\n\nCode\nobs_fit_data_large %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = source, shape = response)) +\n    geom_line(aes(group = interaction(rt_p, source), linetype = source), alpha = 0.5) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response probability\", y = \"RT Quantile (s)\", color = NULL, linetype = NULL, shape = \"Response\", title = \"1000 simulated trials\")\n\n\n\n\n\n\n\n\n\nMuch better! And notice that the best-fitting parameter values (fit_large$par, above) are quite close to those used to simulate the data.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#fitting-multiple-conditions",
    "href": "diffusion_fit.html#fitting-multiple-conditions",
    "title": "5  Fitting a diffusion model to data",
    "section": "5.3 Fitting multiple conditions",
    "text": "5.3 Fitting multiple conditions\nImagine that we are modeling someone doing a recognition memory task. In this task, participants study a set of items like words or images. Later, during a test phase participants are shown many items and, for each one, they have to decide whether it had been on the study list or not. Therefore, on any given test trial, the item shown could have been studied—called a target—or not studied—called a foil. The participant needs to accumulate evidence from their memory in order to decide whether the item had or had not been studied. It seems reasonable to assume that, if the item is a target, the drift rate for that evidence would tend to be positive, i.e., supporting the choice that the item had been studied. If the item is a foil, the drift rate would tend to be negative, since the samples of evidence from memory would support the choice that the item wasn’t studied. Modeling this task would therefore require estimating two drift rate parameters, one for trials in which a target is shown and one for trials in which a foil is shown. However, because the participant cannot know which type of item they were shown, their response boundaries and residual time should be the same regardless of whether the trial shows a target or a foil.\nThe example above is just one case in which we need to model a decision task by assuming that some parameters differ between conditions (like between targets and foils) while others stay the same (like the response boundaries and residual time). For example, we could simulate the situation above by assuming that \\(v_1 = 0.5\\) is the drift rate for targets while \\(v_2 = -0.5\\) is the drift rate for foils, while keeping \\(a = 2\\), \\(w = 0.5\\), and \\(t_0 = 0.2\\) constant:\n\n\nCode\ntarget_trials &lt;- sampWiener(N = 80, a = 2, v = 0.5, w = 0.5, t0 = 0.2)\nfoil_trials &lt;- sampWiener(N = 80, a = 2, v = -0.5, w = 0.5, t0 = 0.2)\n\n(all_trials &lt;- tibble(\n    rt = c(target_trials$q, foil_trials$q),\n    response = c(target_trials$response, foil_trials$response),\n    item = factor(rep(c(\"Target\", \"Foil\"), each = 80), levels = c(\"Target\", \"Foil\"))\n))\n\n\n# A tibble: 160 × 3\n      rt response item  \n   &lt;dbl&gt; &lt;chr&gt;    &lt;fct&gt; \n 1 1.77  upper    Target\n 2 1.95  upper    Target\n 3 0.486 upper    Target\n 4 0.396 upper    Target\n 5 0.543 upper    Target\n 6 1.40  upper    Target\n 7 0.748 upper    Target\n 8 0.721 upper    Target\n 9 0.938 upper    Target\n10 1.99  upper    Target\n# ℹ 150 more rows\n\n\nWe can use the fit_wienr function to fit these data by supplying it with a drift_index vector. This is a vector of positive integers (1, 2, 3, …) that indicate which drift rate to use when computing the log-likelihood of a particular trial. In this case, since we want drift rate to depend on the type of item shown, we can use the item column of our simulated data to define the drift index:\n\n\nCode\nas.numeric(all_trials$item)\n\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[149] 2 2 2 2 2 2 2 2 2 2 2 2\n\n\nHere is how we supply that to fit_wienr:\n\n\nCode\n(recog_fit &lt;- fit_wienr(rt = all_trials$rt, response = all_trials$response, drift_index = as.numeric(all_trials$item)))\n\n\n$par\n      a[1]       v[1]       v[2]       w[1]      t0[1] \n 2.0345129  0.3167632 -0.7274032  0.5145780  0.2058107 \n\n$value\n[1] 219.4175\n\n$convergence\n[1] 4\n\n$message\n[1] \"Stopped by zero step from line search\"\n\n$invhessian.lt\n [1] 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1\n\n$info\nmaxgradient    laststep     stepmax       neval \n 0.04339481  0.00000000  1.00000000  1.00000000 \n\n\nAs we can see, the estimated drift rates v[1] and v[2] go in the direction we would expect, given that drift_index = 1 indicates a target (positive drift) and drift_index = 2 indicates a foil (negative drift).\nAnd how well does the best-fitting model do? Again, we can supply drift_index to the qp_fit function and make a set of quantile-probability plots.\n\n\nCode\nrecog_obs_fit_data &lt;- qp_fit(rt = all_trials$rt, response = all_trials$response, par = recog_fit$par, drift_index = as.numeric(all_trials$item))\n\n\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response)`\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response, p_resp, rt_p, rt_q, source)`\n\n\nCode\nrecog_obs_fit_data %&gt;%\n    mutate(drift_index_label = factor(drift_index, levels = 1:2, labels = c(\"Target\", \"Foil\"))) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = source, shape = response)) +\n    geom_line(aes(group = interaction(rt_p, source), linetype = source), alpha = 0.5) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    facet_wrap(\"drift_index_label\") +\n    labs(x = \"Response probability\", y = \"RT Quantile (s)\", color = NULL, linetype = NULL, shape = \"Response\")\n\n\n\n\n\n\n\n\n\nNot too bad, though as we saw earlier, the fit isn’t as good for errors or for the tails of the RT distributions.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#comparing-model-fits",
    "href": "diffusion_fit.html#comparing-model-fits",
    "title": "5  Fitting a diffusion model to data",
    "section": "5.4 Comparing model fits",
    "text": "5.4 Comparing model fits\nContinuing with the recognition memory example, imagine that we wanted to address the question: Can this participant distinguish between studied and unstudied items? That question can be reframed in terms of diffusion model parameters: is the drift rate the same or different between target and foil trials?\nTo address that question, we can fit one version of a diffusion model that uses the drift_index vector to estimate separate drift rates for targets and foils, like we just did. We can then fit another version that does not include drift_index, so that it estimates a single drift rate for all trials. The second model would correspond to the hypothesis that the participant cannot distinguish between targets and foils, because they get the same quality of evidence from their memory either way. By comparing how well those two models account for the data, taking into account the fact that the one-drift-rate model is less complex, then we can get evidence to help us address our question.\nLater in the course, we will delve more deeply into the issue of model comparison. For now, we will just introduce the basic idea of “scoring” a model based on (a) how well it fits data; and (b) how complex the model is. As noted below, a more complex model will, in general, be able to fit a wider variety of data. Thus, we should only favor a more complex model to the extent that it fits data better than would be expected based on its complexity. As we will discuss later in the course, there are other reasons why we should be reticent to favor more complex models, but these reasons can sometimes be hard to quantify. Thus, for now, we introduce some simple quantitative methods by which models can be compared while still accounting for their complexity.\n\n5.4.1 Information criteria\nIn the previous section, we already fit the two-drift-rate model. Now let’s fit the one-drift-rate version:\n\n\nCode\n(recog_fit_onedrift &lt;- fit_wienr(rt = all_trials$rt, response = all_trials$response))\n\n\n$par\n      a[1]       v[1]       w[1]      t0[1] \n 1.9376960 -0.1943884  0.5129813  0.2166252 \n\n$value\n[1] 239.2321\n\n$convergence\n[1] 4\n\n$message\n[1] \"Stopped by zero step from line search\"\n\n$invhessian.lt\n [1] 1 0 0 0 1 0 0 1 0 1\n\n$info\nmaxgradient    laststep     stepmax       neval \n 0.06124639  0.00000000  1.00000000  1.00000000 \n\n\nTo compare the fit of the two models, we have a number of techniques at our disposal, but one of the most common is to use an information criterion. These values essentially “score” a model, with higher scores being worse (like golf). The score takes into account how well the model fits the data as well as how complex the model is. Taking into account complexity is critical because, generally speaking, a more complex model will be able to fit data better, so it needs to be “handicapped” to account for this advantage (again, sort of like golf). The information criteria we will use here measure a model’s complexity in terms of the number of free parameters it has.\n\n5.4.1.1 Akaike Information Criterion (AIC)\nThe Akaike Information Criterion (AIC, Akaike, 1974) is defined: \\[\nAIC = 2 \\times NLL + 2 \\times k\n\\] where \\(NLL\\) is the negative log-likelihood of the fitted model (i.e., the quantity that is minimized during model fitting) and \\(k\\) is the number of free parameters in the model.\nWhen we use fit_wienr, the negative log-likelihood is the $value entry in the result. Meanwhile, we can get the number of free parameters as the length of the vector of best-fitting parameter estimates, as illustrated below:\n\n\nCode\naic_twodrift &lt;- 2 * recog_fit$value + 2 * length(recog_fit$par)\naic_onedrift &lt;- 2 * recog_fit_onedrift$value + 2 * length(recog_fit_onedrift$par)\n\nc(\"Two drift\" = aic_twodrift, \"One drift\" = aic_onedrift)\n\n\nTwo drift One drift \n 448.8351  486.4642 \n\n\nRecall that lower scores are better. Since the two-drift model has the lower AIC, we have evidence that this (simulated) participant is able to distinguish between targets and foils.\n\n\n5.4.1.2 Bayesian Information Criterion (BIC)\nThe Bayesian Information Criterion (BIC, Schwarz (1978)) is similar to the AIC but places a stronger penalty on the number of free parameters. Specifically, the penalty scales up with the logarithm of the number of observations. Letting \\(N\\) stand for the number of observed trials, the BIC is defined \\[\nBIC = 2 \\times NLL + k \\log N\n\\] where, again, \\(NLL\\) is the negative log-likelihood and \\(k\\) is the number of free parameters. We can now calculate BIC for each model:\n\n\nCode\nbic_twodrift &lt;- 2 * recog_fit$value + log(nrow(all_trials)) * length(recog_fit$par)\nbic_onedrift &lt;- 2 * recog_fit_onedrift$value + log(nrow(all_trials)) * length(recog_fit_onedrift$par)\n\nc(\"Two drift\" = bic_twodrift, \"One drift\" = bic_onedrift)\n\n\nTwo drift One drift \n 464.2109  498.7649 \n\n\nOnce again, the two-drift model gets the lower—and therefore better—score.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#shiny-app",
    "href": "diffusion_fit.html#shiny-app",
    "title": "5  Fitting a diffusion model to data",
    "section": "5.5 Shiny App",
    "text": "5.5 Shiny App\nThe “Manual parameter fitting” tab of the Shiny app from the previous chapter allows you to try searching for optimal parameters yourself. You will find how difficult it is to do by hand! There is also a “Parameter recovery” tab that replicates what we did in this chapter, namely, using the diffusion model to simulate data and then fitting the diffusion model to the simulated data. Finally, note that the App allows you to play around with other parameters, like the trial-by-trial variability parameters, that we did not use in this chapter.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#exercises",
    "href": "diffusion_fit.html#exercises",
    "title": "5  Fitting a diffusion model to data",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\n\nConsider a dataset with one particularly short RT. Would you want to allow for variability in residual time when fitting these data? Why or why not?\nUsing the Shiny App, see if you can “fool” the model! Specifically, try simulating data until there is a notable discrepancy between the best-fitting parameter values and the values you used to simulate the data. What features of the simulated data may have resulted in this discrepancy?\nWe saw an example in the chapter of how drift rates might vary between conditions of an experiment even while the other parameters of the model would stay the same.\n\nWhat other examples can you think of where the properties of the evidence change but other model parameters do not?\nGive an example of a situation in which you would expect response boundaries to differ between conditions, but not drift rates.\nGive an example of a situation in which you would expect residual time to differ between conditions, but not drift rates or boundaries.\n\nRepeat our recognition memory simulations and model comparisons, but instead of simulating data where the drift rate actually differs between targets and foils, simulate data in which targets and foils have the same drift rate. Use both AIC and BIC to compare the fits of the two-drift and one-drift models. Do these information criteria favor the “correct” model?\n\n\n\n\n\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6), 716–723.\n\n\nBlurton, S. P., Kesselmeier, M., & Gondan, M. (2012). Fast and accurate calculations for cumulative first-passage time distributions in wiener diffusion models. Journal of Mathematical Psychology, 56(6), 470–475. https://doi.org/https://doi.org/10.1016/j.jmp.2012.09.002\n\n\nGondan, M., Blurton, S. P., & Kesselmeier, M. (2014). Even faster and even more accurate first-passage time densities and distributions for the wiener diffusion model. Journal of Mathematical Psychology, 60, 20–22. https://doi.org/https://doi.org/10.1016/j.jmp.2014.05.002\n\n\nHartmann, R., & Klauer, K. C. (2021). Partial derivatives for the first-passage time distribution in wiener diffusion models. Journal of Mathematical Psychology, 103, 102550. https://doi.org/https://doi.org/10.1016/j.jmp.2021.102550\n\n\nHartmann, R., & Klauer, K. C. (2023). WienR: Derivatives of the first-passage time density and cumulative distribution function, and random sampling from the (truncated) first-passage time distribution. https://CRAN.R-project.org/package=WienR\n\n\nNavarro, D. J., & Fuss, I. G. (2009). Fast and accurate calculations for first-passage times in wiener diffusion models. Journal of Mathematical Psychology, 53(4), 222–230. https://doi.org/https://doi.org/10.1016/j.jmp.2009.02.003\n\n\nRatcliff, R. (1978). A theory of memory retrieval. Psychological Review, 85(2), 59–108.\n\n\nSchwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6(2), 461–464.\n\n\nTuerlincx, F. (2004). The efficient computation of the cumulative distribution and probability density functions in the diffusion model. Behavior Research Methods, Instruments, & Computers, 36(4), 702–716.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "model_comparison.html",
    "href": "model_comparison.html",
    "title": "6  Model complexity and model comparison",
    "section": "",
    "text": "6.1 Complexity and generalization\nIn the previous chapter, we were introduced to the idea of using different “information criteria” to compare models with different numbers of parameters. The idea behind these criteria was to provide a quantitative “score” that rewards a model for fitting data but penalizes it for complexity, so as to identify which of a set of models achieved the best balance between complexity and fit. In this chapter, we delve more deeply into some of the methods by which we compare different models, focusing on just a handful of these methods for our purposes. That said, all model comparison approaches share the goal of balancing quality of fit against complexity, inviting us to consider more broadly the different reasons we might compare models and why we (sometimes) prefer simpler models over more complex ones.\nIn a way, the issue with model complexity boils down to the same issue we have in statistics when using a sample to estimate some population quantity. In statistics, where we use models on the “descriptive” end of the modeling continuum, our goal is to identify patterns in our sample of data that we would expect to see in other samples from some broader population. In that way, we generalize whatever conclusions we draw about our sample to this broader population. We have essentially the same goal when applying a cognitive model, even though it falls on the “causal” end of the modeling continuum: By fitting the model to a sample of data, we are hoping to draw some inferences about how a sample of participants accomplished a particular task. We hope that those inferences apply more broadly, i.e., that we can make a general statement about how some broader population accomplishes that task.\nThe challenge we face in both statistics and cognitive modeling is that we know that not every sample from the same population is identical. This sampling variability has two consequences: First, it is possible that our observed data sample is biased, in the sense that it has some idiosyncratic property that is not representative of what we would expect to see in the population more broadly. In that case, what we conclude about our sample may not generalize to the broader population. Second, even if our sample were unbiased, variability in the population means that we cannot expect our conclusions to generalize equally well to every member of the population—all we can hope is that our conclusions apply on average.\nAddressing the consequences of sampling variability is challenging because, by definition, we do not know how variable the population is nor whether our sample is biased or not. In statistics, we address this lack of omniscience by constructing a descriptive model which enables us to estimate how wrong we might be. This is the meaning behind the “standard error” in traditional statistics or the posterior distribution in Bayesian statistics. In the end, we confine ourselves to conclusions that are supported by estimates that are strong enough to overcome this baseline level of wrongness, in which case we call our results “significant” or “credible”. Of course, this does not completely inoculate us from drawing improper generalizations, but it helps us pay attention to data patterns that are more likely to generalize while still acknowledging our uncertainty.\nOur techniques for comparing computational cognitive models serve the same function as a “standard error” or posterior distribution in descriptive statistical modeling. A good model fit may be due to idiosyncracies of the model or of the sample, neither of which may be representative of the broader population to which we are hoping to generalize. Because a more complex model has more flexibility to fit any possible sample, we want to avoid favoring a complex model unless it fits better over and above the degree to which we would expect it to fit better just due to its flexibility. As in statistics, model comparison is not guaranteed to identify the “true” model that explains performance on some task more generally. However, model comparison is a valuable tool that helps us identify the aspects of a model that are most essential for explaining performance and which are most likely to generalize.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "model_comparison.html#cross-validation",
    "href": "model_comparison.html#cross-validation",
    "title": "6  Model complexity and model comparison",
    "section": "6.2 Cross-validation",
    "text": "6.2 Cross-validation\nThe motivation behind many issues in model comparison is exemplified by the approach known as cross-validation (Arlot & Celisse, 2010; Browne, 2000; Zucchini, 2000). In cross-validation, one divides the sample of data into two parts, a training set and a testing set. The model is fit to the data in the training set and we then compute the log-likelihood of the data in the testing set, using the parameter values obtained by fitting the model to the training set. A model is preferred to the extent that is able to assign higher likelihood to the data in the training set. The rationale behind cross-validation is, thus, to evaluate a model on its ability to generalize from the training data to the test data. A model that is too flexible will tend to “over-fit” the various idiosyncratic features of the training data that are not reproduced in the testing data, meaning it will perform worse on average than a model that captures the systematic aspects that are common to both the training and testing data.\n\n6.2.1 Example\nTo make this situation concrete, let’s use a diffusion model to simulate some data and then use cross-validation to compare different potential models we could use to fit that data. Let’s again assume we are doing a recognition memory task, where target items have positive drift rates and foil items have negative drift rates. We will also assume that there is trial-by-trial variability in drift rates (the sv parameter) and that it is the same for both targets and foils.\n\n\nCode\nn_trials &lt;- 100\n\ntarget_trials &lt;- sampWiener(N = n_trials, a = 2, v = 0.5, w = 0.5, t0 = 0.2, sv = 0.3)\nfoil_trials &lt;- sampWiener(N = n_trials, a = 2, v = -0.5, w = 0.5, t0 = 0.2, sv = 0.3)\n\n(all_trials &lt;- tibble(\n    rt = c(target_trials$q, foil_trials$q),\n    response = factor(c(target_trials$response, foil_trials$response), levels = c(\"lower\", \"upper\")),\n    item = factor(rep(c(\"Target\", \"Foil\"), each = n_trials), levels = c(\"Target\", \"Foil\"))\n))\n\n\n# A tibble: 200 × 3\n      rt response item  \n   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt; \n 1 1.29  upper    Target\n 2 0.973 upper    Target\n 3 0.695 upper    Target\n 4 1.75  upper    Target\n 5 1.17  upper    Target\n 6 1.18  upper    Target\n 7 1.91  upper    Target\n 8 1.46  upper    Target\n 9 0.988 upper    Target\n10 0.554 upper    Target\n# ℹ 190 more rows\n\n\nIf we were coming to this data “fresh”, like we would in a real experiment, we might consider applying a few models, since we wouldn’t know which one best accounts for our data. For the sake of the present example, let’s focus on three possible models:\n\nModel A assumes that both the mean drift rate (v) and the drift rate standard deviation (sv) are the same for both targets and foils. This model is “incorrect”, in the sense that it assumes equal parameters for both targets and foils. Nonetheless, we may want to verify that participants are actually able to distinguish between targets and foils. In a different experiment, we might be interested in comparing two conditions to see whether drift rate differs between them. In any case, we can think of model A as a sort of “null” model.\nModel B assumes that the mean drift rate (v) varies between targets and foils, but the drift rate standard deviation (sv) is the same for both targets and foils. This model is “correct”, in the sense that it allows parameters to vary in the same way that they did in our simulations.\nModel C assumes that both the mean drift rate (v) and drift rate standard deviation (sv) vary between targets and foils. This model is “incorrect”, in the sense that it is too flexible relative to how the parameters varied in simulation. Nonetheless, we expect that this model will probably fit better than the “correct” model (B) since the additional drift rate variability parameter will enable it to fit any quirks in the simulated data.\n\n\n6.2.1.1 The steps of cross-validation\nTo see how cross-validation works, let’s go through a single example of applying it to our simulated data. First, we need to split our data into “training” and “testing” sets. We will do this randomly, so as not to introduce any bias. Relatedly, we will need to make sure that all the conditions of the experiment are represented in both testing and training sets in the same proportion that they are in the full data. Again, this avoids introducing bias by not “over-representing” one condition or the other.\nFor our first pass, let’s have the training and testing data be of equal size. We will use R’s sample function to randomly assign each trial within each condition (defined by item) to either the training or testing set.\n\n\nCode\nall_trials_traintest &lt;- all_trials %&gt;%\n    group_by(item) %&gt;%\n    mutate(set = factor(\n            # In the line below, `n()` is the number of trials within the groups defined by the variables in the `group_by` line above\n            sample(rep(c(\"training\", \"testing\"), round(c(0.5, 0.5) * n())))[1:n()],\n            levels = c(\"training\", \"testing\")\n        )\n    )\n\n\nYou may already have noticed something important about cross-validation: Because we have to divide the data up at random, it can give different results each time you do it! We will return to this issue.\nFor now, though, once we have divided up our data, we need to fit each model to only the training data. This looks just like it did in the previous chapter, where we are using fit_wienr to find parameter estimates for each model.\n\n\nCode\nfit_a &lt;- with(\n    all_trials_traintest %&gt;%\n        filter(set == \"training\"),\n    fit_wienr(rt = rt, response = response, fit_sv = TRUE)\n)\n\nfit_b &lt;- with(\n    all_trials_traintest %&gt;%\n        filter(set == \"training\"),\n    fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), fit_sv = TRUE)\n)\n\nfit_c &lt;- with(\n    all_trials_traintest %&gt;%\n        filter(set == \"training\"),\n    fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE)\n)\n\n\nWe can take a look at each of the fits to see what the estimated parameters are and how well each model fared on the training data:\n\n\nCode\nfit_a\n\n\n$par\n        a[1]         v[1]         w[1]        t0[1]        sv[1] \n2.091667e+00 1.159507e-01 4.695512e-01 1.982995e-01 1.731895e-10 \n\n$value\n[1] 165.1481\n\n$convergence\n[1] 1\n\n$message\n[1] \"Stopped by small gradient (grtol).\"\n\n$invhessian.lt\n [1]  9.527092e-03  1.491059e-04  1.064512e-04 -1.656450e-03  3.294449e-04\n [6]  1.339079e-02 -2.183984e-03  2.129479e-04  2.940087e-05  1.135847e-03\n[11] -1.118024e-04 -7.466536e-06  8.637283e-04 -1.958822e-04  7.797146e-01\n\n$info\n maxgradient     laststep      stepmax        neval \n2.435586e-08 2.894598e-07 1.838266e-03 2.200000e+01 \n\n\nCode\nfit_b\n\n\n$par\n      a[1]       v[1]       v[2]       w[1]      t0[1]      sv[1] \n 2.3376486  0.8377254 -0.4882018  0.4610377  0.1776323  0.4922548 \n\n$value\n[1] 148.8284\n\n$convergence\n[1] 1\n\n$message\n[1] \"Stopped by small gradient (grtol).\"\n\n$invhessian.lt\n [1]  0.0541570486  0.0358211608 -0.0340917434  0.0001095013 -0.0048551547\n [6]  0.0931226495  0.0635757612 -0.0216317695 -0.0030307138 -0.0015420380\n[11]  0.0813262103  0.0578574019 -0.0027352232  0.0020638460 -0.0799306514\n[16]  0.0012060421 -0.0001094818  0.0001507736  0.0012252126 -0.0052349758\n[21]  0.2343820377\n\n$info\n maxgradient     laststep      stepmax        neval \n5.156036e-09 7.943901e-07 1.838266e-03 2.300000e+01 \n\n\nCode\nfit_c\n\n\n$par\n      a[1]       v[1]       v[2]       w[1]      t0[1]      sv[1]      sv[2] \n 2.3478544  0.8629154 -0.4818190  0.4583973  0.1775218  0.5569940  0.4829696 \n\n$value\n[1] 148.8213\n\n$convergence\n[1] 1\n\n$message\n[1] \"Stopped by small gradient (grtol).\"\n\n$invhessian.lt\n [1]  0.0588198420  0.0499271162 -0.0276548559 -0.0016398158 -0.0047462573\n [6]  0.1222239896  0.0814171966  0.1050278225 -0.0079168115 -0.0075676409\n[11] -0.0013898078  0.1741511850  0.0577339615  0.0584003183 -0.0039679555\n[16]  0.0019161317 -0.0412546290 -0.0804351304  0.0016883348 -0.0001103656\n[21] -0.0109935933  0.0020953430  0.0012042522 -0.0045784185 -0.0048449822\n[26]  0.4517784401  0.1613120960  0.2311467593\n\n$info\n maxgradient     laststep      stepmax        neval \n3.801981e-08 2.683823e-07 5.252187e-03 2.100000e+01 \n\n\nAs expected, model A had the highest negative log-likelihood (i.e., the worst fit), followed by model B, with model C only doing barely better than model B. The estimated parameters for the “correct” model (B) are pretty close to those we used to simulate the data. Meanwhile, the estimated parameters for models A and C also tend to correspond pretty well with those used to generate the data (for example, the boundary separation a, response bias w, and residual time t0 for those models are all pretty close to the values we used in simulation).\nBut the real question is how well each model does with the testing data. To do that, we need to compute the negative log-likelihood of the data using the parameters estimated above. We can do that by passing the par element of the fits above as the init_par argument to the fit_wienr function and setting the return_nll argument to TRUE.\n\n\nCode\nwith(\n    all_trials_traintest %&gt;%\n        filter(set == \"testing\"),\n    fit_wienr(rt = rt, response = response, init_par = fit_a$par, fit_sv = TRUE, return_nll = TRUE)\n)\n\n\n[1] 146.1184\n\n\nCode\nwith(\n    all_trials_traintest %&gt;%\n        filter(set == \"testing\"),\n    fit_wienr(rt = rt, response = response, init_par = fit_b$par, drift_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n)\n\n\n[1] 139.8552\n\n\nCode\nwith(\n    all_trials_traintest %&gt;%\n        filter(set == \"testing\"),\n    fit_wienr(rt = rt, response = response, init_par = fit_c$par, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n)\n\n\n[1] 140.3905\n\n\nBased on these results, model A is the worst of the three, as we might have expected. But when evaluated on the testing data, model B actually fares slightly better than model C, despite the fact that model C achieved a better negative log-likelihood on the training data. This is an example of cross-validation working as intended—it has identified that model C is too flexible in this context. Model C “overfit” the training data to such an extent that it did not generalize as well to the testing data as model B.\n\n\n6.2.1.2 Repeating cross-validation\nAs noted above, though, we would get different results from cross-validation if we split the data into training/testing sets differently. To get a sense of which models are consistently able to generalize better, we need to replicate the cross-validation procedure several times, each with a different training/test split. In the code below, I use a for loop to do this. In the cv_results tibble, I keep track of the negative log-likelihood that each model achieves on both the training and testing data, so I can plot those at the end.\n\n\nCode\nn_cv &lt;- 100\n\ncv_results &lt;- c()\n\nfor (cv_index in 1:n_cv) {\n    # Split data into training/testing sets\n    all_trials_traintest &lt;- all_trials %&gt;%\n        group_by(item) %&gt;%\n        mutate(set = factor(\n                sample(rep(c(\"training\", \"testing\"), round(c(0.5, 0.5) * n())))[1:n()],\n                levels = c(\"training\", \"testing\")\n            )\n        )\n    \n    # Fit each model to the training data\n    fit_a &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"training\"),\n        fit_wienr(rt = rt, response = response, fit_sv = TRUE)\n    )\n    \n    fit_b &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"training\"),\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    fit_c &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"training\"),\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    # Evaluate each model on the testing data\n    test_nll_a &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"testing\"),\n        fit_wienr(rt = rt, response = response, init_par = fit_a$par, fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_b &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"testing\"),\n        fit_wienr(rt = rt, response = response, init_par = fit_b$par, drift_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_c &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"testing\"),\n        fit_wienr(rt = rt, response = response, init_par = fit_c$par, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    # Save results of current iteration\n    cv_results &lt;- rbind(\n        cv_results,\n        tibble(\n            set = \"training\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(fit_a$value, fit_b$value, fit_c$value)\n        ),\n        tibble(\n            set = \"testing\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(test_nll_a, test_nll_b, test_nll_c)\n        )\n    )\n}\n\ncv_results %&gt;%\n    mutate(set = factor(set, levels = c(\"training\", \"testing\"))) %&gt;%\n    ggplot(aes(x = model, y = nll, color = model)) +\n    geom_point(position = position_jitter(width = 0.4), alpha = 0.1, size = 0.5) +\n    stat_summary(fun.data = mean_cl_boot) +\n    facet_wrap(\"set\", scales = \"free_y\") +\n    labs(x = \"Model\", y = \"Negative log-likelihood\")\n\n\n\n\n\nSmall, light points show each individual cross-validation run, the large points show the mean and bootstrapped 95% confidence interval across runs.\n\n\n\n\nNotice that model C does, on average, achieve a slightly better NLL than model B on the training data. Specifically, the average NLL for model B on the training data is 141.9735778 and for model C is 141.4932351. However, model B achieves a slightly better NLL than model C on the testing data (146.5512123 for model B, 146.6566674 for model C). These differences are not particularly large, of course, but they show the basic idea behind cross-validation as an approach to model comparison.\n\n\n\n6.2.2 \\(K\\)-fold cross-validation\nIn the example above, the testing and training sets were the same size. This is not terribly efficient. Because the models are only being fit to half the data, there is more variability/uncertainty in the estimated parameters than there would be if they were fit to the entire dataset. As such, in most applications of cross-validation, the training set is larger than the testing set.\nThese applications are often referred to as “\\(K\\)-fold cross-validation” because they involve splitting the data into \\(K\\) evenly-sized sets and then performing cross-validation \\(K\\) times. Each time, a different one of the \\(K\\) sets is treated as the “testing” data, with the remaining \\(K - 1\\) sets used for training. A common choice for \\(K\\) is 10, such that the proportion of data “left out” for testing is 0.1, not 0.5.\nLet’s see how we would implement \\(K\\)-fold cross-validation in our running example. The first step is to split the data into \\(K\\) equal sets. The code below shows one way to do this using the sample function like we did in the example above. Notice that we use the rep function to repeat each index ceiling(n() / K) times. The ceiling function rounds any fractional amounts up, so we will always have at least as many indexes to sample from as we have trials. The [1:n()] truncates the vector of repeated indices so that it has exactly n() elements.\n\n\nCode\nK &lt;- 10\n\nall_trials_split &lt;- all_trials %&gt;%\n    group_by(item) %&gt;%\n    mutate(set = sample(rep(1:K, ceiling(n() / K))[1:n()]))\n\n\nThe result looks like this, although it is worth keeping in mind that different runs will produce different splits since they are done randomly.\n\n\nCode\nall_trials_split\n\n\n# A tibble: 200 × 4\n# Groups:   item [2]\n      rt response item     set\n   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt;\n 1 1.29  upper    Target     4\n 2 0.973 upper    Target     9\n 3 0.695 upper    Target     6\n 4 1.75  upper    Target     1\n 5 1.17  upper    Target     4\n 6 1.18  upper    Target     8\n 7 1.91  upper    Target     2\n 8 1.46  upper    Target    10\n 9 0.988 upper    Target     3\n10 0.554 upper    Target     2\n# ℹ 190 more rows\n\n\nOnce we have split the data, we can adapt the for loop we used earlier so that it loops over the \\(K\\) folds in the splitted data.\n\n\nCode\nk_fold_cv_results &lt;- c()\n\nfor (fold in 1:K) {\n    # Fit each model to the training data\n    fit_a &lt;- with(\n        all_trials_split %&gt;%\n            filter(set != fold),\n        fit_wienr(rt = rt, response = response, fit_sv = TRUE)\n    )\n    \n    fit_b &lt;- with(\n        all_trials_split %&gt;%\n            filter(set != fold),\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    fit_c &lt;- with(\n        all_trials_split %&gt;%\n            filter(set != fold),\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    # Evaluate each model on the testing data\n    test_nll_a &lt;- with(\n        all_trials_split %&gt;%\n            filter(set == fold),\n        fit_wienr(rt = rt, response = response, init_par = fit_a$par, fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_b &lt;- with(\n        all_trials_split %&gt;%\n            filter(set == fold),\n        fit_wienr(rt = rt, response = response, init_par = fit_b$par, drift_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_c &lt;- with(\n        all_trials_split %&gt;%\n            filter(set == fold),\n        fit_wienr(rt = rt, response = response, init_par = fit_c$par, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    # Save results of current iteration\n    k_fold_cv_results &lt;- rbind(\n        k_fold_cv_results,\n        tibble(\n            fold = fold,\n            set = \"training\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(fit_a$value, fit_b$value, fit_c$value)\n        ),\n        tibble(\n            fold = fold,\n            set = \"testing\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(test_nll_a, test_nll_b, test_nll_c)\n        )\n    )\n}\n\nk_fold_cv_results %&gt;%\n    mutate(set = factor(set, levels = c(\"training\", \"testing\"))) %&gt;%\n    ggplot(aes(x = model, y = nll, color = model)) +\n    geom_point(position = position_jitter(width = 0.1), alpha = 0.5, size = 0.5) +\n    stat_summary(fun.data = mean_cl_boot) +\n    facet_wrap(\"set\", scales = \"free_y\") +\n    labs(x = \"Model\", y = \"Negative log-likelihood\")\n\n\n\n\n\n\n\n\n\nThe result looks pretty similar to what we had previously, in that model C fits slightly better than model B on the training data, but they fare about equally well on the testing data.\n\n\n6.2.3 Leave-one-out cross-validation\nAs a reminder, each time we run \\(K\\)-fold cross-validation, we will get a slightly different result because of the random way in which we split the data. Moreover, using \\(K\\)-fold CV was motivated by an attempt to make efficient use of the data at hand, so as not to artificially inflate our uncertainty about estimated model parameters. If we take these two issues—randomness and efficiency—seriously, then the best way to do cross-validation would actually be to have as many “folds” as we have observations. In other words, we fit each model to all but one observation and then test them on the one that we left out and repeat this process for all \\(N\\) observations in our dataset. That solves the efficiency problem, since the models are able to train on essentially all of the data. It also solves the randomness problem because instead of doing CV with random subsets, we do it exhaustively, once for each observation. This approach is, prosaically, referred to as Leave-One-Out Cross-Validation (LOOCV).\nWe said LOOCV resolves the “efficiency” issue with cross-validation, but only in the sense that the models are able to make use of nearly all the data. LOOCV is certainly not efficient in terms of computing time, since it requires fitting each model \\(N\\) times, once for each left-out observation. We typically apply computational cognitive models to data from experiments where we have a few hundred trials per participant (and we would need to replicate LOOCV for each participant too). Moreover, as we’ve seen, estimating best-fitting parameters even for a relatively simple cognitive model like a diffusion model is not trivial. Therefore, LOOCV is almost never used in practice.\nFor fun, though, let’s try it with our running example, where the code below adapts the \\(K\\)-fold CV code we used in the previous section. Note the use of the “negative indexing” trick to exclude each observation i from the training data in the for loop.\n\n\nCode\nloocv_results &lt;- c()\n\nfor (i in 1:nrow(all_trials)) {\n    # Fit each model to the training data\n    fit_a &lt;- with(\n        all_trials[-i,],\n        fit_wienr(rt = rt, response = response, fit_sv = TRUE)\n    )\n    \n    fit_b &lt;- with(\n        all_trials[-i,],\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    fit_c &lt;- with(\n        all_trials[-i,],\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    # Evaluate each model on the testing data\n    test_nll_a &lt;- with(\n        all_trials[i,],\n        fit_wienr(rt = rt, response = response, init_par = fit_a$par, fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_b &lt;- with(\n        all_trials[i,],\n        fit_wienr(rt = rt, response = response, init_par = fit_b$par, drift_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_c &lt;- with(\n        all_trials[i,],\n        fit_wienr(rt = rt, response = response, init_par = fit_c$par, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    # Save results of current iteration\n    loocv_results &lt;- rbind(\n        loocv_results,\n        tibble(\n            fold = i,\n            set = \"training\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(fit_a$value, fit_b$value, fit_c$value)\n        ),\n        tibble(\n            fold = i,\n            set = \"testing\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(test_nll_a, test_nll_b, test_nll_c)\n        )\n    )\n}\n\nloocv_results %&gt;%\n    mutate(set = factor(set, levels = c(\"training\", \"testing\"))) %&gt;%\n    ggplot(aes(x = model, y = nll, color = model)) +\n    geom_point(position = position_jitter(width = 0.1), alpha = 0.1, size = 0.5) +\n    stat_summary(fun.data = mean_cl_boot) +\n    facet_wrap(\"set\", scales = \"free_y\") +\n    labs(x = \"Model\", y = \"Negative log-likelihood\")\n\n\n\n\n\n\n\n\n\nConsistent with the other varieties of CV above, LOOCV finds that model A generalizes the worst on average (mean testing NLL = 1.5718645), followed by model C (mean testing NLL = 1.4550586) then closely by model B ((mean testing NLL = 1.453186)). Again, the difference between models B and C is not dramatic, but consider that model C consistently outperforms model B on the training data—the message that we get from LOOCV is that this advantage is due to overfitting, not because model C captures anything systematic beyond that which is captured by model B. Therefore, we should prefer the simpler model B when deciding which model best explains our data.\n\n\n6.2.4 Summary\nCross-validation is not always the most practical approach to assessing model fit vs. complexity. That said, it shows one reason why we might prefer a simpler model: Such a model is less likely to “overfit” our data and is therefore better able to generalize to new data. This has practical advantages if we are using the model to make predictions about future unseen data. It is also theoretically meaningful because a model that generalizes better is probably one that has mechanisms that are important for producing the systematic features of our data.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "model_comparison.html#akaike-information-criterion",
    "href": "model_comparison.html#akaike-information-criterion",
    "title": "6  Model complexity and model comparison",
    "section": "6.3 Akaike Information Criterion",
    "text": "6.3 Akaike Information Criterion\nThe practical issues with cross-validation mean that it is rarely used to compare cognitive models. That said, one of the model comparison approaches we saw in the last chapter, the Akaike Information Criterion (AIC; Akaike (1974)), is in fact an asymptotic approximation to LOOCV. We won’t prove this fact here, but check out Stone (1977). For our purposes, we can simply appreciate that the asymptotic equivalence of AIC and LOOCV is very convenient because it means that we can often reasonably approximate LOOCV while only needing to fit the model once.\nLet’s calculate the AIC for each of the three models in our running example. To do this, we will first need to fit each model to the full dataset (no more splitting into testing/training sets). This is done in the chunk of code below.\n\n\nCode\nfit_a &lt;- with(\n    all_trials,\n    fit_wienr(rt = rt, response = response, fit_sv = TRUE)\n)\n\nfit_b &lt;- with(\n    all_trials,\n    fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), fit_sv = TRUE)\n)\n\nfit_c &lt;- with(\n    all_trials,\n    fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE)\n)\n\n\nNow recall that the AIC is defined as\n\\[\nAIC = 2 \\times NLL + 2 \\times N_p\n\\]\nwhere \\(NLL\\) is the negative log-likelihood of the fitted model and \\(N_p\\) is the number of free parameters in the model. Thus, the code below computes the AIC for each of the three models\n\n\nCode\n2 * fit_a$value + 2 * length(fit_a$par)\n\n\n[1] 630.4553\n\n\nCode\n2 * fit_b$value + 2 * length(fit_b$par)\n\n\n[1] 583.0139\n\n\nCode\n2 * fit_c$value + 2 * length(fit_c$par)\n\n\n[1] 583.6266\n\n\nLike with cross-validation above, AIC finds that model A is the worst and that model B has a slight advantage over model C.\nReturning to LOOCV for a moment, recall that the value we obtained was the mean negative log-likelihood across each of the \\(N\\) left-out observations. Meanwhile, the \\(NLL\\) we get from fitting the full model is the summed negative log-likelihood across all \\(N\\) observations. So if we want to put the results from LOOCV on the same scale as the results we get from AIC, we need to multiply them by \\(2N\\). I do this in the chunk of code below.\n\n\nCode\nloocv_results %&gt;%\n    filter(set == \"testing\") %&gt;%\n    group_by(model) %&gt;%\n    summarize(rescaled_result = 2 * sum(nll))\n\n\n# A tibble: 3 × 2\n  model rescaled_result\n  &lt;chr&gt;           &lt;dbl&gt;\n1 A                629.\n2 B                581.\n3 C                582.\n\n\nAlthough the reader is again referred to Stone (1977) for a formal proof, this example shows that, when appropriately rescaled, AIC and LOOCV give very similar results and will generally lead us to the same conclusions regarding which of a set of models to prefer.\nThis rough equivalence also shows that AIC ultimately assesses models on their predictive performance, that is, their ability to fit future unseen data generated by the same processes that produced our original data.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "model_comparison.html#bayes-factors-and-bayesian-information-criterion",
    "href": "model_comparison.html#bayes-factors-and-bayesian-information-criterion",
    "title": "6  Model complexity and model comparison",
    "section": "6.4 Bayes Factors and Bayesian Information Criterion",
    "text": "6.4 Bayes Factors and Bayesian Information Criterion\nIn the last chapter, we were introduced to another model comparison metric, the so-called “Bayesian” Information Criterion [BIC; Schwarz (1978)]. The BIC is, under certain very restrictive circumstances, asymptotically equivalent to a Bayes Factor (Raftery, 1995). The relationship between AIC/LOOCV and the Bayes factor/BIC can be summarized like this: AIC/LOOCV assess the ability of a model to fit future data conditional on the data that has already been observed; Bayes factors/BIC assess the ability of a model to fit any data, irrespective of the data that has already been observed. In other words, AIC/LOOCV assess the posterior predictive ability of a model whereas BIC/Bayes Factors assess the prior predictive ability of a model (Gelman et al., 2014; Piironen & Vehtari, 2017; Vehtari & Lampinen, 2002).\nIt is worth repeating that BIC does not have the same formal relationship to Bayes factors that AIC has to LOOCV, so BIC should not be thought of, outside of very special cases, as equivalent to a Bayes factor. Nonetheless, it has the same underlying motivation, which is to favor models that make more limited predictions a priori. This is why the formula for BIC imposes a stronger penalty for the number of free parameters in a model, because the flexibility afforded by those parameters doesn’t just allow the model to “overfit” the data we observed, it allows it to overfit any data we might have observed.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "model_comparison.html#simplicity-vs.-complexity",
    "href": "model_comparison.html#simplicity-vs.-complexity",
    "title": "6  Model complexity and model comparison",
    "section": "6.5 Simplicity vs. Complexity",
    "text": "6.5 Simplicity vs. Complexity\nUltimately, model comparison allows us to answer the question, “what is the simplest model, among those I am considering, that is sufficient to achieve a good quantiative fit to the data in my sample?” By laying out this question explicitly, we are in a position to see three of the important qualifiers on any conclusions we draw based on model comparisons:\n\nDefining “simplicity”: Different model comparison metrics have different operational definitions of “simplicity”. AIC and BIC each define it in terms of the number of free parameters in a model. Cross-validation defines it in terms of how well a model fit to training data can account for test data. A Bayes factor defines it in terms of the prior predictive distribution of a model, etc.\nConditional on the set of models being compared: Although it may be possible to identify a “preferred” model using a model comparison metric, that preference is only with respect to the set of models being compared. It is entirely possible that an additional unconsidered model would be preferred if it were included. It may also be the case that the “preferred” model is only the “least bad” model among those under consideration—that’s why it is always important to verify that a model is actually reproducing the data patterns that you think are most important in your application.\nConditional on the sample: It may be that a different sample would have lead to a different “preferred” model, although as noted above, model comparison metrics are usually designed to account for this form of sampling variability. This qualification is more important when attempting to generalize more broadly, for example, to other kinds of related tasks or to the same task but with different materials.\n\nOften, model comparison is analogized to “Occam’s Razor”, the famous principle that, if many explanations are available, we should prefer the simplest one. The issue with this analogy is that it conflates two ways in which a model can be “simple”: A model can be “simple” according to one of the operational definitions of simplicity/complexity employed by a particular model comparison metric. But a model can also be “simple” in the sense that it is easier for a scientist to understand or to describe to someone else. The first sense of “simplicity” can be quantified (as in the methods reviewed in this chapter), but the second sense of “simplicity” is more to do with the background and expertise of particular scientists, the means by which they communicate, and the broader culture in which they are working. In other words, the second sense of “simplicity” has to do with the fact that a causal model is not meant just to fit data, but also to help people understand why the data turned out that way. As the bumper sticker says, scientists are people too and, being limited creatures, cannot understand everything. This second sense of simplicity should not be dismissed, though: If someone can understand a model more easily, they may also be able to devise predictions, tests, and extensions of the model more easily too.\nBecause the two senses of “simplicity” are separate, they are not guaranteed to align with one another. There may be cases in which a model that is “simple” in the sense of having few free parameters or a narrow prior predictive distribution may be very difficult to explain or describe. It is also possible that a model that is easier to explain or describe might be more flexible or have more parameters than needed to account for any particular sample of data. The latter situation is likely to occur if a model is designed to account for a wide variety of phenomena—such a model may contain mechanisms (with associated parameters) that are only relevant for certain phenomena.\nIt is also worth repeating that a simpler model—regardless of the sense of “simplicity”—is not guaranteed to be any more “true” or “correct” than a complex model. The “truth”, whatever that is, is almost certainly more complex than any model we would devise. Rather, given that all models are deliberate simplifications, the virtue of a simpler model is that (a) it is more likely to generalize well because it is less likely that its ability to fit data is due to some idiosyncratic property of the model or the sample; and (b) it is often (but not always) easier to describe and explain.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "model_comparison.html#exercises",
    "href": "model_comparison.html#exercises",
    "title": "6  Model complexity and model comparison",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\n\nThe discussions of cross-validation and generalization in this chapter focused on situations in which we wanted to “generalize” to data from the same (simulated) participant in the same (simulated) task. How would you adapt cross-validation to assess other kinds of generalization, such as from one participant to another? Or from one task to another? In formulating your thoughts, you may want to read Busemeyer & Wang (2000) and Navarro (2018).\nUnlike in cognitive modeling, where cross-validation is rarely used, machine learning models are often compared using cross-validation. Models in machine learning sit on the “descriptive” end of the modeling spectrum. Machine learning models are typically applied to very large datasets and have a lot of free parameters (e.g., each weight in a neural network model is technically a free parameter). Why do you think cross-validation is more common in machine learning than in cognitive modeling?\nGiven that AIC and BIC judge models according to different criteria, which do you think is better suited for identifying the model the “best explains” a given set of data? What reasons might there be to prefer one approach over the other? Could the term “explain” have different interpretations in different applications?\n\n\n\n\n\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6), 716–723.\n\n\nArlot, S., & Celisse, A. (2010). A survey of cross-validation procedures for model selection. Statistics Surveys, 4, 40–79. https://doi.org/10.1214/09-SS054\n\n\nBrowne, M. W. (2000). Cross-validation methods. Journal of Mathematical Psychology, 44(1), 108–132. https://doi.org/10.1006/jmps.1999.1279\n\n\nBusemeyer, J. R., & Wang, Y.-M. (2000). Model comparisons and model selections based on generalization criterion methodology. Journal of Mathematical Psychology, 44(1), 171–189. https://doi.org/10.1006/jmps.1999.1282\n\n\nGelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for bayesian models. Statistics and Computing, 24(6), 997–1016. https://doi.org/10.1007/s11222-013-9416-2\n\n\nNavarro, D. J. (2018). Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection. Computational Brain & Behavior. https://doi.org/10.1007/s42113-018-0019-z\n\n\nPiironen, J., & Vehtari, A. (2017). Comparison of bayesian predictive methods for model selection. Statistics and Computing, 27(3), 711–735. https://doi.org/10.1007/s11222-016-9649-y\n\n\nRaftery, A. E. (1995). Bayesian model selection in social research. Sociological Methodology, 25, 111–163. https://doi.org/10.2307/271063\n\n\nSchwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6(2), 461–464.\n\n\nStone, M. (1977). An asymptotic equivalence of choice of model by cross-validation and Akaike’s criterion. Journal of the Royal Statistical Society. Series B (Methodological), 39(1), 44–47.\n\n\nVehtari, A., & Lampinen, J. (2002). Bayesian model assessment and comparison using cross-validation predictive densities. Neural Computation, 14(10), 2439–2468. https://doi.org/10.1162/08997660260293292\n\n\nZucchini, W. (2000). An introduction to model selection. Journal of Mathematical Psychology, 44(1), 41–61. https://doi.org/10.1006/jmps.1999.1276",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "blast_example.html",
    "href": "blast_example.html",
    "title": "7  A worked example",
    "section": "",
    "text": "7.1 The data\nThis chapter presents a complete worked example of applying the diffusion model to a two-choice task. In doing so, we illustrate the kinds of data that can be modeled within this framework, how to fit the diffusion model to a typical cognitive dataset, and some ways we can use the resulting fits to draw inferences about the cognitive processes behind the choices people made.\nThe data for this example were reported originally by Trueblood et al. (2018). There’s a lot about this study that we won’t get to here, and I encourage you to check out the original paper.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A worked example</span>"
    ]
  },
  {
    "objectID": "blast_example.html#the-data",
    "href": "blast_example.html#the-data",
    "title": "7  A worked example",
    "section": "",
    "text": "7.1.1 Participants and procedures\nParticipants in this study did several blocks of a categorization task. The stimuli used in this task were images of cells that were either indicative of cancer—these are called “blast” cells—or normal—these are “non-blast” cells. The images were further subdivided into “easy” and “hard” versions, based on expert judgments. The image below illustrates the kinds of images that participants would see in this task.\n\n\nCode\nknitr::include_graphics(\"img/blast_example_stimuli.png\")\n\n\n\n\n\n(a) An easy blast image. (b) A hard blast image. (c) An easy non-blast image. (d) A hard non-blast image.\n\n\n\n\nAfter several blocks of training in which participants became familiar with these kinds of images (if they were not already; see below), participants moved on to the categorization task. On each trial of this task, an image was shown. Blast and non-blast images were shown equally often. Easy and hard versions of each type were also shown at the same rates. The participant’s job was to decide whether or not each image was a “blast” cell. The categorization task was itself divided into several blocks, each of which was a different type. We will be looking at data from two types of block: “Accuracy” blocks in which participants were encouraged to take their time and be accurate in their categorization of each image; and “Speed” blocks in which participants were encouraged to make their decisions quickly without regard to accuracy.\nThe participants in this study came from three different groups. Novice participants were just that—typical undergraduate university students who had no prior experience with these kinds of medical images. Inexperienced participants were pathologists who had just begun their training, so while they would be knowledgeable about these kinds of images, they might not have much practice categorizing them. Experienced participants were pathologists who had completed at least four training rotations who would have had plenty of practice dealing with these kinds of images.\nFinally, I note that, in addition to the blast/non-blast categorization task, all participants did a “Novel Object Memory Task” (NOMT) designed to measure their general ability to recognize visual objects, not just medical images of cells.\n\n\n7.1.2 Getting the data\nYou can download the data from this study that we will be examining in this tutorial by running the code below. The first line downloads the data to a file called blast_data.rdata in your current working directory. The second line loads that data into your R environment.\n\n\nCode\ndownload.file(\"https://github.com/gregcox7/choice_rt_models/raw/refs/heads/main/data/blast_data.rdata\", \"blast_data.rdata\")\nload(\"blast_data.rdata\")\n\n\nThe data should now be in your R environment in a data frame called blast_data. Let’s take a look at that data now:\n\n\nCode\nglimpse(blast_data)\n\n\nRows: 21,628\nColumns: 15\n$ dateCompleted    &lt;chr&gt; \"30/6/2017 @ 10:15:24\", \"30/6/2017 @ 10:15:26\", \"30/6…\n$ block            &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ blockType        &lt;fct&gt; Speed, Speed, Speed, Speed, Speed, Speed, Speed, Spee…\n$ trial            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17…\n$ stimulus         &lt;chr&gt; \"nonBlastEasy/SNE_25598565.jpg\", \"blastHard/BL_406213…\n$ difficulty       &lt;fct&gt; Easy, Hard, Easy, Hard, Hard, Easy, Easy, Hard, Easy,…\n$ response         &lt;fct&gt; Non-blast, Blast, Blast, Blast, Non-blast, Non-blast,…\n$ rt               &lt;dbl&gt; 0.662, 0.496, 0.528, 0.431, 0.817, 0.495, 0.540, 0.68…\n$ correct_response &lt;fct&gt; Non-blast, Blast, Blast, Blast, Blast, Non-blast, Non…\n$ bias_shown       &lt;fct&gt; Bias not shown, Bias not shown, Bias not shown, Bias …\n$ subject          &lt;chr&gt; \"M002\", \"M002\", \"M002\", \"M002\", \"M002\", \"M002\", \"M002…\n$ group            &lt;fct&gt; Experienced, Experienced, Experienced, Experienced, E…\n$ nomt_corr        &lt;dbl&gt; 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 9…\n$ nomt_n           &lt;int&gt; 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108…\n$ nomt             &lt;dbl&gt; 0.9166667, 0.9166667, 0.9166667, 0.9166667, 0.9166667…\n\n\nWe can already see the columns that will be most important for us:\n\nblockType: Whether the block instructions emphasized Accuracy or Speed.\ncorrect_response: Whether the image on that trial was a Blast or Non-blast cell.\ndifficulty: Whether the image on that trial was Easy or Hard.\nrt: The response time (RT) in seconds.\nresponse: Whether the participant classified the image as a Blast or Non-blast cell.\nsubject: An identifier for each individual participant.\ngroup: Which of the three groups the participant came from (Experienced, Inexperienced, or Novice).\nnomt: The score on the Novel Object Memory Test (NOMT) for the participant on that trial.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A worked example</span>"
    ]
  },
  {
    "objectID": "blast_example.html#a-single-participant",
    "href": "blast_example.html#a-single-participant",
    "title": "7  A worked example",
    "section": "7.2 A single participant",
    "text": "7.2 A single participant\nIn the next section, we will fit a diffusion model to data from every participant. Before we do that, though, let’s see how to do it for a single participant. We will replicate this procedure for each individual participant in the next section.\n\n7.2.1 A single participant’s data\nI arbitrarily picked the participant with ID “M003” for us to examine. The code below uses the filter function to extract the data from just this participant:\n\n\nCode\nsubj_data &lt;- blast_data %&gt;%\n    filter(subject == \"M003\")\n\n\n\n\n7.2.2 Grouping the trials\nFor the next bit, make sure that you have sourced the wienr_fit_utils.r script:\n\n\nCode\nsource(\"wienr_fit_utils.r\")\n\n\nIf we omit the par argument, we can use the qp_fit function to get the observed response proportions and RT quantiles and make a quantile-probability plot of the observed data. However, to do this, we need to decide how to group the individual trials using the “indexing” trick we used in the last chapter. The way we do this will ultimately inform what diffusion model parameters we will estimate, so it is worth putting in the thought now.\nSpecifically, we need to think about what factors would influence the drift rate of the evidence being accumulated on each trial, what factors would influence how the participant sets their response boundaries on a given trial, and what factors might influence the residual time on each trial. Later, we will also consider how trial-by-trial variability in these three aspects of the model might or might not vary between conditions.\n\n7.2.2.1 What factors influence drift rates?\nThe “evidence” in this task arises from some kind of evaluation of how much the image looks like what the participant thinks of as a “blast” cell versus a “non-blast” cell. In other words, the “evidence” should depend on whether the image on that trial shows a blast or non-blast cell, just like how “evidence” in recognition memory depends on whether the test item is a target or foil. In addition, we would expect “hard” images to yield worse evidence than “easy” images, by definition. These two aspects of the data are reflected in the difficulty and correct_response columns. So we can specify a drift_index based on the interaction between these two factors.\nThe emphasis of the current block—Accuracy vs. Speed—could also impact drift rates (Rae et al., 2014), though exploring that possibility is left as an exercise for the reader.\n\n\n7.2.2.2 What factors influence response boundaries?\nThe response boundaries cannot be influenced by the type of image shown on a trial—if they were, then the participant would already know what kind of image they were seeing! On the other hand, it is reasonable to expect that participants would adjust their response boundaries depending on whether the current block emphasized speed or accuracy. This suggests that we can define a bound_index using the blockType column in the data.\n\n\n7.2.2.3 What factors influence residual time?\nIf residual time reflects only the processes involved in executing the motor response associated with a choice, then we might expect it to be unaffected by any experimental factors. On the other hand, it may be that participants are able to adjust their “response vigor” in light of speed/accuracy emphasis. In addition, it may be that participants can more quickly orient their attention to a stimulus if speed is emphasized. So we can specify a resid_index that also depends on blockType.\n\n\n7.2.2.4 Defining indices\nOn the basis of the considerations above, we will define three indices: one that specifies what conditions can have different drift rates (drift_index), one that specifies what conditions can have different response boundaries (bound_index), and one that specifies what conditions can have different residual time (resid_index):\n\n\nCode\nsubj_data &lt;- subj_data %&gt;%\n    mutate(\n        drift_index = as.numeric(interaction(difficulty, correct_response)),\n        bound_index = as.numeric(blockType),\n        resid_index = as.numeric(blockType)\n    )\n\n\nIt is important to keep in mind that the grouping defined above is not necessarily the “one true grouping”! It is merely meant to give a sense of the kind of things to think about when deciding how different model parameters will be assigned to different conditions.\n\n\n\n7.2.3 Plotting the observed data\nHaving defined our indices, we can pass them to the qp_fit function so that we can make a quantile-probability plot of this participant’s data. Note that I had to\n\n\nCode\nobs_qp &lt;- qp_fit(\n    rt = subj_data$rt,\n    response = subj_data$response,\n    drift_index = subj_data$drift_index,\n    bound_index = subj_data$bound_index,\n    resid_index = subj_data$resid_index\n)\n\n\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response)`\n\n\nWhen making the plot, I found it helpful to “undo” the transformation of the different factors into numerical indices. That “undoing” is the purpose of the two mutate lines.\n\n\nCode\nobs_qp %&gt;%\n    mutate(item_type = factor(drift_index, levels = 1:4, labels = levels(interaction(blast_data$difficulty, blast_data$correct_response)))) %&gt;%\n    mutate(blockType = factor(bound_index, levels = 1:2, labels = levels(blast_data$blockType))) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = item_type, shape = response)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response probability\", y = \"RT quantile\") +\n    facet_wrap(\"blockType\")\n\n\n\n\n\n\n\n\n\nIt is worth noting a few features of these data that are apparent from the quantile-probability plot. First, this participant was indeed faster in the Speed block than the Accuracy block. Even the faster RT’s (the 0.1 quantiles) are faster in the Speed block, supporting the idea that residual time could differ between blocks if residual time represents the minimal time needed to respond. It also looks like this participant was less accurate in the Speed block—at least for Blast images, they had nearly perfect accuracy in the Accuracy block but not in the speed block. This participant was not very good in either block at categorizing Non-blast images. It doesn’t look like difficulty (Easy vs. Hard) made a big difference for this participant in terms of their choice/RT behavior. Finally, it looks like this participant’s errors tended to be a bit slower than their correct responses, suggesting that the diffusion model will need to allow for trial-by-trial variability in drift rates to accommodate these data. This same consideration suggests that we don’t need to assume variability in boundaries (since that would produce fast errors instead).\n\n\n7.2.4 Fitting a diffusion model\nWith all the preliminaries out of the way, let’s try fitting a diffusion model to this participant’s data. This will look just like it did in the last chapter, only with real data instead of simulated data!\nWe have already decided how to assign parameters to trials using the indices we defined in the previous section. We also have good reason to believe that drift rates can vary from trial to trial. We can estimate \\(s_v\\), the standard deviation of the trial-by-trial distribution of drift rates, by including the argument fit_sv = TRUE to the fit_wienr function. We don’t have reason to assume variability in boundaries, which would be reflected in the \\(s_w\\) parameter, but we could do so if we passed fit_sw = TRUE to fit_wienr. Finally, we will allow for variability in residual time by including fit_st0 = TRUE in the function call to fit_wienr.\nFor present purposes, we will only estimate one value of \\(s_v\\) and one value of \\(s_{t_0}\\) parameter, and these values will apply to all trials. If we wanted to allow them to vary, we could pass a sv_index, sw_index, or st0_index vector to the fit_wienr function—these index vectors work just like the drift_index, bound_index, and resid_index vectors we defined above.\nPutting it all together, the code below fits our desired diffusion model to this participant’s choice and RT data.\n\n\nCode\nsubj_fit &lt;- fit_wienr(\n    rt = subj_data$rt,\n    response = subj_data$response,\n    fit_sv = TRUE,\n    fit_sw = FALSE,\n    fit_st0 = TRUE,\n    drift_index = subj_data$drift_index,\n    bound_index = subj_data$bound_index,\n    resid_index = subj_data$resid_index\n)\n\n\nLet’s have a look at the estimated parameter values:\n\n\nCode\nsubj_fit$par\n\n\n      a[1]       a[2]       v[1]       v[2]       v[3]       v[4]       w[1] \n 1.3875689  0.9074497 -0.9075545 -0.3305451  1.5610599  1.6110799  0.6583813 \n      w[2]      t0[1]      t0[2]      sv[1]     st0[1] \n 0.6743305  0.3969458  0.3926721  0.7117295  0.1218538 \n\n\nThe first two parameters are the response caution parameters, with a[1] corresponding to the Accuracy blocks and a[2] to the Speed blocks. As we might expect, the fact thata a[2] \\(&lt;\\) a[1] tells us that this participant was less cautious in the Speed blocks, being more willing to sacrifice accuracy for speed. Skipping ahead to w[1] and w[2], these parameters tell us that this participant was biased toward calling images “Blast” images in both Accuracy and Speed blocks (the response caution and response bias parameters have the same indices). Although we allowed for residual time to vary between Accuracy and Speed blocks, the estimates t0[1] and t0[2] look pretty similar to one another.\nThe drift rate parameters also make some sense: v[1], for easy non-blast images, is negative and has a greater magnitude than v[2], for hard non-blast images. The magnitudes of the drift rates for Blast images, v[3] and v[4], are greater than for the non-blast images and are not too different from one another, in accord with our observation that this participant was better at identifying blast images than non-blasts and that the difficulty of the blast image didn’t seem to matter much.\nFinally, we can see that the drift-rate variability parameter sv[1] and the residual time variability parameter st0[1] are both greater than zero. That said, we did not have strong theoretical reasons to expect these parameters to take any particular value—we just suspected they would be important to account for the data. We can verify that intuition by fitting a model without any trial-by-trial variability and seeing whether AIC and/or BIC still prefers the more complex model with both forms of variability.\n\n\nCode\nsubj_fit_novar &lt;- fit_wienr(\n    rt = subj_data$rt,\n    response = subj_data$response,\n    fit_sv = FALSE,\n    fit_sw = FALSE,\n    fit_st0 = FALSE,\n    drift_index = subj_data$drift_index,\n    bound_index = subj_data$bound_index,\n    resid_index = subj_data$resid_index\n)\n\naic_wvar &lt;- 2 * subj_fit$value + 2 * length(subj_fit$par)\naic_novar &lt;- 2 * subj_fit_novar$value + 2 * length(subj_fit_novar$par)\n\nbic_wvar &lt;- 2 * subj_fit$value + log(nrow(subj_data)) * length(subj_fit$par)\nbic_novar &lt;- 2 * subj_fit_novar$value + log(nrow(subj_data)) * length(subj_fit_novar$par)\n\nc(aic_wvar, aic_novar)\n\n\n[1]  8.878357 64.029238\n\n\nCode\nc(bic_wvar, bic_novar)\n\n\n[1]  56.68559 103.86860\n\n\nBoth AIC and BIC are lower for the model with trial-by-trial variability, suggesting that this additional complexity is warranted in light of the data.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A worked example</span>"
    ]
  },
  {
    "objectID": "blast_example.html#all-the-participants",
    "href": "blast_example.html#all-the-participants",
    "title": "7  A worked example",
    "section": "7.3 All the participants",
    "text": "7.3 All the participants\nHaving fit a diffusion model to one participant, we will now replicate that procedure for every participant. First, it will be convenient to define our three index vectors using the whole dataset:\n\n\nCode\nblast_data &lt;- blast_data %&gt;%\n    mutate(\n        drift_index = as.numeric(interaction(difficulty, correct_response, drop = TRUE)),\n        bound_index = as.numeric(blockType),\n        resid_index = as.numeric(blockType)\n    )\n\n\nNow comes the big stuff. We will write a for loop that does the following for each participant:\n\nExtracts that participant’s data from the complete dataset.\nFits a diffusion model to that participant’s data.\nExtracts the estimated parameters for that participant and saves them in a data frame called model_pars. This is so we can examine the estimated parameters later.\nComputes both observed and model-produced RT quantiles and response probabilities and saves them in a data frame called model_qp. This is so we can verify that the model is fitting the data.\n\nAll of that is accomplished with the following chunk of R code, which begins by using the unique function to extract all the unique participant ID’s in the dataset. Note that this is used to define what the for loop iterates over. This will take a while to run, but patience is a virtue!\n\n\nCode\nsubj_to_fit &lt;- unique(blast_data$subject)\n\nmodel_pars &lt;- c()\nmodel_qp &lt;- c()\n\nfor (id in subj_to_fit) {\n    this_subj_data &lt;- blast_data %&gt;%\n        filter(subject == id)\n    \n    this_fit &lt;- fit_wienr(\n        rt = this_subj_data$rt,\n        response = (this_subj_data$response == \"Blast\") + 1,\n        fit_sv = TRUE,\n        fit_sw = FALSE,\n        fit_st0 = TRUE,\n        drift_index = this_subj_data$drift_index,\n        bound_index = this_subj_data$bound_index,\n        resid_index = this_subj_data$resid_index\n    )\n    \n    model_pars &lt;- rbind(\n        model_pars,\n        tibble(subject = id, group = this_subj_data$group[1], par_name = names(this_fit$par), val = this_fit$par) %&gt;% extract(par_name, into = c(\"par\", \"index\"), regex = \"(.+)\\\\[(.+)\\\\]\")\n    )\n    \n    this_qp &lt;- qp_fit(\n        rt = this_subj_data$rt,\n        response = (this_subj_data$response == \"Blast\") + 1,\n        par = this_fit$par,\n        drift_index = this_subj_data$drift_index,\n        bound_index = this_subj_data$bound_index,\n        resid_index = this_subj_data$resid_index\n    ) %&gt;%\n        mutate(subject = id, group = this_subj_data$group[1])\n    \n    model_qp &lt;- rbind(\n        model_qp,\n        this_qp\n    )\n}\n\n\n\n7.3.1 Comparing parameters between groups\nOnce we have our parameter estimates safely stored in model_pars, we can visualize the resulting estimates using color to distinguish between the three groups. The plot below was made by using tiny, slightly faded points for each individual participant (note the alpha = 0.5, size = 0.5 settings in the geom_point line). Overlaid on those is a big point with error bars that shows the mean and 95% confidence interval for the mean, computed separately for each group.\n\n\nCode\nmodel_pars %&gt;%\n    ggplot(aes(x = index, y = val, color = group, shape = group)) +\n    geom_point(position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.4), alpha = 0.5, size = 0.5) +\n    stat_summary(geom = \"pointrange\", fun.data = mean_cl_boot, position = position_dodge(width = 0.4)) +\n    labs(x = \"Index\", y = \"Estimated value\", color = \"Group\") +\n    facet_wrap(\"par\", scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n7.3.1.1 Response caution\nLet’s focus first on a, the response caution parameter. As we know, a[1] corresponds to the Accuracy blocks while a[2] corresponds to the Speed blocks. It certainly looks like participants, on average, had lower response caution in the Speed blocks than in the Accuracy blocks. It also looks like the more experienced participants tended to have greater response caution in both block types.\nTo get some statistical evidence for differences between groups and between conditions, we can use our old friend, the Analysis of Variance (ANOVA). While you might normally think of applying ANOVA to observed values, like mean response time or accuracy, it can be applied just as well to estimated parameter values. In both cases, we have a single value for each participant in each condition and we are testing the null hypothesis that the parameter estimate does not differ, on average, between conditions/groups.\nTo do ANOVA, I’ll use the afex R package and make sure to run its set_sum_contrasts() function (by default, R uses “treatment” contrasts, which are not always appropriate).\n\n\nCode\nlibrary(afex)\n\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nRegistered S3 methods overwritten by 'car':\n  method                          from\n  influence.merMod                lme4\n  cooks.distance.influence.merMod lme4\n  dfbeta.influence.merMod         lme4\n  dfbetas.influence.merMod        lme4\n\n\n************\nWelcome to afex. For support visit: http://afex.singmann.science/\n\n\n- Functions for ANOVAs: aov_car(), aov_ez(), and aov_4()\n- Methods for calculating p-values with mixed(): 'KR', 'S', 'LRT', and 'PB'\n- 'afex_aov' and 'mixed' objects can be passed to emmeans() for follow-up tests\n- NEWS: library('emmeans') now needs to be called explicitly!\n- Get and set global package options with: afex_options()\n- Set orthogonal sum-to-zero contrasts globally: set_sum_contrasts()\n- For example analyses see: browseVignettes(\"afex\")\n************\n\n\n\nAttaching package: 'afex'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nCode\nset_sum_contrasts()\n\n\nsetting contr.sum globally: options(contrasts=c('contr.sum', 'contr.poly'))\n\n\nNow, we can use the aov_ez function to do the ANOVA on the a parameter estimates.\n\n\nCode\naov_ez(\n    id = \"subject\",      # Specify the name of the column that identifies unique participants\n    dv = \"val\",          # Specify the name of the column that contains the values to be analyzed\n    data = model_pars %&gt;% filter(par == \"a\"), # The data for this ANOVA is stored in \"model_pars\", but we are only interested in the estimates of the \"a\" parameter\n    between = \"group\",   # Specify the name of the column that identifies between-subject comparisons\n    within = \"index\"     # Specify the name of the column that identifies within-subject comparisons\n)\n\n\nAnova Table (Type 3 tests)\n\nResponse: val\n       Effect    df  MSE          F  ges p.value\n1       group 2, 52 0.15  17.86 *** .264   &lt;.001\n2       index 1, 52 0.14 129.70 *** .544   &lt;.001\n3 group:index 2, 52 0.14   9.23 *** .145   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAs we can see, there is a main effect of “group”, consistent with our observation that more experienced participants had higher response caution. There is also a main effect of “index”, consistent with our observation that participants tended to set lower response caution in Speed blocks. Finally, there is a significant interaction between “group” and “index”, although it looks from the graph above that this is likely to be a “fan” interaction, with a bigger increase from Speed to Accuracy for the more experienced participants.\n\n\n7.3.1.2 Drift rates\nNow, let’s consider the drift rate parameters. Again, we will use ANOVA to look for statistical evidence of differences in drift rates between groups and between conditions. Things are a little more complicated, though, because drift rate was allowed to vary by both difficulty and image type (blast vs. non-blast). To properly specify the ANOVA, then, we should “undo” the drift rate indices back into those original two factors. That’s what the mutate lines in the data specification do in the code below.\n\n\nCode\naov_ez(\n    id = \"subject\",\n    dv = \"val\",\n    data = model_pars %&gt;%\n        filter(par == \"v\") %&gt;%\n        mutate(\n            difficulty = factor(index, levels = 1:4, labels = c(\"Easy\", \"Hard\", \"Easy\", \"Hard\")),\n            correct_response = factor(index, levels = 1:4, labels = c(\"Non-blast\", \"Non-blast\", \"Blast\", \"Blast\"))),\n    between = \"group\",\n    within = c(\"difficulty\", \"correct_response\")\n)\n\n\nAnova Table (Type 3 tests)\n\nResponse: val\n                             Effect    df  MSE          F   ges p.value\n1                             group 2, 52 1.65       0.02 &lt;.001    .980\n2                        difficulty 1, 52 0.25  68.87 ***  .052   &lt;.001\n3                  group:difficulty 2, 52 0.25     3.24 *  .005    .047\n4                  correct_response 1, 52 3.37 252.60 ***  .728   &lt;.001\n5            group:correct_response 2, 52 3.37  21.45 ***  .313   &lt;.001\n6       difficulty:correct_response 1, 52 0.83 164.02 ***  .300   &lt;.001\n7 group:difficulty:correct_response 2, 52 0.83  16.01 ***  .077   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nThe ANOVA finds evidence for significant differences for all but “group” on its own. However, this analysis is a bit misleading in that, as you’ll recall, drift rates for non-blast images tend to be negative while drift rates for blast images tend to be positive. We may be more interested in analyzing how drift rates toward the correct response boundary may or may not differ between groups/conditions.\nTo do this, we can add another mutate line that reverses the sign of the estimated drift rates for non-blast images:\n\n\nCode\naov_ez(\n    id = \"subject\",\n    dv = \"val\",\n    data = model_pars %&gt;%\n        filter(par == \"v\") %&gt;%\n        mutate(\n            difficulty = factor(index, levels = 1:4, labels = c(\"Easy\", \"Hard\", \"Easy\", \"Hard\")),\n            correct_response = factor(index, levels = 1:4, labels = c(\"Non-blast\", \"Non-blast\", \"Blast\", \"Blast\")),\n            val = if_else(correct_response == \"Blast\", val, -val)\n    ),\n    between = \"group\",\n    within = c(\"difficulty\", \"correct_response\")\n)\n\n\nAnova Table (Type 3 tests)\n\nResponse: val\n                             Effect    df  MSE          F   ges p.value\n1                             group 2, 52 3.37  21.45 ***  .313   &lt;.001\n2                        difficulty 1, 52 0.83 164.02 ***  .300   &lt;.001\n3                  group:difficulty 2, 52 0.83  16.01 ***  .077   &lt;.001\n4                  correct_response 1, 52 1.65       0.31  .002    .578\n5            group:correct_response 2, 52 1.65       0.02 &lt;.001    .980\n6       difficulty:correct_response 1, 52 0.25  68.87 ***  .052   &lt;.001\n7 group:difficulty:correct_response 2, 52 0.25     3.24 *  .005    .047\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nNow the ANOVA correctly detects a main effect of group that was obscured in the previous analysis, among other things.\n\n\n7.3.1.3 Individual differences\nFinally, recall that each participant also completed the “NOMT”, a test of general visual object processing ability. It would be reasonable to ask whether participants who have high NOMT scores also tend to have higher drift rates toward the correct response boundary. To analyze this, we need to first extract the NOMT scores for each participant, append them to the model parameter estimates, and include NOMT as a covariate in the ANOVA. For interpretability, I also “center” the NOMT scores by subtracting the group mean.\n\n\nCode\n# Extract NOMT scores and center them.\nnomt_scores &lt;- blast_data %&gt;%\n    group_by(group, subject) %&gt;%\n    summarize(nomt = first(nomt)) %&gt;%\n    mutate(nomt_centered = nomt - mean(nomt))\n\n\n`summarise()` has grouped output by 'group'. You can override using the\n`.groups` argument.\n\n\nCode\n# Append the NOMT scores to the parameter estimates\nmodel_pars_nomt &lt;- left_join(model_pars, nomt_scores)\n\n\nJoining with `by = join_by(group, subject)`\n\n\nCode\n# Run the same ANOVA as above, now including `nomt_centered` as a `covariate`\naov_ez(\n    id = \"subject\",\n    dv = \"val\",\n    data = model_pars_nomt %&gt;%\n        filter(par == \"v\") %&gt;%\n        mutate(\n            difficulty = factor(index, levels = 1:4, labels = c(\"Easy\", \"Hard\", \"Easy\", \"Hard\")),\n            correct_response = factor(index, levels = 1:4, labels = c(\"Non-blast\", \"Non-blast\", \"Blast\", \"Blast\")),\n            val = if_else(correct_response == \"Blast\", val, -val)\n    ),\n    between = \"group\",\n    within = c(\"difficulty\", \"correct_response\"),\n    covariate = \"nomt_centered\",\n    factorize = FALSE  # This last setting is necessary to ensure that \"nomt_centered\" isn't accidentally treated like a factor\n)\n\n\nAnova Table (Type 3 tests)\n\nResponse: val\n                                      Effect    df  MSE          F   ges\n1                                      group 2, 51 3.05  23.64 ***  .334\n2                              nomt_centered 1, 51 3.05     6.33 *  .063\n3                                 difficulty 1, 51 0.68 199.27 ***  .320\n4                           group:difficulty 2, 51 0.68  19.46 ***  .084\n5                   nomt_centered:difficulty 1, 51 0.68   12.18 **  .028\n6                           correct_response 1, 51 1.67       0.31  .002\n7                     group:correct_response 2, 51 1.67       0.02 &lt;.001\n8             nomt_centered:correct_response 1, 51 1.67       0.34  .002\n9                difficulty:correct_response 1, 51 0.24  71.48 ***  .057\n10         group:difficulty:correct_response 2, 51 0.24     3.36 *  .006\n11 nomt_centered:difficulty:correct_response 1, 51 0.24     2.97 +  .003\n   p.value\n1    &lt;.001\n2     .015\n3    &lt;.001\n4    &lt;.001\n5     .001\n6     .581\n7     .980\n8     .562\n9    &lt;.001\n10    .043\n11    .091\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nIn fact, it looks like NOMT not only has a main effect on drift rates, it also interacts with difficulty, suggesting that group differences alone do not account for individual differences in performance on this task—categorizing images of cells also seems to depend on general object processing ability.\n\n\n\n7.3.2 Visualizing model fit\nFinally, we come to the most challenging section: How to visualize the quality of the model fit. We could, of course, produce quantile-probability plots for each participant separately, but this would only be feasible with very few participants.\nInstead, the code below plots the observed and fitted RT quantiles and response probabilities averaged over the participants in each group. This is not meant to be the final word, but just a way to verify that the model is close to the data and that it is accurately reproducing the important aspects of the data.\n\n\nCode\nmodel_qp %&gt;%\n    mutate(\n        blockType = factor(bound_index, labels = levels(blast_data$blockType)),\n        item_type = factor(drift_index, labels = levels(interaction(blast_data$difficulty, blast_data$correct_response, sep = \" \", drop = T)))\n    ) %&gt;%\n    group_by(group, blockType, item_type, response, source, rt_p) %&gt;%\n    summarize(rt_q = mean(rt_q, na.rm = TRUE), p_resp = mean(p_resp, na.rm = TRUE)) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = item_type)) +\n    geom_point(aes(shape = source), fill = \"white\") +\n    scale_linetype_manual(values = c(\"Observed\" = \"solid\", \"Fitted\" = \"dashed\")) +\n    scale_shape_manual(values = c(\"Observed\" = 16, \"Fitted\" = 21)) +\n    facet_grid(blockType ~ group, scales = \"free_y\")\n\n\n`summarise()` has grouped output by 'group', 'blockType', 'item_type',\n'response', 'source'. You can override using the `.groups` argument.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's linetype values.\n\n\n\n\n\n\n\n\n\nThe upshot is that it looks like the model is, at least on average, doing a very good job of capturing the response proportion and a pretty good one capturing the RT quantiles. That said, some of the misfits for the highest and lowest quantiles (see, e.g., the green points in the “Speed” conditions) may be due to sampling error, as discussed earlier.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A worked example</span>"
    ]
  },
  {
    "objectID": "blast_example.html#exercises",
    "href": "blast_example.html#exercises",
    "title": "7  A worked example",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\nRun an ANOVA analysis on other estimated model parameters, like bias (w), residual time (t0), and the two variability parameters (sv and st0). Do you find evidence for differences, on average, between groups or between conditions (for sv and st0, you can only compare between groups)?\nUsing the sv_index and st0_index parameters, modify the diffusion model we used above so that drift rate variability and residual time variability can also vary by block type. Does this more complex model provide a better account of the data, as scored by either AIC or BIC?\nModify the way we defined drift_index so that it allows drift rates to vary by blockType as well as difficulty and correct_response (this will just involve adding the additional factor to the interaction function). Leave everything else the same as we specified it in the main text and re-run the model fits. Using an ANOVA on the drift rates, do you find evidence that drift rates vary as a function of block type? Note that, to do this analysis, you will have to “reverse-engineer” the index number to recover the correct factor combination, like we did in the main text. The basic idea is shown below, where you’ll have to figure out how to correctly assign the labels in the mutate line below.\n\n\n\nCode\naov_ez(\n    id = \"subject\",\n    dv = \"val\",\n    data = model_pars %&gt;%\n        filter(par == \"v\") %&gt;%\n        mutate(\n            difficulty = factor(index, levels = 1:8, labels = c(___)),\n            correct_response = factor(index, levels = 1:8, labels = c(___)),\n            blockType = factor(index, levels = 1:8, labels = c(___)),\n            val = if_else(correct_response == \"Blast\", val, -val)\n    ),\n    between = \"group\",\n    within = c(\"difficulty\", \"correct_response\")\n)\n\n\n\n\n\n\nRae, B., Heathcote, A., Donkin, C., Averell, L., & Brown, S. (2014). The hare and the tortoise: Emphasizing speed can change the evidence used to make decisions. Journal of Experimental Psychology: Learning, Memory, and Cognition, 40(5), 1226–1243.\n\n\nTrueblood, J. S., Holmes, W. R., Seegmiller, A. C., Douds, J., Compton, M., Szentirmai, E., Woodruff, M., Huang, W., Stratton, C., & Eichbaum, Q. (2018). The impact of speed and bias on the cognitive processes of experts and novices in medical image decision-making. Cognitive Research: Principles and Implications, 3, 1–14.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A worked example</span>"
    ]
  },
  {
    "objectID": "accumulator_models.html",
    "href": "accumulator_models.html",
    "title": "8  Accumulator models of choice and response time",
    "section": "",
    "text": "8.1 Race model simulations\nSo far, we have focused on models that are designed to explain how people choose between two opposing options. We built a random walk model and a diffusion model and then saw how we could fit a diffusion model to choice/RT data. For the latter purpose, we relied on the WienR package to calculate the (log-)likelihood of making a particular choice at a particular time. This is because the math for calculating those likelihoods is not trivial, requiring some bespoke numerical approximations that are beyond the scope of this course. Nonetheless, our cognitive models so far have touched on several major themes that are shared by all cognitive models:\nIn this chapter, we will build a new class of model that allows us to address more complex choice scenarios. We will not only write our own simulation code, like we did before; we will also write our own likelihood function so we can get a bit more insight into how those work. The specific category of model we will address are called accumulator models.\nAccumulator models are very similar to the random walk and diffusion models of choice that we have seen already. Like those models, they aim to explain choices and response times in terms of an evidence accumulation process. The difference is in how accumulator models represent the decision-maker’s momentary states of evidence. Instead of representing evidence as a single value representing the “balance” of evidence between two options, accumulator models represent evidence in terms of multiple values. In some accumulator models, each option available to a decision maker is associated with its own evidence value. But it is also possible to make accumulator models more general than that. For example, perhaps each feature of an item (like the color and shape of an object) are processed in separate channels, each of which is associated with an evidence value. In these more general models, it is possible to formulate different decision rules by specifying how values map onto making different choices.\nThe preceding overview may seem a bit abstract, so let’s consider a concrete situation that will let us see how accumulator models can be used model different kinds of tasks with different characteristics and decision rules. This situation is a visual search task. In a visual search task, a participant is shown a search array consisting of items; one or more of those items could be a “target” for which the participant is supposed to search. For example, an array could consist of colored shapes, with the target defined as a red square.\nIn the first version of the visual search task we will consider, let’s say that each array shown to the participant contains exactly one target item and the job of the participant is to identify which item is the target. Let’s say that the participant expresses their choice by making a saccade (an eye movement) so that they fixate the item they think is the target. (Alternatively, they could make their selection by clicking on the item if using a mouse or tapping on the item if using a touchscreen.) The point is that, unlike in the random walk and diffusion models we’ve considered so far, the participant may have more than two options if there are more than two items in the array. A similar situation could arise when deciding which of several products to buy, for example.\nThe first kind of accumulator model we will build is called a race model for the simple reason that it models making a decision as a race between different evidence accumulation processes. Each option is associated with a level of evidence which can fluctuate over time as evidence that either favors or disfavors that option gets accumulated. Whichever accumulator first reaches a threshold level of evidence then determines the outcome of the choice, and the response time depends on how long the “winning” accumulator needed to reach threshold (plus residual time, of course).\nAs we will see, each accumulator can be modeled as a diffusion or random walk process. The only difference is that instead of stopping when it hits either an upper or lower boundary, there is only one upper boundary that acts as the “finishing line”. In general, different accumulators may have different threshold levels.\nThus, accumulator models instantiate the same four theoretical constructs as random walk/diffusion models: Decisions still depend on accumulating evidence regarding the available options. Decisions depend on accumulating until a threshold amount of evidence is reached, such that higher thresholds amount to greater response caution. However, because thresholds can be set at different levels for different options, there is the possibility of response bias. Finally, we must acknowledge the ever-present residual time that contributes to observed response times.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Accumulator models of choice and response time</span>"
    ]
  },
  {
    "objectID": "accumulator_models.html#race-model-simulations",
    "href": "accumulator_models.html#race-model-simulations",
    "title": "8  Accumulator models of choice and response time",
    "section": "",
    "text": "8.1.1 A single accumulator\nTo begin, let’s simulate just a single accumulator. Although there are a number of ways to do this (and we will consider some variations later on), we will make a minor modification to the diffusion_sim function we wrote already. We will simply remove the lower boundary, so that the process will stop only when hitting its upper threshold. The revised function is shown below, along with comments that indicate the meaning of each of the function arguments.\n\n\nCode\n# Function arguments\n# v: \"drift rate\", the average evidence in support of this option\n# a: threshold level of evidence\n# t0: residual time\n# dt: how long each tiny time interval lasts\n# t_max: optional \"cut off\" time to stop accumulating\naccum_sim &lt;- function(v = 0, a = 1, t0 = 0.2, dt = 0.01, t_max = Inf) {\n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; a & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = v * dt, sd = sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nLet’s see what the behavior of one of these single accumulators looks like. The chunk of code below simulates 1000 trials using a few different settings of the drift rate parameter v and the threshold parameter a. Note that because there is only one threshold and only one accumulator, there are no “choices” being made yet. We are just simulating how long it takes a single accumulator to reach its threshold.\n\n\nCode\nN_sims &lt;- 1000\n\nparamsToSim &lt;- expand_grid(a = c(1, 2), v = c(1, 2, 3))\n\nsim_rt_results &lt;- c()\n\nfor (param_index in 1:nrow(paramsToSim)) {\n    for (sim_index in 1:N_sims) {\n        this_sim &lt;- accum_sim(\n            v = paramsToSim$v[param_index],\n            a = paramsToSim$a[param_index]\n        )\n        \n        sim_rt_results &lt;- rbind(\n            sim_rt_results,\n            tibble(\n                sim_index = sim_index,\n                v = paramsToSim$v[param_index],\n                a = paramsToSim$a[param_index],\n                finishing_time = max(this_sim$t)\n            )\n        )\n    }\n}\n\n# Plot conditional RT distributions\ndens_plot &lt;- sim_rt_results %&gt;%\n    ggplot(aes(x = finishing_time, color = v, linetype = factor(a), group = interaction(v, a))) +\n    geom_density() +\n    scale_color_gradient2(mid = \"#bbbbbb\") +\n    coord_cartesian(xlim = c(0, 5)) +\n    labs(x = \"Finishing time\", y = \"Density\", color = \"Drift rate (v)\", linetype = \"Threshold (a)\")\n\ncdf_plot &lt;- sim_rt_results %&gt;%\n    ggplot(aes(x = finishing_time, color = v, linetype = factor(a), group = interaction(v, a))) +\n    stat_ecdf() +\n    scale_color_gradient2(mid = \"#bbbbbb\") +\n    coord_cartesian(xlim = c(0, 5), ylim = c(0, 1)) +\n    labs(x = \"Finishing time\", y = \"Cumulative\", color = \"Drift rate (v)\", linetype = \"Threshold (a)\")\n\nprint(\n    dens_plot + cdf_plot + plot_layout(nrow = 1, guides = \"collect\")\n)\n\n\n\n\n\n\n\n\n\nThe code below plots two graphs. The left graph shows the probability density function (PDF) of the simulated finishing times for each combination of drift rate (v) and threshold (a). The right graph shows the cumulative distribution function (CDF) of the simulated finishing times. The left graph shows the probability of finishing at time \\(t\\). The right graph shows the probability of having finished at or before time \\(t\\). Both the PDF and the CDF will turn out to be important when we simulate a race between multiple accumulators.\nFor now, though, we can appreciate two things, which are fairly intuitive:\n\nThe greater the drift rate v, the faster the accumulator is to finish.\nThe greater the threshold a, the slower the accumulator is to finish.\n\n\n\n8.1.2 A race between independent accumulators\nNow let’s introduce a second accumulator to the mix. To return to our visual search example, this would correspond to a search array with two items. For now, we will assume that each accumulator operates independently of the other, meaning that the evidence accumulated by each accumulator is not affected by the evidence level of the other. Shortly, we will relax this assumption for our simulations. Unfortunately, we will have to keep the independence assumption when we later write a function that computes the likelihood of a particular accumulator winning at a particular time. When accumulators can interact with one another, computing the likelihood is a lot harder and requires some bespoke numerical methods that are beyond this course.\nFor now, we need to modify our simulation code so that it treats x not as a single number, but as a vector where each element corresponds to the evidence level in each accumulator. As a corollary to that, we will also need to treat the v and a arguments as vectors, since they may differ between accumulators. That said, the first three lines of code in the function will repeat the values of v and a if necessary, so we can be lazy and provide just a single value if we want it to be equal across accumulators.\n\n\nCode\n# Function arguments\n# v: \"drift rate\", the average evidence in support of each option\n# a: threshold level of evidence for each accumulator\n# t0: residual time\n# dt: how long each tiny time interval lasts\n# t_max: optional \"cut off\" time to stop accumulating\nrace_sim &lt;- function(v = 0, a = 1, t0 = 0.2, dt = 0.01, t_max = Inf) {\n    n_accum &lt;- max(length(v), length(a))\n    \n    v &lt;- rep(v, n_accum)[1:n_accum]\n    a &lt;- rep(a, n_accum)[1:n_accum]\n    \n    x &lt;- rep(0, n_accum)\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (all(x &lt; a) & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = n_accum, mean = v * dt, sd = sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- rbind(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    to_return &lt;- cbind(t_record, x_record)\n    \n    colnames(to_return) &lt;- c(\"t\", paste0(\"x\", 1:n_accum))\n    \n    return(as_tibble(to_return))\n}\n\n\nNote that each time we run the simulation, we get the trajectory of accumulated evidence for both accumulators, labeled x1 and x2.\n\n\nCode\nrace_sim(v = c(1, 0), a = 1, t0 = 0.2)\n\n\n# A tibble: 35 × 3\n       t     x1       x2\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.2  0       0      \n 2  0.21 0.0595 -0.114  \n 3  0.22 0.141  -0.0747 \n 4  0.23 0.282   0.00144\n 5  0.24 0.392  -0.125  \n 6  0.25 0.504  -0.0206 \n 7  0.26 0.383  -0.0453 \n 8  0.27 0.353  -0.0878 \n 9  0.28 0.464  -0.221  \n10  0.29 0.555  -0.162  \n# ℹ 25 more rows\n\n\nThis makes for some lovely plots:\n\n\nCode\nrace_sim(v = c(1, 0), a = 1, t0 = 0.2) %&gt;%\n    pivot_longer(matches(\"x(\\\\d+)\"), names_to = \"accumulator\", values_to = \"x\") %&gt;%\n    ggplot(aes(x = t, y = x, color = accumulator)) +\n    geom_line() +\n    geom_hline(yintercept = 1, linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", color = \"Accumulator\")\n\n\n\n\n\n\n\n\n\nWe can also easily simulate situations with more than two options. For example, maybe there are four items in the search array. Say that item 1 is the search target, a red square. Item 2 is a red circle—it matches the target in color but not shape. Item 3 is a blue square—it matches the target in shape but not color. Finally, item 4 is a blue circle, which matches in neither color nor shape. If “evidence” reflects the degree of match between an item and the search target, then it would make sense for the drift rate for item 1 to be the greatest, the drift rate for item 4 to be the smallest, and the drift rates for items 2 and 3 to be intermediate (whether the drift rate for item 2 is greater than for item 3 may depend on how much attention is devoted to each feature). I picked some sensible-seeming values for those drift rates in the simulation below, also assuming the same threshold of a = 2 across all accumulators.\n\n\nCode\nrace_sim(v = c(1, 0.4, 0.2, 0), a = 2, t0 = 0.2) %&gt;%\n    pivot_longer(matches(\"x(\\\\d+)\"), names_to = \"accumulator\", values_to = \"x\") %&gt;%\n    ggplot(aes(x = t, y = x, color = accumulator)) +\n    geom_line() +\n    geom_hline(yintercept = 2, linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", color = \"Accumulator\")\n\n\n\n\n\n\n\n\n\nAlthough the graph above is just one simulated trial, it gives a visual sense of how evidence accumulation works in a race model. It also shows how the option with the highest drift rate (accumulator x1) need not always win the race!\nOf course, to get a sense of the full distribution of behavior this model predicts, we can return to our old friend, the Quantile-Probability plot. The code below simulates 1000 trials using the same parameter values as those used in the plot above.\n\n\nCode\nn_sims &lt;- 1000\n\nsim_results &lt;- c()\n\nfor (i in 1:n_sims) {\n    current_result &lt;- race_sim(c(1, 0.4, 0.2, 0), a = 2, t0 = 0.2)\n    \n    # Extract just the choice and RT\n    rt &lt;- current_result$t[nrow(current_result)]\n    choice &lt;- which.max(current_result[nrow(current_result), 2:ncol(current_result)])\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        tibble(\n            sim_index = i,\n            rt = rt,\n            choice = choice\n        )\n    )\n}\n\n# Plot conditional RT distributions\nsim_results %&gt;%\n    ggplot(aes(x = rt, color = factor(choice))) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\nsim_choice_p &lt;- sim_results %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- sim_results %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = factor(choice))) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", color = \"Choice\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\nThere’s a couple things worth noting in the simulations above. First, looking at the QP plot, it is clear that accumulators with higher drift rates tend to “win” the race more often—the target item (item 1) gets chosen more often than the other items. However, you may also have noticed that the RT distributions look pretty similar regardless of which item was actually chosen in the end. This is somewhat reminiscent of how error and correct RT distributions were the same in the diffusion/random walk until we introduced trial-by-trial variability in drift rates.\nThe next set of simulations illustrate how we can model response bias by allowing the threshold to vary between accumulators. In the simulations below, I use four accumulators representing the factorial combination of two values of drift rate (v is either 1 or 0) and two values of threshold (a is either 1 or 2).\n\n\nCode\nn_sims &lt;- 1000\n\nsim_results &lt;- c()\n\nfor (i in 1:n_sims) {\n    current_result &lt;- race_sim(c(1, 1, 0, 0), a = c(2, 1, 2, 1), t0 = 0.2)\n    \n    # Extract just the choice and RT\n    rt &lt;- current_result$t[nrow(current_result)]\n    choice &lt;- which.max(current_result[nrow(current_result), 2:ncol(current_result)])\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        tibble(\n            sim_index = i,\n            rt = rt,\n            choice = choice\n        )\n    )\n}\n\n# Plot conditional RT distributions\nsim_results %&gt;%\n    ggplot(aes(x = rt, color = factor(choice))) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\nsim_choice_p &lt;- sim_results %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- sim_results %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = factor(choice))) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", color = \"Choice\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\nThe results show that, as would make sense, accumulators with lower thresholds (2 and 4) tend to win more quickly than those with higher thresholds (1 and 3). The simulations also illustrate a tendency which we are about to explore: Accumulators with lower drift rates (3 and 4) tend to be associated with faster responses.\nTo get a better sense of what’s going on, let’s run another set of simulations. In the following, we assume only two items, a target and a distractor. The target will always have a drift rate of 1 but we will vary the drift rate associated with the distractor. This might happen, for example, if we make the distractor progressively more similar to the target. We will keep a threshold of 2 on both accumulators and residual time of 0.2.\n\n\nCode\nn_sims &lt;- 1000\n\nsim_results &lt;- c()\n\nfor (distractor_drift in c(0, 0.5, 0.9)) {\n    for (i in 1:n_sims) {\n        current_result &lt;- race_sim(c(1, distractor_drift), a = 2, t0 = 0.2)\n        \n        # Extract just the choice and RT\n        rt &lt;- current_result$t[nrow(current_result)]\n        choice &lt;- which.max(current_result[nrow(current_result), 2:ncol(current_result)])\n        \n        # \"Bind\" the current simulation to the ongoing record of simulation results\n        sim_results &lt;- rbind(\n            sim_results,\n            tibble(\n                distractor_drift = distractor_drift,\n                sim_index = i,\n                rt = rt,\n                choice = choice\n            )\n        )\n    }\n}\n\n# Quantile-probability plot\nsim_choice_p &lt;- sim_results %&gt;%\n    group_by(distractor_drift, choice) %&gt;%\n    count() %&gt;%\n    group_by(distractor_drift) %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- sim_results %&gt;%\n    group_by(distractor_drift, choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = factor(choice))) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    facet_wrap(\"distractor_drift\", labeller = label_both) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", color = \"Choice\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(distractor_drift, choice)`\n\n\n\n\n\n\n\n\n\nAs the drift rate for the distractor (item 2) increases, two things happen: The probability of choosing the distractor increases and response times decrease. Meanwhile, if you choose the distractor, you tend to do so a bit faster than when you pick the target.\nSpeeding up when the competition gets more heated—that seems a bit counterintuitive, no? However, it is a consequence of statistical facilitation (Raab, 1962), which is a general phenomenon exhibited by race models. It happens because, for an accumulator to “win” the race, it must have been faster than its competition. Therefore, when the distractor has a high drift rate, the target must be even faster in order to win.\nFor the same reason, if we introduce more distractors—and therefore more “runners” in the race—the race model also produces faster responses. This is shown in the simulations below, which vary the number of distractors in the array, assuming each distractor has a drift rate of 0.1.\n\n\nCode\nn_sims &lt;- 1000\n\nsim_results &lt;- c()\n\nfor (num_distractors in c(1, 3, 7)) {\n    for (i in 1:n_sims) {\n        current_result &lt;- race_sim(c(1, rep(0.1, num_distractors)), a = 2, t0 = 0.2)\n        \n        # Extract just the choice and RT\n        rt &lt;- current_result$t[nrow(current_result)]\n        choice &lt;- which.max(current_result[nrow(current_result), 2:ncol(current_result)])\n        \n        # \"Bind\" the current simulation to the ongoing record of simulation results\n        sim_results &lt;- rbind(\n            sim_results,\n            tibble(\n                num_distractors = num_distractors,\n                sim_index = i,\n                rt = rt,\n                # Note that I don't bother to keep track of which distractor was selected\n                choice = factor(ifelse(choice == 1, \"Target\", \"Distractor\"), levels = c(\"Target\", \"Distractor\"))\n            )\n        )\n    }\n}\n\n# Quantile-probability plot\nsim_choice_p &lt;- sim_results %&gt;%\n    group_by(num_distractors, choice) %&gt;%\n    count() %&gt;%\n    group_by(num_distractors) %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- sim_results %&gt;%\n    group_by(num_distractors, choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = factor(choice))) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    facet_wrap(\"num_distractors\", labeller = label_both) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", color = \"Choice\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(num_distractors, choice)`\n\n\n\n\n\n\n\n\n\nFirst, it is clear that introducing more distractors means the probability of selecting the target decreases, which makes intuitive sense. But paradoxically, making the decision harder and more error-prone by introducing more distractors has sped up the model. This is statistical facilitation once again—for any single accumulator to “win” the race it has to be faster than all of its competitors.\nFinally, it is worth remarking once again that errors (i.e., selecting a distractor) take about the same amount of time as correct responses, regardless of the number of distractors.\n\n\n8.1.3 Introducing competition\nTo enable the race model to be a bit more flexible—and potentially more psychologically realistic—we will introduce two kinds of competition between the accumulators. As noted above, some forms of competition do not permit easy computation of likelihoods, but they are still important from a theoretical standpoint.\n\n8.1.3.1 Feedforward competition\nThe first kind of competition we introduce is feedforward competition. This mechanism treats the drift rate for an accumulator as something that can be inhibited by the drift rates for other accumulators.\nTo get mathy about it, let \\(v_i\\) stand for the drift rate to accumulator \\(i\\), assuming \\(N\\) total accumulators. Then the inhibited drift rate \\(v_i'\\) is\n\\[\nv_i' = v_i - \\alpha \\sum_{i \\neq j}^n v_j\n\\]\nwhere \\(\\sum_{i \\neq j}^n v_j\\) is the sum of the drift rates for all other accumulators and \\(\\alpha\\) is a free parameter representing the strength of feedforward competition.\nThe code below introduces such a parameter, but calls it feedforward_comp instead of \\(\\alpha\\). This parameter is used to define a new vector of drift rates v_comp which represents the drift rates v after being subject to feedforward competition.\n\n\nCode\n# Function arguments\n# v: \"drift rate\", the average evidence in support of each option\n# a: threshold level of evidence for each accumulator\n# t0: residual time\n# dt: how long each tiny time interval lasts\n# t_max: optional \"cut off\" time to stop accumulating\n# feedforward_comp: strength of feed-forward competition between accumulators\nrace_sim &lt;- function(v = 0, a = 1, t0 = 0.2, dt = 0.01, t_max = Inf, feedforward_comp = 0) {\n    n_accum &lt;- max(length(v), length(a))\n    \n    v &lt;- rep(v, n_accum)[1:n_accum]\n    a &lt;- rep(a, n_accum)[1:n_accum]\n    \n    v_comp &lt;- rep(0, n_accum)\n    \n    for (i in 1:n_accum) {\n        v_comp[i] &lt;- v[i] - feedforward_comp * sum(v[-i])\n    }\n    \n    x &lt;- rep(0, n_accum)\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (all(x &lt; a) & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = n_accum, mean = v_comp * dt, sd = sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- rbind(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    to_return &lt;- cbind(t_record, x_record)\n    \n    colnames(to_return) &lt;- c(\"t\", paste0(\"x\", 1:n_accum))\n    \n    return(as_tibble(to_return))\n}\n\n\nLet’s take the new model out for a spin! First, let’s repeat the simulations varying distractor strength, but now set feedforward_comp = 0.5.\n\n\nCode\nn_sims &lt;- 1000\n\nsim_results &lt;- c()\n\nfor (distractor_drift in c(0, 0.5, 0.9)) {\n    for (i in 1:n_sims) {\n        current_result &lt;- race_sim(c(1, distractor_drift), a = 2, t0 = 0.2, feedforward_comp = 0.5)\n        \n        # Extract just the choice and RT\n        rt &lt;- current_result$t[nrow(current_result)]\n        choice &lt;- which.max(current_result[nrow(current_result), 2:ncol(current_result)])\n        \n        # \"Bind\" the current simulation to the ongoing record of simulation results\n        sim_results &lt;- rbind(\n            sim_results,\n            tibble(\n                distractor_drift = distractor_drift,\n                sim_index = i,\n                rt = rt,\n                choice = choice\n            )\n        )\n    }\n}\n\n# Quantile-probability plot\nsim_choice_p &lt;- sim_results %&gt;%\n    group_by(distractor_drift, choice) %&gt;%\n    count() %&gt;%\n    group_by(distractor_drift) %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- sim_results %&gt;%\n    group_by(distractor_drift, choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = factor(choice))) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    facet_wrap(\"distractor_drift\", labeller = label_both) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", color = \"Choice\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(distractor_drift, choice)`\n\n\n\n\n\n\n\n\n\nAdding feedforward competition now causes the model to slow down when the distractor is more similar to a target. This matches the intuition we may have about many tasks that a harder choice should take longer, on average. Feedforward competition produces this behavior because strong competitors with high drift rates reduce the drift rates for all accumulators.\nWhat about increasing the number of options? The simulations below replicate our earlier simulations in which we varied the number of distractors. By introducing feedforward competition, each additional distractor now acts to suppress the drift rate for the target accumulator.\n\n\nCode\nn_sims &lt;- 1000\n\nsim_results &lt;- c()\n\nfor (num_distractors in c(1, 3, 7)) {\n    for (i in 1:n_sims) {\n        current_result &lt;- race_sim(c(1, rep(0.1, num_distractors)), a = 2, t0 = 0.2, feedforward_comp = 0.5)\n        \n        # Extract just the choice and RT\n        rt &lt;- current_result$t[nrow(current_result)]\n        choice &lt;- which.max(current_result[nrow(current_result), 2:ncol(current_result)])\n        \n        # \"Bind\" the current simulation to the ongoing record of simulation results\n        sim_results &lt;- rbind(\n            sim_results,\n            tibble(\n                num_distractors = num_distractors,\n                sim_index = i,\n                rt = rt,\n                # Note that I don't bother to keep track of which distractor was selected\n                choice = factor(ifelse(choice == 1, \"Target\", \"Distractor\"), levels = c(\"Target\", \"Distractor\"))\n            )\n        )\n    }\n}\n\n# Quantile-probability plot\nsim_choice_p &lt;- sim_results %&gt;%\n    group_by(num_distractors, choice) %&gt;%\n    count() %&gt;%\n    group_by(num_distractors) %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- sim_results %&gt;%\n    group_by(num_distractors, choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = factor(choice))) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    facet_wrap(\"num_distractors\", labeller = label_both) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", color = \"Choice\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(num_distractors, choice)`\n\n\n\n\n\n\n\n\n\nAs shown above, with feedforward competition between accumulators, adding more distractors slows responding, in keeping with intuition. That said, it is still the case that the race model predicts errors (selecting a distractor) to be faster than correct responses. Again, statistical facilitation is at work—the only way for a race model to make an error is for the “wrong” accumulator (i.e., one with a comparatively low drift rate) to be even faster than the “right” accumulator (i.e., one with a comparatively high drift rate).\n\n\n8.1.3.2 Lateral competition\nA different form of competition enables race models to predict slower errors than correct responses: lateral competition. Lateral competition occurs between accumulators during the accumulation process, unlike feedforward competition which only affects the “inputs” to the accumulators. With lateral competition, the level of evidence in one accumulator acts to suppress the level of evidence in other accumulators.\nMathematically, lateral competition enters into the equation that describes how evidence evolves from one moment in time to the next: \\[\n\\overbrace{x_i(t + \\Delta t)}^{\\text{Updated evidence}} = \\underbrace{x_i(t)}_{\\text{Current accumulated evidence}} + \\overbrace{\\Delta x_i(t)}^{\\text{Current sample of evidence}} - \\underbrace{\\beta \\Delta t \\sum_{j \\neq i}^N x_j(t)}_{\\text{Lateral competition}}\n\\] As described in the equation above, during each interval of time, the evidence in accumulator \\(i\\) not only receives a new sample of evidence, it loses some evidence in proportion to how much evidence has accumulated in the other accumulators (accumulators \\(j\\), where \\(j \\neq i\\)). Conceptually, lateral competition embodies the idea that having accumulated a lot of evidence for one option counts as evidence against picking the other options, encouraging a sort of “winner-take-all” competition. Metaphorically, we can think of lateral competition as sort of like the famous chariot race scene in Ben Hur, where instead of the competitors running in their own separate lanes, they are able to “jostle” with one another during the event.\nThe chunk of code below amends our race model simulation to include lateral competition. It also includes a new argument to the function min_x, which sets the minimum allowable value for accumulated evidence. By default, min_x = -Inf, such that evidence is allowed to be negative. However, we will see that allowing negative evidence results in some interesting (not necessarily incorrect!) behavior from the model.\n\n\nCode\n# Function arguments\n# v: \"drift rate\", the average evidence in support of each option\n# a: threshold level of evidence for each accumulator\n# t0: residual time\n# dt: how long each tiny time interval lasts\n# t_max: optional \"cut off\" time to stop accumulating\n# feedforward_comp: strength of feed-forward competition between accumulators\n# lateral_comp: strength of lateral competition between accumulators\n# min_x: accumulators are constrained to have at minimum *this much* evidence\nrace_sim &lt;- function(v = 0, a = 1, t0 = 0.2, dt = 0.01, t_max = Inf, feedforward_comp = 0, lateral_comp = 0, min_x = -Inf) {\n    n_accum &lt;- max(length(v), length(a))\n    \n    v &lt;- rep(v, n_accum)[1:n_accum]\n    a &lt;- rep(a, n_accum)[1:n_accum]\n    \n    v_comp &lt;- rep(0, n_accum)\n    \n    for (i in 1:n_accum) {\n        v_comp[i] &lt;- v[i] - feedforward_comp * sum(v[-i])\n    }\n    \n    x &lt;- rep(0, n_accum)\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (all(x &lt; a) & t &lt; t_max) {\n        # Compute the total amount of lateral competition for each accumulator\n        lat &lt;- rep(0, n_accum)\n        for (i in 1:n_accum) {\n            lat[i] &lt;- sum(x[-i])\n        }\n        \n        x_sample &lt;- rnorm(n = n_accum, mean = v_comp * dt, sd = sqrt(dt))\n        \n        # Updated values account for lateral competition and use \"pmax\" to keep them above \"min_x\"\n        x &lt;- pmax(min_x, x + x_sample - dt * lateral_comp * lat)\n        t &lt;- t + dt\n        x_record &lt;- rbind(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    to_return &lt;- cbind(t_record, x_record)\n    \n    colnames(to_return) &lt;- c(\"t\", paste0(\"x\", 1:n_accum))\n    \n    return(as_tibble(to_return))\n}\n\n\nTo see the effect of lateral competition, let’s take a look at some simulated trials. For comparison purposes, I have set R’s “random seed” to the same value prior to each simulation, which has the effect of making the sequence of random evidence samples the same for each simulated trial. This allows us to focus on the effects of lateral competition and of enforcing a minimum evidence level.\n\n\nCode\nset.seed(2)\nbaseline_sim &lt;- race_sim(v = c(1, 0), a = 2, t0 = 0.2, lateral_comp = 0, min_x = -Inf)\n\nset.seed(2)\nlateral_sim &lt;- race_sim(v = c(1, 0), a = 2, t0 = 0.2, lateral_comp = 1, min_x = -Inf)\n\nset.seed(2)\nlateral_min_sim &lt;- race_sim(v = c(1, 0), a = 2, t0 = 0.2, lateral_comp = 1, min_x = 0)\n\nall_sims &lt;- rbind(\n    baseline_sim %&gt;% mutate(label = \"No lateral competition,\\nno minimum value\"),\n    lateral_sim %&gt;% mutate(label = \"Lateral competition,\\nno minimum value\"),\n    lateral_min_sim %&gt;% mutate(label = \"Lateral competition,\\nnonnegative evidence\")\n) %&gt;%\n    mutate(label = factor(label, levels = c(\"No lateral competition,\\nno minimum value\", \"Lateral competition,\\nno minimum value\", \"Lateral competition,\\nnonnegative evidence\")))\n\nall_sims %&gt;%\n    pivot_longer(matches(\"x(\\\\d+)\"), names_to = \"accumulator\", values_to = \"x\") %&gt;%\n    ggplot(aes(x = t, y = x, color = accumulator)) +\n    geom_line() +\n    geom_hline(yintercept = 2, linetype = \"dashed\") +\n    geom_hline(yintercept = 0, linetype = \"dotted\") +\n    facet_wrap(\"label\", nrow = 1) +\n    labs(x = \"Time\", y = \"Accumulated evidence\", color = \"Accumulator\")\n\n\n\n\n\n\n\n\n\nComparing the first two plots (with and without lateral competition), we can see that the effect of lateral competition is twofold: the “stronger” accumulator (x1) suppresses the “weaker” accumulator (x2), causing its evidence level to diminish over time. Comparing the second two plots, we can see that an accumulator with negative evidence can actually accelerate accumulation for its competitors—after all, subtracting a negative is a positive! When evidence is constrained to be non-negative (as in the third plot), that kind of acceleration is no longer possible.\nIt is worth noting that many models with lateral competition are inspired by the ways that individual neurons interact, making an analogy between the level of accumulated evidence and the amount of activity in either a single neuron or group of neurons (Purcell et al., 2010; Purcell et al., 2012; Teodorescu & Usher, 2013; Usher & McClelland, 2001). Because neural activity cannot be negative, these models also constrain evidence to be non-negative. As a result, most models that include some form of lateral competition also assume that the level of accumulated evidence cannot be negative, although this assumption is not strictly required.\nNow let’s see the effect of lateral competition on the model’s predictions regarding distractor similarity and number of distractors. The simulations below set the lateral competition parameter to 1 and, in keeping with the majority of models with lateral competition, do not allow for negative evidence.\n\n\nCode\nn_sims &lt;- 1000\n\nsim_results &lt;- c()\n\nfor (distractor_drift in c(0, 0.5, 0.9)) {\n    for (i in 1:n_sims) {\n        current_result &lt;- race_sim(c(1, distractor_drift), a = 2, t0 = 0.2, feedforward_comp = 0, lateral_comp = 1, min_x = 0)\n        \n        # Extract just the choice and RT\n        rt &lt;- current_result$t[nrow(current_result)]\n        choice &lt;- which.max(current_result[nrow(current_result), 2:ncol(current_result)])\n        \n        # \"Bind\" the current simulation to the ongoing record of simulation results\n        sim_results &lt;- rbind(\n            sim_results,\n            tibble(\n                distractor_drift = distractor_drift,\n                sim_index = i,\n                rt = rt,\n                choice = choice\n            )\n        )\n    }\n}\n\n# Quantile-probability plot\nsim_choice_p &lt;- sim_results %&gt;%\n    group_by(distractor_drift, choice) %&gt;%\n    count() %&gt;%\n    group_by(distractor_drift) %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- sim_results %&gt;%\n    group_by(distractor_drift, choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = factor(choice))) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    facet_wrap(\"distractor_drift\", labeller = label_both) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", color = \"Choice\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(distractor_drift, choice)`\n\n\n\n\n\n\n\n\n\nWith lateral inhibition, errors (choosing 2) are generally slower than correct responses (choosing 1). This is because errors tend to occur when the correct response has more competition from the error accumulators. Notice, though, that making the distractor stronger does not slow responding overall—it actually speeds responses. Statistical facilitation still occurs even in the presence of lateral competition.\nBut while increasing the strength of competition doesn’t necessarily slow responding, the simulations below show that increasing the number of competitors does slow responding if there is lateral competition. The more competitors in the race, the more lateral competition there is, slowing down all the accumulators.\n\n\nCode\nn_sims &lt;- 1000\n\nsim_results &lt;- c()\n\nfor (num_distractors in c(1, 3, 7)) {\n    for (i in 1:n_sims) {\n        current_result &lt;- race_sim(c(1, rep(0.1, num_distractors)), a = 2, t0 = 0.2, lateral_comp = 1, min_x = 0)\n        \n        # Extract just the choice and RT\n        rt &lt;- current_result$t[nrow(current_result)]\n        choice &lt;- which.max(current_result[nrow(current_result), 2:ncol(current_result)])\n        \n        # \"Bind\" the current simulation to the ongoing record of simulation results\n        sim_results &lt;- rbind(\n            sim_results,\n            tibble(\n                num_distractors = num_distractors,\n                sim_index = i,\n                rt = rt,\n                # Note that I don't bother to keep track of which distractor was selected\n                choice = factor(ifelse(choice == 1, \"Target\", \"Distractor\"), levels = c(\"Target\", \"Distractor\"))\n            )\n        )\n    }\n}\n\n# Quantile-probability plot\nsim_choice_p &lt;- sim_results %&gt;%\n    group_by(num_distractors, choice) %&gt;%\n    count() %&gt;%\n    group_by(num_distractors) %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- sim_results %&gt;%\n    group_by(num_distractors, choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = factor(choice))) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    facet_wrap(\"num_distractors\", labeller = label_both) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", color = \"Choice\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(num_distractors, choice)`\n\n\n\n\n\n\n\n\n\n\n\n8.1.3.3 Summary\nFeedforward competition diminishes the drift rates for each accumulator, with the result that stronger competition tends to slow responding overall. However, feedforward competition still generally predicts fast errors because errors occur when the accumulators for incorrect responses happen to be fast enough to “beat” the accumulator associated with the correct response.\nLateral competition does not always predict that stronger competition will slow responding. Lateral competition can produce slow errors because they arise in trials where the incorrect accumulators happened to be strong enough to impede the correct accumulator.\nNote that the race model theory for slow errors is different from how slow errors were explained with a diffusion model. With a diffusion model, slow errors arose from trial-by-trial variability in the drift rate. One of your exercises will be to compare and contrast these two theories of slow errors.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Accumulator models of choice and response time</span>"
    ]
  },
  {
    "objectID": "accumulator_models.html#race-model-likelihood-function",
    "href": "accumulator_models.html#race-model-likelihood-function",
    "title": "8  Accumulator models of choice and response time",
    "section": "8.2 Race model likelihood function",
    "text": "8.2 Race model likelihood function\nIn the previous section, we built code to simulate a race model, where each accumulator was associated with a particular response. We will now write a function to calculate the negative log-likelihood (NLL) of a response at a particular time. This will enable us to fit a race model to data. The process of building the function will also illustrate the essential ingredients in any NLL function, namely\n\nTranslating a mathematical expression of the likelihood into code.\nDeciding how to represent the data to be fit, along with any experimental factors relevant to the model.\nExtracting the values for the model’s parameters from a vector (this last step is necessary for using most parameter search algorithms).\n\n\n8.2.1 Math to Code\nIn the previous section, we began by simulating just a single accumulator with a single boundary. There were three parameters describing the accumulator: the drift rate, the threshold, and the residual time. There is, in fact, a mathematical expression for the distribution of finishing times for this process: the shifted Wald distribution (also called the shifted inverse Gaussian distribution).\nAs you can see in the Wikipedia entry, it is possible to write a mathematical expression for both the probability density function (PDF) (the probability of finishing at a particular time) and the cumulative distribution function (CDF) (the probability of finishing by a particular time). Since the Wikipedia entry doesn’t explicitly include residual time in its formulae, let’s write them out ourselves for our own reference.\n\n8.2.1.1 Probability density function for a single accumulator\nThe PDF, describing the probability that accumulator \\(i\\) reaches its threshold at time \\(t\\), can be written \\[\nf_i \\left( t \\mid a_i, v_i, t_{0i} \\right) = \\frac{a_i}{\\sqrt{2 \\pi \\left(t - t_{0i} \\right)^3}} \\exp \\left\\lbrace -\\frac{\\left[a_i - v_i \\left(t - t_{0i} \\right) \\right]^2}{2 \\left(t - t_{0i} \\right)} \\right\\rbrace\n\\] where \\(a_i\\) is the threshold, \\(v_i\\) is the drift rate, and \\(t_{0i}\\) is the residual time. While the expression above may look a bit cumbersome, it only involves elementary mathematical operations which are already implemented in R (and almost all programming languages). Therefore, we can write an R function that will compute the PDF for a single accumulator. That’s what I’ve done below. Note the comments in the code below explain the purpose of each line.\n\n\nCode\naccum_pdf &lt;- function(t, a, v, t0) {\n    # Because the term \"t - t0\" shows up multiple times in the formula, it's easier\n    # to compute it once and then refer back to it whenever we need it.  It also\n    # gives us a way to \"clip\" the values so they cannot be negative, since that\n    # would otherwise give a weird result.\n    t_minus_t0 &lt;- pmax(0, t - t0)\n    \n    # The next line is a direct implementation of the formula above.\n    pdf &lt;- a * exp(-(a - v * t_minus_t0)^2 / (2 * t_minus_t0)) / sqrt(2 * pi * t_minus_t0^3)\n    \n    # This does some final error-checking, ensuring that there is zero probability\n    # of finishing in zero (or negative) time.\n    pdf[t_minus_t0 &lt;= 0] &lt;- 0\n    \n    # Finally, we use an explicit \"return\" statement to say exactly what the\n    # function gives back.\n    return(pdf)\n}\n\n\nCool. Let’s take this function out for a spin!\n\n\nCode\naccum_pdf(\n    t = 1,\n    a = 2,\n    v = 1,\n    t0 = 0.2\n)\n\n\n[1] 0.4533567\n\n\nNice! But perhaps even more impressive is that the function will also accept vectors for its arguments, not just single numbers. This is very convenient for us because we will often want to compute the likelihood for many trials all at once. While we could use a for loop for that purpose, R code runs much faster if we can use vectors instead.\nFor example, here’s what we get if we want to compute the likelihood of a single accumulator finishing at a range of times:\n\n\nCode\naccum_pdf(\n    t = c(0, 0.5, 1, 1.5, 2, 2.5, 3),\n    a = 2,\n    v = 1,\n    t0 = 0.2\n)\n\n\n[1] 0.00000000 0.03930126 0.45335671 0.44583815 0.32674264 0.22431135 0.15190405\n\n\nNotice that we get a vector back from the function, which is the likelihood for each finishing time in the vector we supplied for the t argument.\nWe can also supply vectors for the other arguments to the function if we want. This is illustrated in the example below.\n\n\nCode\naccum_pdf(\n    t = c(0.5, 0.5, 0.5, 1, 1, 1, 2, 2, 2),\n    a = c(2, 2, 1, 2, 2, 1, 2, 2, 1),\n    v = c(1, 2, 3, 1, 2, 3, 1, 2, 3),\n    t0 = 0.2\n)\n\n\n[1] 0.0393012555 0.1851666937 2.3877559853 0.4533567093 1.0089639117\n[6] 0.1637813117 0.3267426369 0.1622555916 0.0007628903\n\n\nJust be aware that you can supply vectors with unequal lengths and R will “recycle” them until they are the length of the longest vector you supplied. This is generally a bad idea, though, because it can lead to all sorts of “silent” errors.\n\n\nCode\naccum_pdf(\n    t = c(0.5, 1, 1.5, 2),\n    a = c(2, 1),\n    v = 1,\n    t0 = 0.2\n)\n\n\n[1] 0.03930126 0.54377310 0.44583815 0.13829084\n\n\nAt least R will give you a warning if you supply vectors that can’t be recycled evenly.\n\n\nCode\naccum_pdf(\n    t = c(0.5, 1, 1.5, 2),\n    a = c(2, 1, 3),\n    v = 1,\n    t0 = 0.2\n)\n\n\nWarning in a - v * t_minus_t0: longer object length is not a multiple of\nshorter object length\n\n\nWarning in a * exp(-(a - v * t_minus_t0)^2/(2 * t_minus_t0)): longer object\nlength is not a multiple of shorter object length\n\n\n[1] 0.03930126 0.54377310 0.26569371 0.32674264\n\n\nBut in general, you should adhere to the following rule: For each argument to the function, provide either a single number or a vector that is the same length as any other vectors you might supply.\nOne nice side-effect of being able to provide vectors to our accum_pdf function is that we can use it to make some nice plots of the distributions of finishing times for different combinations of parameter values:\n\n\nCode\nexpand_grid(a = c(1, 2), v = c(1, 2, 3), t0 = 0.2, t = seq(0, 5, length.out = 201)) %&gt;%\n    mutate(pdf = accum_pdf(t = t, a = a, v = v, t0 = t0)) %&gt;%\n    ggplot(aes(x = t, y = pdf, color = v, linetype = factor(a), group = interaction(v, a))) +\n    geom_line() +\n    labs(x = \"Time (s)\", y = \"PDF\", color = \"Drift rate (v)\", linetype = \"Threshold (a)\")\n\n\n\n\n\n\n\n\n\n\n\n8.2.1.2 Cumulative distribution function for a single accumulator\nJust like there’s a mathematical expression for the PDF, we have one for the CDF too. As a reminder, the CDF is the probability that accumulator \\(i\\) reached threshold by time \\(t\\). The formula is given below, using the same parameters as the formula for the PDF above: \\[\nF_i \\left( t \\mid a_i, v_i, t_{0i} \\right) = \\Phi \\left[ \\frac{v_i \\left( t - t_{0i} \\right) - a_i}{\\sqrt{t - t_{0i}}} \\right] + \\exp \\left( 2 a_i v_i \\right) \\Phi \\left[ -\\frac{v_i \\left( t - t_{0i} \\right) + a_i}{\\sqrt{t - t_{0i}}} \\right]\n\\] The only new thing in the CDF formula is \\(\\Phi \\left( \\cdot \\right)\\), which is a shorthand for the CDF of the standard normal distribution. Recall that the standard normal distribution has a mean of zero and a standard deviation of one. Fortunately, this is built into R in the form of the pnorm function. Although you can supply a mean and sd to pnorm, if you don’t, it will use default settings of mean = 0 and sd = 1, amounting to a standard normal distribution.\nWe can thus write an R function to calculate the expression in the formula above for the CDF. This function is shown below.\n\n\nCode\naccum_cdf &lt;- function(t, a, v, t0) {\n    # As with the pdf function, I compute \"t - t0\" once at the very beginning,\n    # which also offers an opportunity to \"clip\" any negative values that result.\n    t_minus_t0 &lt;- pmax(0, t - t0)\n    \n    # This expression is a direct translation of the formula above\n    cdf &lt;- pnorm((v * t_minus_t0 - a) / sqrt(t_minus_t0)) + exp(2 * a * v) * pnorm(-(v * t_minus_t0 + a) / sqrt(t_minus_t0))\n    \n    # This formula doesn't give weird results when t_minus_t0 == 0, so we can just\n    # return the result as-is.\n    return(cdf)\n}\n\n\nLike accum_pdf, accum_cdf can accept vectors for its arguments too (with the same caveat that you should be sure to supply only single values or vectors of the same length to avoid unpredictable “recycling” issues). We can thus make some very nice plots, just like we did for the PDF:\n\n\nCode\nexpand_grid(a = c(1, 2), v = c(1, 2, 3), t0 = 0.2, t = seq(0, 5, length.out = 201)) %&gt;%\n    mutate(cdf = accum_cdf(t = t, a = a, v = v, t0 = t0)) %&gt;%\n    ggplot(aes(x = t, y = cdf, color = v, linetype = factor(a), group = interaction(v, a))) +\n    geom_line() +\n    expand_limits(y = c(0, 1)) +\n    labs(x = \"Time (s)\", y = \"CDF\", color = \"Drift rate (v)\", linetype = \"Threshold (a)\")\n\n\n\n\n\n\n\n\n\n\n\n8.2.1.3 Likelihood expression\nArmed with functions for calculating the PDF and CDF for a single accumulator, we are now in a position to write a function that computes the likelihood of a particular accumulator winning at a particular time \\(t\\). We will first write the likelihood mathematically and then translate it into an R function, like we did for the PDF and CDF.\nTo understand the math, it may help to think conceptually first: If accumulator \\(i\\) is the first of \\(N\\) accumulators to reach its threshold at time \\(t\\), that means two things must be true: First, and most obvious, accumulator \\(i\\) must have hit threshold at time \\(t\\). Second, and perhaps less obvious, is that none of the other accumulators must have hit threshold by time \\(t\\). The likelihood must therefore express the probability that both of these things are true.\nThere are two more conceptual ingredients we need to express the likelihood function: First, we need to know how to express the probability that a non-winning accumulator hasn’t hit threshold by time \\(t\\). Since the CDF is the probability that an accumulator has hit threshold by time \\(t\\), the probability that it hasn’t is just one minus the CDF, since “has already hit threshold” and “hasn’t already hit threshold” are mutually exclusive and exhaustive possibilities. Second, we have to remember that the joint probability of multiple independent events being true is the product of the individual probabilities.\nFinally, we can express the likelihood that accumulator \\(i\\) is the first out of \\(N\\) accumulators to reach its threshold at time \\(t\\): \\[\n\\overbrace{L \\left(i, t \\right)}^{\\text{Likelihood that } i \\text{ wins at } t} = \\underbrace{f_i \\left( t \\mid a_i, v_i, t_{0i} \\right)}_{i \\text{ finishes at } t} \\overbrace{\\prod_{j \\neq i}^N \\left[1 - F_j \\left(t \\mid a_j, v_j, t_{0j} \\right) \\right]}^{\\text{Nothing else has finished by } t}\n\\]\nIt is worth emphasizing that the formula above only works when the accumulators are (conditionally) independent of one another. As a result, the race model cannot include lateral competition or any other interactions between accumulators that would cause them to be dependent on one another. It can, however, accommodate feedforward competition, since that affects the drift rates but not how the accumulators evolve from moment to moment. We will return to feedforward competition later in this section.\nTo translate the likelihood formula above into R code, we will first see how to compute it for a single trial. We will then generalize that approach so we can compute the likelihood for many trials, exploiting the fact that our accum_pdf and accum_cdf functions can accept vectors as arguments.\nBut first, let’s imagine a concrete situation: Say we present an array of three items in a visual search task. The target is a red square and the job of the participant is to select which of the three items is the target. The item in position 1 is a blue circle and so doesn’t resemble the target at all; let’s say it has a drift rate of 0. The item in position 2 is the target—let’s say it has a drift rate of 2. Finally, the item in position 3 is a red circle, matching the target’s color but not its shape—let’s say it has a drift rate of 1. Let’s assume that all accumulators have the same threshold value (2) and the same residual time (0.2). We can now write the parameters for all three accumulators as vectors, although we will be pedantic and write out the same value 3 times for the threshold and residual time:\n\n\nCode\nv &lt;- c(0, 2, 1)\na &lt;- c(2, 2, 2)\nt0 &lt;- c(0.2, 0.2, 0.2)\n\n\nWhat is the likelihood of selecting the target (item 2) in 1 second? Based on the formula above, we can compute that likelihood in R using the accum_pdf and accum_cdf functions from earlier:\n\n\nCode\nv &lt;- c(0, 2, 1)\na &lt;- c(2, 2, 2)\nt0 &lt;- c(0.2, 0.2, 0.2)\n\naccum_pdf(t = 1, a = a[2], v = v[2], t0 = t0[2]) *\n    (1 - accum_cdf(t = 1, a = a[1], v = v[1], t0 = t0[1])) *\n    (1 - accum_cdf(t = 1, a = a[3], v = v[3], t0 = t0[3]))\n\n\n[1] 0.8481769\n\n\nWe are going to make the expression above a bit more general and more compact. First, rather than refer explicitly to a response time of 1, we will refer to a variable rt (which we can set to one):\n\n\nCode\nv &lt;- c(0, 2, 1)\na &lt;- c(2, 2, 2)\nt0 &lt;- c(0.2, 0.2, 0.2)\n\nrt &lt;- 1\n\naccum_pdf(t = rt, a = a[2], v = v[2], t0 = t0[2]) *\n    (1 - accum_cdf(t = rt, a = a[1], v = v[1], t0 = t0[1])) *\n    (1 - accum_cdf(t = rt, a = a[3], v = v[3], t0 = t0[3]))\n\n\n[1] 0.8481769\n\n\nSecond, we can use the prod function to multiply together the two terms at the end, making use of the fact that accum_cdf can accept vectors as arguments:\n\n\nCode\nv &lt;- c(0, 2, 1)\na &lt;- c(2, 2, 2)\nt0 &lt;- c(0.2, 0.2, 0.2)\n\nrt &lt;- 1\n\naccum_pdf(t = rt, a = a[2], v = v[2], t0 = t0[2]) *\n    prod(1 - accum_cdf(t = rt, a = a[c(1, 3)], v = v[c(1, 3)], t0 = t0[c(1, 3)]))\n\n\n[1] 0.8481769\n\n\nThird, instead of explicitly referring to individual accumulators, we can define a variable called response (and set it to 2), and use the “negative indexing trick” for the second term:\n\n\nCode\nv &lt;- c(0, 2, 1)\na &lt;- c(2, 2, 2)\nt0 &lt;- c(0.2, 0.2, 0.2)\n\nrt &lt;- 1\nresponse &lt;- 2\n\naccum_pdf(t = rt, a = a[response], v = v[response], t0 = t0[response]) *\n    prod(1 - accum_cdf(t = rt, a = a[-response], v = v[-response], t0 = t0[-response]))\n\n\n[1] 0.8481769\n\n\nNice! The only problem now is that our code only applies to a single trial at a time. In the next section, we confront the issue of how to compute the likelihood for multiple trials.\n\n\n\n8.2.2 Likelihood for multiple trials\nIn the previous section, we effectively translated a mathematical expression of the model into R code that performs the relevant computations to find the likelihood of a given response at a given time. To apply this code to an entire dataset, which contains many trials, we need to think about how to specify the details of each individual trial.\n\n8.2.2.1 Characterizing each trial\nAgain, it will help to think about a concrete task like visual search. Any given trial involves a number of items, each of which can occupy a given position in the visual search array. For example, one trial might have 3 items with the target occupying the second position while another trial might have 6 items with the target occupying the fifth position. The distractors occupying the non-target positions could be of varying types depending on whether they are more or less similar to the target. Therefore, to characterize any individual trial in this experiment, we need to know, for each possible location that could contain an item, whether or not it does contain an item and, if so, what kind of item it is.\nThere are a few ways we could represent this information in an R data structure, but the simplest is in the form of a matrix. Each row of the matrix will correspond to a different trial in the experiment and each column of the matrix corresponds to a different location that may or may not contain an item on that trial. We can give each type of item a numerical index. For example, maybe 1 corresponds to targets, 2 corresponds to similar distractors, and 3 corresponds to dissimilar distractors. To indicate that a location does not contain an item, we can give it the value NA. For example, the following matrix could describe four trials in a search experiment with six possible display locations:\n\n\nCode\nmatrix(c(\n    1, NA, NA,  2, NA, NA,\n    2,  1,  2,  3,  3,  3,\n    NA, NA, 3,  1,  3, NA,\n    NA,  3, NA,  2, NA, 1\n), nrow = 4, ncol = 6, byrow = TRUE)\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1   NA   NA    2   NA   NA\n[2,]    2    1    2    3    3    3\n[3,]   NA   NA    3    1    3   NA\n[4,]   NA    3   NA    2   NA    1\n\n\nAlternatively, we could imagine a decision making task in which each trial presents three options selected from a set of six possible products:\n\n\nCode\nmatrix(c(\n    1, 2, 3,\n    1, 4, 5,\n    6, 4, 2,\n    5, 2, 6\n), nrow = 4, ncol = 3, byrow = TRUE)\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    1    4    5\n[3,]    6    4    2\n[4,]    5    2    6\n\n\nThese concrete examples suggest the general structure of the tasks to which we can typically apply a race model: Each trial presents a number of options, each of which is selected from a set of possible options. Moreover, sometimes the ordering/location of the options may matter (like in visual search) but sometimes it may not (like in the decision task).\n\n\n8.2.2.2 Defining model parameters\nThese examples also suggest what kind of parameters our model will need to have. Specifically, we will need to have a drift rate parameter for each type of item in the experiment. We could have a single threshold parameter that applies to all accumulators. Alternatively, we could allow different locations to have different thresholds, modeling the idea that there may be a bias in favor of choosing options in particular locations. Similarly, we could have a single residual time parameter for all accumulators or we could allow residual time to vary by location, allowing us to model situations in which processing begins earlier for some locations than others.\nLet’s again make this concrete and return to the visual search experiment. We will keep things simple and assume just one threshold parameter and one residual time parameter apply to all accumulators, regardless of location. We will have three possible values of drift rate for three categories of item:\n\nTargets\nSimilar distractors\nDissimilar distractors\n\nTo compute the likelihood for each trial, we will need to refer to the corresponding row in the matrix and use the drift rates that correspond to the items shown on that trial.\n\n\nCode\n# Specify item types/locations for each trial\nitems &lt;- matrix(c(\n    1, NA, NA,  2, NA, NA,\n    2,  1,  2,  3,  3,  3,\n    NA, NA, 3,  1,  3, NA,\n    NA,  3, NA,  2, NA, 1\n), nrow = 4, ncol = 6, byrow = TRUE)\n\n# Responses and response times for each trial\n# Notice that responses refer to the *position* of the chosen item!\nresponse &lt;- c(1, 2, 4, 4)\nrt &lt;- c(0.6, 1.5, 0.7, 1.2)\n\nn_trials &lt;- nrow(items)\nn_locations &lt;- ncol(items)\n\n# Model parameters\n# One drift rate for each type of item\nv &lt;- c(2, 1, 0)\n# One threshold for each possible location\na &lt;- rep(2, n_locations)\n# One residual time for each possible location\nt0 &lt;- rep(0.2, n_locations)\n\n# Allocate vector that will eventually contain the negative log-likelihood\n# for each trial\nnll &lt;- rep(0, n_trials)\n\n# Calculate the log-likelihood for each trial\nfor (i in 1:n_trials) {\n    # Note that the whole thing is wrapped in \"-log()\" so we get the negative log-likelihood\n    # Note also the use of \"na.rm = TRUE\" in the \"prod()\" function to ignore any locations\n    # that don't contain an item\n    nll[i] &lt;- -log(\n        accum_pdf(t = rt[i], a = a[response[i]], v = v[items[i, response[i]]], t0 = t0[response[i]]) *\n            prod(1 - accum_cdf(t = rt[i], a = a[-response[i]], v = v[items[i, -response[i]]], t0 = t0[-response[i]]), na.rm = TRUE)\n    )\n}\n\nprint(nll)\n\n\n[1] 0.6611439 1.9408143 0.1954480 1.6747733\n\n\nYou’ll note that the NLL for each trial is calculated using a for loop at the end—there are ways to make this more efficient, but I will leave those as an exercise for the reader!\n\n\n\n8.2.3 Extracting parameters\nWe are almost there! We have code that will take a set of model parameters and a representation of the options on each trial and compute the negative log-likelihood of each response made on each trial. All that is left is to “wrap” this code into a function. However, we will need to define the arguments of this function in a particular way in order to get the function to play nicely with the parameter search functions we will use. Specifically, the function will need to take the following arguments:\n\nThe matrix that specifies the options on each trial; we called this items in the code above.\nThe vector of responses made on each trial; we called this response in the code above.\nThe vector of response times on each trial; we called this rt in the code above.\nFinally, and this is the tricky part, we need a single vector that contains all the parameters of our model. As we will see, the “trick” is to extract the parameter values from this vector based on the names of the elements in the vector.\n\nLet’s lay out the outline of our function before we get to the tricky part:\n\n\nCode\nrace_nll &lt;- function(par, response, rt, items) {\n    n_trials &lt;- nrow(items)\n    n_locations &lt;- ncol(items)\n    \n    # TRICKY PART HERE\n    v &lt;- ...\n    a &lt;- ...\n    t0 &lt;- ...\n    \n    nll &lt;- rep(0, n_trials)\n    \n    for (i in 1:n_trials) {\n        nll[i] &lt;- -log(\n            accum_pdf(t = rt[i], a = a[response[i]], v = v[items[i, response[i]]], t0 = t0[response[i]]) *\n                prod(1 - accum_cdf(t = rt[i], a = a[-response[i]], v = v[items[i, -response[i]]], t0 = t0[-response[i]]), na.rm = TRUE)\n        )\n    }\n    \n    return(sum(nll))\n}\n\n\nTo get a grasp on the problem we have to solve, it’ll be good to have in mind a picture of what par looks like. It will be a named vector, meaning it will look something like this:\n\n\nCode\npar &lt;- c(\n    \"v[1]\" = 2,\n    \"v[2]\" = 1,\n    \"v[3]\" = 0,\n    \"a\" = 2,\n    \"t0\" = 0.2\n)\n\n\nWe can use the names of the elements to extract them, like so:\n\n\nCode\npar[\"a\"]\n\n\na \n2 \n\n\nWe can also get a bit clever by using the paste0 function:\n\n\nCode\npar[paste0(\"v[\", 1:3, \"]\")]\n\n\nv[1] v[2] v[3] \n   2    1    0 \n\n\nFinally, we can check to see whether the vector contains an element with a particular name by using is.na, which is TRUE if an element does not exist in the vector:\n\n\nCode\nis.na(par[\"t0\"])\n\n\n   t0 \nFALSE \n\n\nCode\nis.na(par[\"t0[1]\"])\n\n\n&lt;NA&gt; \nTRUE \n\n\nThe latter trick will allow us to make our race_nll function more general, because we can check to see whether the par vector contains, for example, several threshold parameters or only one. The whole shebang is illustrated in the complete function below:\n\n\nCode\nrace_nll &lt;- function(par, response, rt, items, n_items = max(items, na.rm = TRUE)) {\n    n_trials &lt;- nrow(items)\n    n_locations &lt;- ncol(items)\n    \n    if (is.na(par[\"v\"])) {\n        v &lt;- par[paste0(\"v[\", 1:n_items, \"]\")]\n    } else {\n        v &lt;- rep(par[\"v\"], n_items)\n    }\n    \n    if (is.na(par[\"a\"])) {\n        a &lt;- par[paste0(\"a[\", 1:n_locations, \"]\")]\n    } else {\n        a &lt;- rep(par[\"a\"], n_locations)\n    }\n    \n    if (is.na(par[\"t0\"])) {\n        t0 &lt;- par[paste0(\"t0[\", 1:n_locations, \"]\")]\n    } else {\n        t0 &lt;- rep(par[\"t0\"], n_locations)\n    }\n    \n    nll &lt;- rep(0, n_trials)\n    \n    for (i in 1:n_trials) {\n        nll[i] &lt;- -log(\n            accum_pdf(t = rt[i], a = a[response[i]], v = v[items[i, response[i]]], t0 = t0[response[i]]) *\n                prod(1 - accum_cdf(t = rt[i], a = a[-response[i]], v = v[items[i, -response[i]]], t0 = t0[-response[i]]), na.rm = TRUE)\n        )\n    }\n    \n    return(sum(nll))\n}\n\n\nFor each parameter, we check to see whether the par vector contains an element named without an index after it. If so, then the corresponding value is “recycled” for each item/location. Otherwise, we assume that the par vector contains different indexed values for each item/location and we extract those from the par vector for later use. Notice that I added a function argument called n_items so we can specify how many different item types there could be; the default value (the largest number specified in the items matrix) should work, but there may be situations in which it doesn’t.\nBefore closing this section, you may have noticed something missing! We could include a feedforward competition parameter too. However, that is left as an exercise for the reader.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Accumulator models of choice and response time</span>"
    ]
  },
  {
    "objectID": "accumulator_models.html#fitting-the-race-model",
    "href": "accumulator_models.html#fitting-the-race-model",
    "title": "8  Accumulator models of choice and response time",
    "section": "8.3 Fitting the race model",
    "text": "8.3 Fitting the race model\nFinally, we have our race_nll function which we can use to fit the race model to data, that is, to find the parameters of the model that assign the highest likelihood to some set of data.\n\n8.3.1 Simulating some data\nTo do that, we’ll first need some data! For example purposes, we can simulate some plausible data. Like we did with the diffusion model, the advantage of fitting simulated data is that we know the “ground truth” and can therefore see how well we can recover the parameters we used to generate the data.\nThe chunk of code below simulates data in a visual search experiment in which two factors are manipulated, set size and target-distractor similarity. Since the simulations don’t allow for different parameters for different locations, I keep things simple and have the target always appear in location 1.\n\n\nCode\nv &lt;- c(2, 1, 0)\na &lt;- 2\nt0 &lt;- 0.2\n\nn_trials_per_cond &lt;- 50\n\nset_size_vals &lt;- c(2, 4, 8)\n\nitems &lt;- c()\nresponse &lt;- c()\nrt &lt;- c()\n\nfor (set_size in set_size_vals) {\n    for (td_sim in c(\"low\", \"high\")) {\n        distractor_type &lt;- ifelse(td_sim == \"low\", 3, 2)\n        \n        for (i in 1:n_trials_per_cond) {\n            items &lt;- rbind(\n                items,\n                c(1, rep(distractor_type, set_size - 1), rep(NA, max(set_size_vals) - set_size))\n            )\n            \n            current_result &lt;- race_sim(v = c(v[1], rep(v[distractor_type], set_size - 1)), a = a, t0 = t0, dt = 0.001)\n            \n            # Extract just the choice and RT\n            rt &lt;- c(rt, current_result$t[nrow(current_result)])\n            response &lt;- c(response, which.max(current_result[nrow(current_result), 2:ncol(current_result)]))\n        }\n    }\n}\n\n\nIn the end, we have our matrix specifying what is in each display:\n\n\nCode\nitems\n\n\n       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n  [1,]    1    3   NA   NA   NA   NA   NA   NA\n  [2,]    1    3   NA   NA   NA   NA   NA   NA\n  [3,]    1    3   NA   NA   NA   NA   NA   NA\n  [4,]    1    3   NA   NA   NA   NA   NA   NA\n  [5,]    1    3   NA   NA   NA   NA   NA   NA\n  [6,]    1    3   NA   NA   NA   NA   NA   NA\n  [7,]    1    3   NA   NA   NA   NA   NA   NA\n  [8,]    1    3   NA   NA   NA   NA   NA   NA\n  [9,]    1    3   NA   NA   NA   NA   NA   NA\n [10,]    1    3   NA   NA   NA   NA   NA   NA\n [11,]    1    3   NA   NA   NA   NA   NA   NA\n [12,]    1    3   NA   NA   NA   NA   NA   NA\n [13,]    1    3   NA   NA   NA   NA   NA   NA\n [14,]    1    3   NA   NA   NA   NA   NA   NA\n [15,]    1    3   NA   NA   NA   NA   NA   NA\n [16,]    1    3   NA   NA   NA   NA   NA   NA\n [17,]    1    3   NA   NA   NA   NA   NA   NA\n [18,]    1    3   NA   NA   NA   NA   NA   NA\n [19,]    1    3   NA   NA   NA   NA   NA   NA\n [20,]    1    3   NA   NA   NA   NA   NA   NA\n [21,]    1    3   NA   NA   NA   NA   NA   NA\n [22,]    1    3   NA   NA   NA   NA   NA   NA\n [23,]    1    3   NA   NA   NA   NA   NA   NA\n [24,]    1    3   NA   NA   NA   NA   NA   NA\n [25,]    1    3   NA   NA   NA   NA   NA   NA\n [26,]    1    3   NA   NA   NA   NA   NA   NA\n [27,]    1    3   NA   NA   NA   NA   NA   NA\n [28,]    1    3   NA   NA   NA   NA   NA   NA\n [29,]    1    3   NA   NA   NA   NA   NA   NA\n [30,]    1    3   NA   NA   NA   NA   NA   NA\n [31,]    1    3   NA   NA   NA   NA   NA   NA\n [32,]    1    3   NA   NA   NA   NA   NA   NA\n [33,]    1    3   NA   NA   NA   NA   NA   NA\n [34,]    1    3   NA   NA   NA   NA   NA   NA\n [35,]    1    3   NA   NA   NA   NA   NA   NA\n [36,]    1    3   NA   NA   NA   NA   NA   NA\n [37,]    1    3   NA   NA   NA   NA   NA   NA\n [38,]    1    3   NA   NA   NA   NA   NA   NA\n [39,]    1    3   NA   NA   NA   NA   NA   NA\n [40,]    1    3   NA   NA   NA   NA   NA   NA\n [41,]    1    3   NA   NA   NA   NA   NA   NA\n [42,]    1    3   NA   NA   NA   NA   NA   NA\n [43,]    1    3   NA   NA   NA   NA   NA   NA\n [44,]    1    3   NA   NA   NA   NA   NA   NA\n [45,]    1    3   NA   NA   NA   NA   NA   NA\n [46,]    1    3   NA   NA   NA   NA   NA   NA\n [47,]    1    3   NA   NA   NA   NA   NA   NA\n [48,]    1    3   NA   NA   NA   NA   NA   NA\n [49,]    1    3   NA   NA   NA   NA   NA   NA\n [50,]    1    3   NA   NA   NA   NA   NA   NA\n [51,]    1    2   NA   NA   NA   NA   NA   NA\n [52,]    1    2   NA   NA   NA   NA   NA   NA\n [53,]    1    2   NA   NA   NA   NA   NA   NA\n [54,]    1    2   NA   NA   NA   NA   NA   NA\n [55,]    1    2   NA   NA   NA   NA   NA   NA\n [56,]    1    2   NA   NA   NA   NA   NA   NA\n [57,]    1    2   NA   NA   NA   NA   NA   NA\n [58,]    1    2   NA   NA   NA   NA   NA   NA\n [59,]    1    2   NA   NA   NA   NA   NA   NA\n [60,]    1    2   NA   NA   NA   NA   NA   NA\n [61,]    1    2   NA   NA   NA   NA   NA   NA\n [62,]    1    2   NA   NA   NA   NA   NA   NA\n [63,]    1    2   NA   NA   NA   NA   NA   NA\n [64,]    1    2   NA   NA   NA   NA   NA   NA\n [65,]    1    2   NA   NA   NA   NA   NA   NA\n [66,]    1    2   NA   NA   NA   NA   NA   NA\n [67,]    1    2   NA   NA   NA   NA   NA   NA\n [68,]    1    2   NA   NA   NA   NA   NA   NA\n [69,]    1    2   NA   NA   NA   NA   NA   NA\n [70,]    1    2   NA   NA   NA   NA   NA   NA\n [71,]    1    2   NA   NA   NA   NA   NA   NA\n [72,]    1    2   NA   NA   NA   NA   NA   NA\n [73,]    1    2   NA   NA   NA   NA   NA   NA\n [74,]    1    2   NA   NA   NA   NA   NA   NA\n [75,]    1    2   NA   NA   NA   NA   NA   NA\n [76,]    1    2   NA   NA   NA   NA   NA   NA\n [77,]    1    2   NA   NA   NA   NA   NA   NA\n [78,]    1    2   NA   NA   NA   NA   NA   NA\n [79,]    1    2   NA   NA   NA   NA   NA   NA\n [80,]    1    2   NA   NA   NA   NA   NA   NA\n [81,]    1    2   NA   NA   NA   NA   NA   NA\n [82,]    1    2   NA   NA   NA   NA   NA   NA\n [83,]    1    2   NA   NA   NA   NA   NA   NA\n [84,]    1    2   NA   NA   NA   NA   NA   NA\n [85,]    1    2   NA   NA   NA   NA   NA   NA\n [86,]    1    2   NA   NA   NA   NA   NA   NA\n [87,]    1    2   NA   NA   NA   NA   NA   NA\n [88,]    1    2   NA   NA   NA   NA   NA   NA\n [89,]    1    2   NA   NA   NA   NA   NA   NA\n [90,]    1    2   NA   NA   NA   NA   NA   NA\n [91,]    1    2   NA   NA   NA   NA   NA   NA\n [92,]    1    2   NA   NA   NA   NA   NA   NA\n [93,]    1    2   NA   NA   NA   NA   NA   NA\n [94,]    1    2   NA   NA   NA   NA   NA   NA\n [95,]    1    2   NA   NA   NA   NA   NA   NA\n [96,]    1    2   NA   NA   NA   NA   NA   NA\n [97,]    1    2   NA   NA   NA   NA   NA   NA\n [98,]    1    2   NA   NA   NA   NA   NA   NA\n [99,]    1    2   NA   NA   NA   NA   NA   NA\n[100,]    1    2   NA   NA   NA   NA   NA   NA\n[101,]    1    3    3    3   NA   NA   NA   NA\n[102,]    1    3    3    3   NA   NA   NA   NA\n[103,]    1    3    3    3   NA   NA   NA   NA\n[104,]    1    3    3    3   NA   NA   NA   NA\n[105,]    1    3    3    3   NA   NA   NA   NA\n[106,]    1    3    3    3   NA   NA   NA   NA\n[107,]    1    3    3    3   NA   NA   NA   NA\n[108,]    1    3    3    3   NA   NA   NA   NA\n[109,]    1    3    3    3   NA   NA   NA   NA\n[110,]    1    3    3    3   NA   NA   NA   NA\n[111,]    1    3    3    3   NA   NA   NA   NA\n[112,]    1    3    3    3   NA   NA   NA   NA\n[113,]    1    3    3    3   NA   NA   NA   NA\n[114,]    1    3    3    3   NA   NA   NA   NA\n[115,]    1    3    3    3   NA   NA   NA   NA\n[116,]    1    3    3    3   NA   NA   NA   NA\n[117,]    1    3    3    3   NA   NA   NA   NA\n[118,]    1    3    3    3   NA   NA   NA   NA\n[119,]    1    3    3    3   NA   NA   NA   NA\n[120,]    1    3    3    3   NA   NA   NA   NA\n[121,]    1    3    3    3   NA   NA   NA   NA\n[122,]    1    3    3    3   NA   NA   NA   NA\n[123,]    1    3    3    3   NA   NA   NA   NA\n[124,]    1    3    3    3   NA   NA   NA   NA\n[125,]    1    3    3    3   NA   NA   NA   NA\n[126,]    1    3    3    3   NA   NA   NA   NA\n[127,]    1    3    3    3   NA   NA   NA   NA\n[128,]    1    3    3    3   NA   NA   NA   NA\n[129,]    1    3    3    3   NA   NA   NA   NA\n[130,]    1    3    3    3   NA   NA   NA   NA\n[131,]    1    3    3    3   NA   NA   NA   NA\n[132,]    1    3    3    3   NA   NA   NA   NA\n[133,]    1    3    3    3   NA   NA   NA   NA\n[134,]    1    3    3    3   NA   NA   NA   NA\n[135,]    1    3    3    3   NA   NA   NA   NA\n[136,]    1    3    3    3   NA   NA   NA   NA\n[137,]    1    3    3    3   NA   NA   NA   NA\n[138,]    1    3    3    3   NA   NA   NA   NA\n[139,]    1    3    3    3   NA   NA   NA   NA\n[140,]    1    3    3    3   NA   NA   NA   NA\n[141,]    1    3    3    3   NA   NA   NA   NA\n[142,]    1    3    3    3   NA   NA   NA   NA\n[143,]    1    3    3    3   NA   NA   NA   NA\n[144,]    1    3    3    3   NA   NA   NA   NA\n[145,]    1    3    3    3   NA   NA   NA   NA\n[146,]    1    3    3    3   NA   NA   NA   NA\n[147,]    1    3    3    3   NA   NA   NA   NA\n[148,]    1    3    3    3   NA   NA   NA   NA\n[149,]    1    3    3    3   NA   NA   NA   NA\n[150,]    1    3    3    3   NA   NA   NA   NA\n[151,]    1    2    2    2   NA   NA   NA   NA\n[152,]    1    2    2    2   NA   NA   NA   NA\n[153,]    1    2    2    2   NA   NA   NA   NA\n[154,]    1    2    2    2   NA   NA   NA   NA\n[155,]    1    2    2    2   NA   NA   NA   NA\n[156,]    1    2    2    2   NA   NA   NA   NA\n[157,]    1    2    2    2   NA   NA   NA   NA\n[158,]    1    2    2    2   NA   NA   NA   NA\n[159,]    1    2    2    2   NA   NA   NA   NA\n[160,]    1    2    2    2   NA   NA   NA   NA\n[161,]    1    2    2    2   NA   NA   NA   NA\n[162,]    1    2    2    2   NA   NA   NA   NA\n[163,]    1    2    2    2   NA   NA   NA   NA\n[164,]    1    2    2    2   NA   NA   NA   NA\n[165,]    1    2    2    2   NA   NA   NA   NA\n[166,]    1    2    2    2   NA   NA   NA   NA\n[167,]    1    2    2    2   NA   NA   NA   NA\n[168,]    1    2    2    2   NA   NA   NA   NA\n[169,]    1    2    2    2   NA   NA   NA   NA\n[170,]    1    2    2    2   NA   NA   NA   NA\n[171,]    1    2    2    2   NA   NA   NA   NA\n[172,]    1    2    2    2   NA   NA   NA   NA\n[173,]    1    2    2    2   NA   NA   NA   NA\n[174,]    1    2    2    2   NA   NA   NA   NA\n[175,]    1    2    2    2   NA   NA   NA   NA\n[176,]    1    2    2    2   NA   NA   NA   NA\n[177,]    1    2    2    2   NA   NA   NA   NA\n[178,]    1    2    2    2   NA   NA   NA   NA\n[179,]    1    2    2    2   NA   NA   NA   NA\n[180,]    1    2    2    2   NA   NA   NA   NA\n[181,]    1    2    2    2   NA   NA   NA   NA\n[182,]    1    2    2    2   NA   NA   NA   NA\n[183,]    1    2    2    2   NA   NA   NA   NA\n[184,]    1    2    2    2   NA   NA   NA   NA\n[185,]    1    2    2    2   NA   NA   NA   NA\n[186,]    1    2    2    2   NA   NA   NA   NA\n[187,]    1    2    2    2   NA   NA   NA   NA\n[188,]    1    2    2    2   NA   NA   NA   NA\n[189,]    1    2    2    2   NA   NA   NA   NA\n[190,]    1    2    2    2   NA   NA   NA   NA\n[191,]    1    2    2    2   NA   NA   NA   NA\n[192,]    1    2    2    2   NA   NA   NA   NA\n[193,]    1    2    2    2   NA   NA   NA   NA\n[194,]    1    2    2    2   NA   NA   NA   NA\n[195,]    1    2    2    2   NA   NA   NA   NA\n[196,]    1    2    2    2   NA   NA   NA   NA\n[197,]    1    2    2    2   NA   NA   NA   NA\n[198,]    1    2    2    2   NA   NA   NA   NA\n[199,]    1    2    2    2   NA   NA   NA   NA\n[200,]    1    2    2    2   NA   NA   NA   NA\n[201,]    1    3    3    3    3    3    3    3\n[202,]    1    3    3    3    3    3    3    3\n[203,]    1    3    3    3    3    3    3    3\n[204,]    1    3    3    3    3    3    3    3\n[205,]    1    3    3    3    3    3    3    3\n[206,]    1    3    3    3    3    3    3    3\n[207,]    1    3    3    3    3    3    3    3\n[208,]    1    3    3    3    3    3    3    3\n[209,]    1    3    3    3    3    3    3    3\n[210,]    1    3    3    3    3    3    3    3\n[211,]    1    3    3    3    3    3    3    3\n[212,]    1    3    3    3    3    3    3    3\n[213,]    1    3    3    3    3    3    3    3\n[214,]    1    3    3    3    3    3    3    3\n[215,]    1    3    3    3    3    3    3    3\n[216,]    1    3    3    3    3    3    3    3\n[217,]    1    3    3    3    3    3    3    3\n[218,]    1    3    3    3    3    3    3    3\n[219,]    1    3    3    3    3    3    3    3\n[220,]    1    3    3    3    3    3    3    3\n[221,]    1    3    3    3    3    3    3    3\n[222,]    1    3    3    3    3    3    3    3\n[223,]    1    3    3    3    3    3    3    3\n[224,]    1    3    3    3    3    3    3    3\n[225,]    1    3    3    3    3    3    3    3\n[226,]    1    3    3    3    3    3    3    3\n[227,]    1    3    3    3    3    3    3    3\n[228,]    1    3    3    3    3    3    3    3\n[229,]    1    3    3    3    3    3    3    3\n[230,]    1    3    3    3    3    3    3    3\n[231,]    1    3    3    3    3    3    3    3\n[232,]    1    3    3    3    3    3    3    3\n[233,]    1    3    3    3    3    3    3    3\n[234,]    1    3    3    3    3    3    3    3\n[235,]    1    3    3    3    3    3    3    3\n[236,]    1    3    3    3    3    3    3    3\n[237,]    1    3    3    3    3    3    3    3\n[238,]    1    3    3    3    3    3    3    3\n[239,]    1    3    3    3    3    3    3    3\n[240,]    1    3    3    3    3    3    3    3\n[241,]    1    3    3    3    3    3    3    3\n[242,]    1    3    3    3    3    3    3    3\n[243,]    1    3    3    3    3    3    3    3\n[244,]    1    3    3    3    3    3    3    3\n[245,]    1    3    3    3    3    3    3    3\n[246,]    1    3    3    3    3    3    3    3\n[247,]    1    3    3    3    3    3    3    3\n[248,]    1    3    3    3    3    3    3    3\n[249,]    1    3    3    3    3    3    3    3\n[250,]    1    3    3    3    3    3    3    3\n[251,]    1    2    2    2    2    2    2    2\n[252,]    1    2    2    2    2    2    2    2\n[253,]    1    2    2    2    2    2    2    2\n[254,]    1    2    2    2    2    2    2    2\n[255,]    1    2    2    2    2    2    2    2\n[256,]    1    2    2    2    2    2    2    2\n[257,]    1    2    2    2    2    2    2    2\n[258,]    1    2    2    2    2    2    2    2\n[259,]    1    2    2    2    2    2    2    2\n[260,]    1    2    2    2    2    2    2    2\n[261,]    1    2    2    2    2    2    2    2\n[262,]    1    2    2    2    2    2    2    2\n[263,]    1    2    2    2    2    2    2    2\n[264,]    1    2    2    2    2    2    2    2\n[265,]    1    2    2    2    2    2    2    2\n[266,]    1    2    2    2    2    2    2    2\n[267,]    1    2    2    2    2    2    2    2\n[268,]    1    2    2    2    2    2    2    2\n[269,]    1    2    2    2    2    2    2    2\n[270,]    1    2    2    2    2    2    2    2\n[271,]    1    2    2    2    2    2    2    2\n[272,]    1    2    2    2    2    2    2    2\n[273,]    1    2    2    2    2    2    2    2\n[274,]    1    2    2    2    2    2    2    2\n[275,]    1    2    2    2    2    2    2    2\n[276,]    1    2    2    2    2    2    2    2\n[277,]    1    2    2    2    2    2    2    2\n[278,]    1    2    2    2    2    2    2    2\n[279,]    1    2    2    2    2    2    2    2\n[280,]    1    2    2    2    2    2    2    2\n[281,]    1    2    2    2    2    2    2    2\n[282,]    1    2    2    2    2    2    2    2\n[283,]    1    2    2    2    2    2    2    2\n[284,]    1    2    2    2    2    2    2    2\n[285,]    1    2    2    2    2    2    2    2\n[286,]    1    2    2    2    2    2    2    2\n[287,]    1    2    2    2    2    2    2    2\n[288,]    1    2    2    2    2    2    2    2\n[289,]    1    2    2    2    2    2    2    2\n[290,]    1    2    2    2    2    2    2    2\n[291,]    1    2    2    2    2    2    2    2\n[292,]    1    2    2    2    2    2    2    2\n[293,]    1    2    2    2    2    2    2    2\n[294,]    1    2    2    2    2    2    2    2\n[295,]    1    2    2    2    2    2    2    2\n[296,]    1    2    2    2    2    2    2    2\n[297,]    1    2    2    2    2    2    2    2\n[298,]    1    2    2    2    2    2    2    2\n[299,]    1    2    2    2    2    2    2    2\n[300,]    1    2    2    2    2    2    2    2\n\n\nas well as our vectors for the response on each trial\n\n\nCode\nresponse\n\n\nx1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x2 x1 x1 \n 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1 \nx1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x2 x1 x1 x1 x1 x1 x1 \n 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1 \nx1 x1 x1 x1 x1 x1 x1 x1 x2 x1 x1 x2 x2 x2 x1 x1 x1 x1 x2 x1 x1 x1 x1 x1 x1 x1 \n 1  1  1  1  1  1  1  1  2  1  1  2  2  2  1  1  1  1  2  1  1  1  1  1  1  1 \nx1 x1 x1 x1 x1 x1 x2 x2 x1 x2 x2 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 \n 1  1  1  1  1  1  2  2  1  2  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 \nx1 x1 x2 x1 x1 x1 x3 x2 x1 x1 x2 x1 x1 x3 x1 x1 x2 x1 x1 x1 x2 x1 x1 x4 x1 x1 \n 1  1  2  1  1  1  3  2  1  1  2  1  1  3  1  1  2  1  1  1  2  1  1  4  1  1 \nx1 x1 x1 x1 x4 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x4 x1 x1 x1 x1 x3 x4 x1 x3 x3 x3 \n 1  1  1  1  4  1  1  1  1  1  1  1  1  1  1  4  1  1  1  1  3  4  1  3  3  3 \nx3 x1 x4 x1 x2 x1 x2 x1 x2 x1 x1 x4 x1 x1 x4 x1 x3 x1 x2 x1 x2 x3 x2 x1 x1 x1 \n 3  1  4  1  2  1  2  1  2  1  1  4  1  1  4  1  3  1  2  1  2  3  2  1  1  1 \nx1 x2 x1 x1 x1 x3 x1 x2 x1 x1 x2 x2 x3 x1 x4 x1 x1 x1 x5 x1 x1 x1 x8 x1 x1 x1 \n 1  2  1  1  1  3  1  2  1  1  2  2  3  1  4  1  1  1  5  1  1  1  8  1  1  1 \nx1 x1 x1 x1 x4 x1 x1 x1 x1 x1 x1 x6 x3 x1 x1 x1 x1 x6 x1 x1 x1 x6 x8 x1 x1 x1 \n 1  1  1  1  4  1  1  1  1  1  1  6  3  1  1  1  1  6  1  1  1  6  8  1  1  1 \nx1 x1 x1 x1 x1 x1 x5 x5 x1 x8 x1 x1 x1 x1 x1 x2 x7 x7 x1 x2 x4 x1 x1 x1 x2 x1 \n 1  1  1  1  1  1  5  5  1  8  1  1  1  1  1  2  7  7  1  2  4  1  1  1  2  1 \nx8 x4 x1 x1 x7 x1 x1 x3 x1 x1 x1 x2 x1 x1 x7 x1 x4 x2 x1 x8 x8 x1 x4 x6 x7 x7 \n 8  4  1  1  7  1  1  3  1  1  1  2  1  1  7  1  4  2  1  8  8  1  4  6  7  7 \nx1 x2 x1 x3 x8 x1 x4 x1 x4 x5 x6 x2 x2 x1 \n 1  2  1  3  8  1  4  1  4  5  6  2  2  1 \n\n\nand the response time on each trial\n\n\nCode\nrt\n\n\n  [1] 1.136 1.101 1.486 0.996 2.043 1.382 0.707 0.847 2.243 0.693 0.730 0.806\n [13] 0.802 1.124 1.344 1.546 2.010 1.841 0.872 0.842 1.963 0.713 1.208 1.815\n [25] 0.835 1.085 1.266 1.825 1.375 1.861 1.673 1.858 1.324 1.014 1.641 0.946\n [37] 1.185 1.218 1.597 1.051 0.693 1.790 0.637 0.715 0.562 2.067 2.230 1.848\n [49] 1.058 1.741 0.622 0.820 0.513 0.949 1.324 0.818 1.033 0.604 0.741 0.890\n [61] 0.635 0.671 0.612 1.055 0.878 1.054 1.021 1.444 0.844 1.069 0.843 1.347\n [73] 1.013 1.190 0.664 1.068 0.780 0.887 0.678 1.048 0.939 0.522 0.610 0.759\n [85] 0.928 1.490 1.495 0.810 1.180 1.248 0.948 1.064 0.696 1.127 0.703 1.098\n [97] 0.925 1.023 0.802 0.857 1.059 0.643 0.906 1.362 1.182 1.383 1.069 0.721\n[109] 0.966 1.766 1.186 1.764 1.200 0.802 0.957 0.870 0.762 1.328 0.717 0.875\n[121] 1.167 0.762 0.881 2.739 2.425 1.378 1.231 1.073 0.900 1.812 0.860 0.782\n[133] 1.676 0.699 2.438 1.013 1.322 0.765 0.735 1.546 1.098 1.013 1.553 0.842\n[145] 1.384 1.729 0.851 0.900 1.505 1.736 0.607 1.279 0.704 0.537 0.933 1.349\n[157] 1.739 0.805 0.991 0.864 1.159 0.799 0.855 1.569 0.513 0.802 1.416 1.123\n[169] 1.201 0.764 0.658 1.417 1.054 0.757 0.795 0.799 1.025 0.986 0.792 0.898\n[181] 0.661 1.025 0.791 1.323 0.847 1.377 1.034 0.571 0.939 1.207 1.283 2.040\n[193] 1.200 0.986 0.736 1.062 0.549 0.677 0.863 0.847 1.065 0.929 1.000 0.792\n[205] 0.774 0.823 0.997 0.831 0.938 0.968 1.329 1.726 1.359 0.555 0.770 1.358\n[217] 0.492 1.361 0.806 1.713 1.617 1.124 0.846 1.028 0.494 1.828 0.637 1.457\n[229] 1.964 0.636 1.435 0.970 0.652 0.755 1.287 0.801 1.091 0.698 1.575 0.613\n[241] 0.777 0.717 0.640 1.480 0.759 0.567 1.047 0.693 1.204 0.756 0.644 0.944\n[253] 0.662 1.048 0.969 0.692 1.024 0.990 0.561 0.752 0.941 0.657 0.672 1.156\n[265] 0.647 1.046 0.683 0.583 0.699 0.977 0.823 0.698 0.536 0.888 0.840 0.882\n[277] 0.694 0.848 0.589 0.586 0.880 0.616 1.350 0.755 0.728 1.024 0.574 1.259\n[289] 1.166 0.738 0.841 0.621 0.761 0.611 0.908 1.407 1.049 1.232 0.754 1.079\n\n\nThese will be the data to which we will fit our race model.\n\n\n8.3.2 Fitting parameters\nFinally, we are in a position to fit the race model to our (simulated) data. We will be using R’s built-in nlminb function for this purpose, which operates very similarly to the optim function in R. The reason we are using nlminb here is because our model parameters should be bounded, i.e., restricted to fall within particular ranges. nlminb is a bit less clunky in this situation.\nGenerally speaking, calling nlminb will look like this:\n\n\nCode\nnlminb(\n    # A vector of initial values that are the \"starting point\" for the search\n    start = ___,\n    # The function that we want to optimize (i.e., get to be as small as possible)\n    objective = ___,\n    # Lower and upper bounds on model parameters (these can be infinite)\n    lower = ___, upper = ___,\n    # Other options to set\n    control = list(___),\n    # Other arguments which will be conveyed to the `objective` to be optimized\n    ...\n)\n\n\nIt is clear that we first need an initial set of parameter values. Our initial guess does not need to be very good, but it should at least be sensible. We will fit the model assuming different drift rates for each item type and a single threshold and residual time:\n\n\nCode\nstart &lt;- c(\n    \"v[1]\" = 0,\n    \"v[2]\" = 0,\n    \"v[3]\" = 0,\n    \"a\" = 1,\n    \"t0\" = 0\n)\n\n\nThe values above were chosen because they are typical of what we might use as a “default” when fitting real data and we don’t know what the true values might be. We also need to specify the lower and upper bounds on each parameter; these bounds are also named vectors:\n\n\nCode\nlower &lt;- c(\n    \"v[1]\" = -Inf, # Drift rates allowed to be negative\n    \"v[2]\" = -Inf,\n    \"v[3]\" = -Inf,\n    \"a\" = 0,       # Thresholds must be nonnegative, since accumulators start at zero\n    \"t0\" = 0       # Minimum residual time is zero\n)\n\nupper &lt;- c(\n    \"v[1]\" = Inf, # Drift rates and threshold can be as high as they need\n    \"v[2]\" = Inf,\n    \"v[3]\" = Inf,\n    \"a\" = Inf,\n    \"t0\" = min(rt) # Residual time cannot be larger than the smallest RT\n)\n\n\nFinally, we have everything we need to fit the model:\n\n\nCode\nfit &lt;- nlminb(\n    start = start,\n    objective = race_nll,\n    lower = lower, upper = upper,\n    # Other options to set (\"eval.max\" is the most common one to play with)\n    control = list(eval.max = 1000),\n    rt = rt,\n    response = response,\n    items = items\n)\n\n\nNow let’s open our present:\n\n\nCode\nfit\n\n\n$par\n      v[1]       v[2]       v[3]          a         t0 \n2.02086803 0.95210575 0.02240357 1.98372418 0.20802591 \n\n$objective\n[1] 350.1683\n\n$convergence\n[1] 0\n\n$iterations\n[1] 44\n\n$evaluations\nfunction gradient \n      54      242 \n\n$message\n[1] \"relative convergence (4)\"\n\n\nAs we can see, the fit we got back from the nlminb function is actually a list. fit$par is a named vector giving us the best-fitting parameters, fit$objective is the best (smallest) value of NLL the model found. We also get some additional information, the most pertinent of which being fit$convergence—if this value is 0, we are probably okay. Be sure to check out the help page on nlminb for more information and examples!\nFor now, it is worth noting that the estimated values of the model parameters are pretty close to those we actually used to simulate the data.\n\n\n8.3.3 Model comparison\nRecall that our simulated data was from a visual search task in which distractors were allowed to vary in how similar they were to the target. Specifically, there were high-similarity distractor (with drift rates of 1) and low-similarity distractors (with drift rates of 0). In a real research context, we might be interested in testing the hypothesis that the two distractor types have different drift rates. We would address that question by fitting the model we used above, which allows for different drift rates for the two types, and also fitting a model in which we allow only a single drift rate parameter that applies to all distractors. We would then use AIC and BIC to decide whether the additional drift rate parameter improved the model fit enough to justify adding the parameter.\nLet’s see how that would go. How can we implement a model that assigns a single drift rate to both types of distractors? We will create a modified version of the items matrix that replaces all the 3s with 2s. We can then fit the model using this modified version. The effect of this will be to apply the same drift rate (which will be v[2] in our simpler model) to both types of foils. The code below accomplishes this.\n\n\nCode\n# Create simpler version of the items matrix\nitems_simple &lt;- items\n# Replace all 3's with 2's\nitems_simple[items_simple == 3] &lt;- 2\n\nstart_simple &lt;- c(\n    \"v[1]\" = 0,\n    \"v[2]\" = 0,\n    \"a\" = 1,\n    \"t0\" = 0\n)\n\nlower_simple &lt;- c(\n    \"v[1]\" = -Inf, # Drift rates allowed to be negative\n    \"v[2]\" = -Inf,\n    \"a\" = 0,       # Thresholds must be nonnegative, since accumulators start at zero\n    \"t0\" = 0       # Minimum residual time is zero\n)\n\nupper_simple &lt;- c(\n    \"v[1]\" = Inf, # Drift rates and threshold can be as high as they need\n    \"v[2]\" = Inf,\n    \"a\" = Inf,\n    \"t0\" = min(rt) # Residual time cannot be larger than the smallest RT\n)\n\nfit_simple &lt;- nlminb(\n    start = start_simple,\n    objective = race_nll,\n    lower = lower_simple, upper = upper_simple,\n    control = list(eval.max = 1000),\n    rt = rt,\n    response = response,\n    items = items_simple\n)\n\n\nAnd here’s the result:\n\n\nCode\nfit_simple\n\n\n$par\n     v[1]      v[2]         a        t0 \n1.7622345 0.1271764 1.6013550 0.2934703 \n\n$objective\n[1] 374.5381\n\n$convergence\n[1] 0\n\n$iterations\n[1] 26\n\n$evaluations\nfunction gradient \n      38      121 \n\n$message\n[1] \"relative convergence (4)\"\n\n\nAs before, we can compute AIC for each model:\n\n\nCode\n2 * fit$objective + 2 * length(fit$par)\n\n\n[1] 710.3365\n\n\nCode\n2 * fit_simple$objective + 2 * length(fit_simple$par)\n\n\n[1] 757.0762\n\n\nas well as BIC\n\n\nCode\n2 * fit$objective + log(length(rt)) * length(fit$par)\n\n\n[1] 728.8554\n\n\nCode\n2 * fit_simple$objective + log(length(rt)) * length(fit_simple$par)\n\n\n[1] 771.8914\n\n\nOn both counts, the original model (fit) is preferred, confirming that the two distractor types are best explained as having two different drift rates.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Accumulator models of choice and response time</span>"
    ]
  },
  {
    "objectID": "accumulator_models.html#decision-rules",
    "href": "accumulator_models.html#decision-rules",
    "title": "8  Accumulator models of choice and response time",
    "section": "8.4 Decision rules",
    "text": "8.4 Decision rules\nOne final thing to note is that the race models we have explored so far assume that responses are based on whichever accumulator wins the race. This represents a decision rule that is self-terminating or minimum time. Alternatively, one could imagine tasks in which decisions could only be made once all accumulators had finished, amounting to an exhaustive or maximum time decision rule.\nFor example, if the visual search task required participants not to pick which item was the target, but to decide whether or not a target was present at all, this would imply different decision rules for different responses. Responding that a target was present at all could be done using a self-terminating rule—if any accumulator reaches threshold, that could cause a participant to decide that there was a target in the display. On the other hand, saying that there were no targets in the display would require exhaustively processing each item to verify that it was not a target. This latter task constraint could imply that each accumulator has both upper and lower bounds, where the lower bound corresponds to a decision that the item is not a target. Alternatively, each item might be associated with two accumulators, one that accumulates evidence that supports the item being a target and another that accumulates evidence that the item is not a target. Finally, one could retain the same racing diffusion model we’ve been developing but include an additional no target accumulator that races against the accumulators for each item—if the “no target” accumulator wins, the participant decides that none of the items were targets.\nThe point in laying out these possibilities is that, once we consider models with many possibly overlapping and interacting processes, we must also think carefully about how the results of those processes translate into observed behavior. There are many different ways of processing and combining information from multiple sources, each of which could lead to a different model of the task. The value of modeling is that this cornucopia of possibilities need not be overwhelming—we can build models that instantiate these different possibilities and see which best account for our data. There are even methods, known as Systems Factorial Technology (Harding et al., 2016; Houpt et al., 2014; Townsend & Nozawa, 1995), that are specifically designed to obtain data to distinguish between different models in these situations.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Accumulator models of choice and response time</span>"
    ]
  },
  {
    "objectID": "accumulator_models.html#excursus-accumulators-with-negative-drift-rates",
    "href": "accumulator_models.html#excursus-accumulators-with-negative-drift-rates",
    "title": "8  Accumulator models of choice and response time",
    "section": "8.5 Excursus: Accumulators with negative drift rates",
    "text": "8.5 Excursus: Accumulators with negative drift rates\nYou may have noticed above that, when we fit our race model, we allowed the drift rates for accumulators to be negative. When we were working with diffusion models, this was no big deal—a negative drift rate just means that samples of evidence are more likely to favor the choice associated with the lower bound than the upper bound. The accumulators in our race model, though, do not have a lower bound and so an accumulator with a negative drift rate could actually keep going forever without reaching threshold.\nMathematically, we can actually compute how often that would happen. This involves taking a look at the formula for the CDF. Since the CDF is the probability that the accumulator has reached threshold by time \\(t\\), we can find the probability that the accumulator ever reaches threshold by taking the limit of the CDF as \\(t\\) approaches infinity. As illustrated below, when the drift rate \\(v_i \\geq 0\\), this limit equals 1, meaning with nonnegative drift rates the accumulator will eventually reach threshold (though it may take a long time). When the drift rate \\(v_i &lt; 0\\), though, the limit is \\(\\exp \\left( 2 a_i v_i \\right)\\).\n\\[\\begin{align}\n\\lim_{t \\rightarrow \\infty} F_i \\left( t \\mid a_i, v_i, t_{0i} \\right) & =\n    \\begin{cases}\n        \\Phi \\left( \\infty \\right) + \\exp \\left( 2 a_i v_i \\right) \\Phi \\left( -\\infty \\right) & \\text{if } v_i &gt; 0 \\\\\n        \\Phi \\left( 0 \\right) + \\exp \\left( 0 \\right) \\Phi \\left( 0 \\right) & \\text{if } v_i = 0 \\\\\n        \\Phi \\left( -\\infty \\right) + \\exp \\left( 2 a_i v_i \\right) \\Phi \\left( \\infty \\right) & \\text{if } v_i &lt; 0\n    \\end{cases} \\\\\n    & = \\begin{cases}\n        1 + \\exp \\left( 2 a_i v_i \\right) \\times 0 & \\text{if } v_i &gt; 0 \\\\\n        \\frac{1}{2} + 1 \\times \\frac{1}{2} & \\text{if } v_i = 0 \\\\\n        0 + \\exp \\left( 2 a_i v_i \\right) \\times 1 & \\text{if } v_i &lt; 0\n    \\end{cases} \\\\\n    & = \\begin{cases}\n        1 & \\text{if } v_i \\geq 0 \\\\\n        \\exp \\left( 2 a_i v_i \\right) & \\text{if } v_i &lt; 0\n    \\end{cases}\n\\end{align}\\]\nThis behavior is illustrated in the graphs below. In particular, note the asymptotic level of the CDF functions in the right set of plots as the drift rate grows more negative.\n\n\nCode\ntoPlot &lt;- expand_grid(t = seq(0, 3, length.out = 101), a = c(1, 2), v = seq(-3, 3, length.out = 7)) %&gt;%\n    mutate(threshold_factor = factor(a, levels = sort(unique(a)), labels = paste(\"Threshold =\", sort(unique(a))))) %&gt;%\n    mutate(d = accum_pdf(t = t, a = a, v = v, t0 = 0)) %&gt;%\n    mutate(p = accum_cdf(t = t, a = a, v = v, t0 = 0))\n\npdf_plot &lt;- toPlot %&gt;%\n    ggplot(aes(x = t, y = d, color = v, group = factor(v))) +\n    geom_line() +\n    scale_color_gradient2(mid = \"#bbbbbb\") +\n    facet_wrap(\"threshold_factor\", ncol = 1) +\n    labs(x = \"Time (s)\", y = \"Accumulator PDF\", color = \"Drift rate\")\n\ncdf_plot &lt;- toPlot %&gt;%\n    ggplot(aes(x = t, y = p, color = v, group = factor(v))) +\n    geom_line() +\n    scale_color_gradient2(mid = \"#bbbbbb\") +\n    expand_limits(y = c(0, 1)) +\n    facet_wrap(\"threshold_factor\", ncol = 1) +\n    labs(x = \"Time (s)\", y = \"Accumulator CDF\", color = \"Drift rate\")\n\nprint(\n    pdf_plot + cdf_plot + plot_layout(guides = \"collect\", nrow = 1)\n)\n\n\n\n\n\n\n\n\n\nIf there were only one accumulator involved, we would generally not want to allow for negative drift rates, since this would predict situations in which someone literally never responded. (To my knowledge, no one has reported such a result, but maybe we just haven’t waited long enough.) In models with multiple accumulators, though, all that matters is that at least one accumulator has a nonnegative drift rate—that way, there will always be a winner that will initiate a response. Indeed, in models with feedforward competition, it may be reasonable to allow that competition to result in negative drift rates, since this would effectively suppress responses associated with those accumulators.\nThe larger point is that negative drift rates need not be excluded outright. There may be theoretical reasons to allow for negative drift rates. Moreover, in models that allow for negative drift rates, the optimization algorithm will tend to avoid combinations of parameter values that result in too many negative drift rates since these would be associated with a low likelihood of making a response.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Accumulator models of choice and response time</span>"
    ]
  },
  {
    "objectID": "accumulator_models.html#exercises",
    "href": "accumulator_models.html#exercises",
    "title": "8  Accumulator models of choice and response time",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\n\nConsider potential psychological interpretations of feedforward inhibition and lateral inhibition. In particular, see if you can relate these two forms of competition to the constructs of attention and capacity.\nAs noted above, diffusion models explain slow errors in terms of trial-by-trial variability in drift rates, while accumulator models explain slow errors in terms of lateral competition between options. Compare and contrast theses. Are there circumstances in which one explanation may be more plausible than the other? Would it be possible to ascribe trial-by-trial variability to lateral competition?\nAs noted in the chapter, it is possible to calculate the negative log-likelihood for the race model including feedforward competition. Implement this mechanism in the race_nll function. To get you started, the chunk of code below contains comments that suggest how this might be done.\n\n\n\nCode\nrace_nll &lt;- function(par, response, rt, items, n_items = max(items, na.rm = TRUE)) {\n    n_trials &lt;- nrow(items)\n    n_locations &lt;- ncol(items)\n    \n    if (is.na(par[\"v\"])) {\n        v &lt;- par[paste0(\"v[\", 1:n_items, \"]\")]\n    } else {\n        v &lt;- rep(par[\"v\"], n_items)\n    }\n    \n    if (is.na(par[\"a\"])) {\n        a &lt;- par[paste0(\"a[\", 1:n_locations, \"]\")]\n    } else {\n        a &lt;- rep(par[\"a\"], n_locations)\n    }\n    \n    if (is.na(par[\"t0\"])) {\n        t0 &lt;- par[paste0(\"t0[\", 1:n_locations, \"]\")]\n    } else {\n        t0 &lt;- rep(par[\"t0\"], n_locations)\n    }\n    \n    # Similar to the above, you can check and see whether a feedforward competition parameter is or is not included; if not, you'll want to set the value of feedforward to zero.\n    if (is.na(par[\"feedforward\"])) {\n        feedforward &lt;- ...\n    } else {\n        feedforward &lt;- ...\n    }\n    \n    nll &lt;- rep(0, n_trials)\n    \n    for (i in 1:n_trials) {\n        # Since drift rates depend on the items presented on each trial, we'll need to calculate them\n        trial_v &lt;- v[items[i, ]]\n        \n        for (j in 1:length(trial_v)) {\n            trial_v[j] &lt;- ...\n        }\n        \n        # Note that the drift rates now refer to \"trial_v\"\n        nll[i] &lt;- -log(\n            accum_pdf(t = rt[i], a = a[response[i]], v = trial_v[response[i]], t0 = t0[response[i]]) *\n                prod(1 - accum_cdf(t = rt[i], a = a[-response[i]], v = trial_v[-response[i]], t0 = t0[-response[i]]), na.rm = TRUE)\n        )\n    }\n    \n    return(sum(nll))\n}\n\n\n\nThe Leaky Competing Accumulator (LCA) model was proposed by Usher & McClelland (2001). In addition to lateral competition, the LCA (as the name implies) assumes that evidence “leaks” out of accumulators at a constant rate. Take a look at their paper (specifically equation 4 on page 559) and modify our race model simulation code to include a leakage parameter. Describe how you did this (and share your code too!).\nAn extension to the racing diffusion model explored in this chapter was proposed by Tillman et al. (2020). Their model allows for trial-by-trial variability in the thresholds of the accumulators. Their Equation 5 and Appendix A provide formulae for the PDF and CDF of the resulting accumulators. Translate these formulae into R code that implements their extended racing diffusion model.\nThe accumulator models we explored in the chapter are all stochastic accumulators, since value of the evidence being accumulated fluctuates randomly around a mean. Brown & Heathcote (2008) proposed a Linear Ballistic Accumulator (LBA) model which only has between-trial variability but is otherwise deterministic. Despite this unrealistic assumption, the model fits data well and often leads to similar conclusions as drawn from stochastic models of evidence accumulation (Donkin et al., 2011). Referring to the equations provided by Brown & Heathcote (2008) for the PDF and CDF of their Linear Ballistic Accumulators, implement their model.\n\n\n\n\n\nBrown, S., & Heathcote, A. (2008). The simplest complete model of choice response time: Linear ballistic accumulation. Cognitive Psychology, 57, 153–178.\n\n\nDonkin, C., Brown, S., Heathcote, A., & Wagenmakers, E.-J. (2011). Diffusion versus linear ballistic accumulation: Different models but the same conclusions about psychological processes? Psychonomic Bulletin & Review, 55, 140–151.\n\n\nHarding, B., Goulet, M.-A., Jolin, S., Tremblay, C., Villeneuve, S.-P., & Durand, G. (2016). Systems factorial technology explained to humans. The Quantitative Methods for Psychology, 12(1), 39–56.\n\n\nHoupt, J. W., Blaha, L. M., McIntire, J. P., Havig, P. R., & Townsend, J. T. (2014). Systems factorial technology with R. Behavior Research Methods, 46, 307–330.\n\n\nPurcell, B. A., Heitz, R. P., Cohen, J. Y., Schall, J. D., Logan, G. D., & Palmeri, T. J. (2010). Neurally constrained modeling of perceptual decision making. Psychological Review, 117(4), 1113–1143. https://doi.org/10.1037/a0020311\n\n\nPurcell, B. A., Schall, J. D., Logan, G. D., & Palmeri, T. J. (2012). From salience to saccades: Multiple-alternative gated stochastic accumulator model of visual search. Journal of Neuroscience, 32(10), 3433–3446. https://doi.org/10.1523/JNEUROSCI.4622-11.2012\n\n\nRaab, D. H. (1962). Statistical facilitation of simple reaction times. Transactions of the New York Academy of Sciences, 24(5 Series II), 574–590.\n\n\nTeodorescu, A. R., & Usher, M. (2013). Disentangling decision models: From independence to competition. Psychological Review, 120(1), 1–38.\n\n\nTillman, G., Van Zandt, T., & Logan, G. D. (2020). Sequential sampling models without random between-trial variability: The racing diffusion model of speeded decision making. Psychonomic Bulletin & Review, 27(5), 911–936. https://doi.org/10.3758/s13423-020-01719-6\n\n\nTownsend, J. T., & Nozawa, G. (1995). Spatio-temporal properties of elementary perception: An investigation of parallel, serial, and coactive theories. Journal of Mathematical Psychology, 39, 321–359.\n\n\nUsher, M., & McClelland, J. L. (2001). The time course of perceptual choice: The leaky, competing accumulator model. Psychological Review, 108(3), 550–592.",
    "crumbs": [
      "Modeling behavior",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Accumulator models of choice and response time</span>"
    ]
  },
  {
    "objectID": "ebrw.html",
    "href": "ebrw.html",
    "title": "9  Exemplar of the Exemplar Based Random Walk",
    "section": "",
    "text": "9.1 Summed similarity\nSo far, we have focused on models that explain behavior (choice and response time) in terms of how representations of “evidence” are processed via some form of accumulation. We pointed out that the “evidence” in these models was an abstract quantity that would be interpreted differently depending on the application—the “evidence” for Buridan’s Ass was largely perceptual in nature, based on the visual features of his two hay piles; the “evidence” in a recognition memory task came from processes that compared a probe item to the contents of memory; the “evidence” in a visual search task cam from processes that compared the items in the display to a representation of the search target. In this chapter, we begin to dig deeper by explaining where this evidence comes from in a particular task domain. Specifically, we will explore the Exemplar-Based Random Walk (EBRW) model (Nosofsky et al., 2011; Nosofsky & Palmeri, 1997). We will see how this model explains choice and response time in a recognition memory task by explaining how the relevant evidence is derived.\nThe EBRW is a dynamic extension of the earlier Generalized Context Model (GCM) (Nosofsky, 1986). According to both tbe EBRW and GCM, each time you experience an item in a particular context, it leaves a “trace” in memory. This trace is called an “exemplar” or an “instance” and it consists of a record of the values the item had for particular features or attributes. When you are presented with a probe item, it activates each of these exemplars in memory in proportion to how similar the features of the probe are to the features of the exemplars in memory.\nThe EBRW was originally developed to account for choice and RT in categorization tasks. In what follows, we apply EBRW to a recognition task. Recognition decisions are based on the summed activation across all exemplars in memory, which is termed the “familiarity” of the probe. The extent to which familiarity is higher than a criterion determines the extent to which the probe will be recognized as having been seen before. Specifically, familiarity determines the rate at which evidence accumulates toward either a “yes” or “no” decision regarding whether the probe item was seen before. If the accumulated evidence reaches an upper threshold, a “yes” decision is made; otherwise, if the accumulated evidence reaches a lower threshold, a “no” decision is made.\nThe text below first describes how familiarity is determined by the summed similarity between a probe and the exemplars in memory, using the concrete example of items consisting of patches of color. Then, the dynamics of evidence accumulation are described. These dynamics are first described as a random walk between the thresholds for “yes” and “no” decisions. Then, we illustrate how the same dynamics can be described in continuous time as a diffusion process instead of a random walk (Nosofsky et al., 2014). Finally, we see how the continuous diffusion process makes it easier to fit the parameters of the EBRW model to recognition data.\nFor concreteness, we imagine an experiment in which several color patches were studied. Colors can be described in terms of three features: hue, chroma (or “saturation”), and luminance (or “brightness”). Imagine that you saw many reddish-hued colors that varied in chroma and luminance. According to the EBRW, each of the colors you saw would leave a trace in memory. Each trace would consist of the values of the color you saw along those three dimensions. In this example, all the colors have the same value for “hue”, which is 0. The value of “0” for “hue” corresponds to a reddish color. The colors differ in their values for chroma and luminance, as illustrated in the graph below:\nCode\ncolDF &lt;- expand_grid(h = hue, c = seq(30, 70, length.out = 5), l = seq(30, 70, length.out = 5)) %&gt;%\n    mutate(col = hcl(h, c, l))\n\ncolDF %&gt;%\n    ggplot(aes(x = c, y = l, fill = col)) +\n    geom_tile(width = 2.5, height = 2.5) +\n    scale_fill_identity() +\n    coord_equal() +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", caption = bquote(plain(hue) == .(round(hue))))\nYour memory traces can also be written as a matrix, where each row corresponds to an item (color) and each column corresponds to a feature (h for “hue”, c for “chroma”, and l for “luminance”):\nCode\nknitr::kable(head(colDF %&gt;% select(h, c, l), 10), row.names = TRUE)\n\n\n\n\n\n\nh\nc\nl\n\n\n\n\n1\n0\n30\n30\n\n\n2\n0\n30\n40\n\n\n3\n0\n30\n50\n\n\n4\n0\n30\n60\n\n\n5\n0\n30\n70\n\n\n6\n0\n40\n30\n\n\n7\n0\n40\n40\n\n\n8\n0\n40\n50\n\n\n9\n0\n40\n60\n\n\n10\n0\n40\n70\nThese coordinates will often be derived using Multidimensional Scaling (MDS) (Nosofsky, 1992; Shepard, 1962a, 1962b). This type of analysis derives a spatial representation of a set of stimuli on the basis of similarity judgments provided by raters.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#summed-similarity",
    "href": "ebrw.html#summed-similarity",
    "title": "9  Exemplar of the Exemplar Based Random Walk",
    "section": "",
    "text": "9.1.1 Perceived distance and similarity\nThe perceived distance between any two of these colors is the Euclidean distance between their feature values, weighted by the degree of attention given to each feature: \\[\nd_{ij} = \\sqrt{\\sum_{k = 1}^{N_F} w_k \\left(x_{ik} - x_{jk} \\right)^2}\n\\] where \\(w_k\\) is the weight given to feature \\(k\\), \\(x_{ik}\\) is the value of item \\(i\\) on feature \\(k\\), \\(x_{jk}\\) is the value of item \\(j\\) on feature \\(k\\), \\(d_{ij}\\) is the perceived distance between items \\(i\\) and \\(j\\), and \\(N_F\\) is the number of features which in this example is 3 (hue, chroma, and luminance). Note also that while Euclidean distance is appropriate for colors, other types of distance metrics may be more appropriate for other kinds of stimuli, particularly those with separable dimensions which are perceived independently from one another (e.g., Garner & Felfoldy, 1970).\nPerceived similarity between two items, \\(s_{ij}\\), is an exponential function of the psychological distance between those items: \\[\ns_{ij} = \\exp(-c d_{ij})\n\\] where \\(c\\) is a sensitivity parameter that controls how quickly perceived similarity decreases with distance, as illustrated in the graph below:\n\n\nCode\nexpand_grid(d = seq(0, 10, length.out = 151), c = seq(0.25, 3, by = 0.25)) %&gt;%\n    mutate(s = exp(-c * d)) %&gt;%\n    ggplot(aes(x = d, y = s, color = c, group = c)) +\n    geom_line() +\n    scale_color_continuous_sequential() +\n    labs(x = expression(d[ij]), y = expression(s[ij])) +\n    theme(legend.position = \"inside\", legend.position.inside = c(1, 1), legend.justification = c(1, 1))\n\n\n\n\n\n\n\n\n\n\n\n9.1.2 Summed similarity\nImagine that you have just been shown two colors from among the set shown earlier. We then present you with a third color—a probe item—and ask whether it was one of the two you just saw. You answer this question on the basis of the summed similarity between the probe item and the two items in memory. The probe item has its own vector of feature values, \\(\\mathbf{x_q}\\). The perceived distance between the probe item and each of the memory items is, as above, the Euclidean distance between the probe’s feature values and those of the memory item: \\[\n\\begin{align}\nd_{qj} & = \\sqrt{\\sum_{k = 1}^{N_F} w_k \\left(x_{qk} - x_{jk} \\right)^2} \\\\\ns_{qj} & = \\exp \\left( -c d_{qj} \\right) \\\\\nS & = \\sum_{j = 1}^{N_M} s_{qj}\n\\end{align}\n\\] and, again, the perceived similarity \\(s_{qj}\\) between the probe \\(q\\) and memory item \\(j\\) is an exponential function of perceived distance \\(d_{qj}\\). Finally, summed similarity \\(S\\) is the sum of the perceived similarities across all \\(N_M\\) items in memory.\nThe graphs below illustrate how this works. The left graph shows contours of equal similarity from each of two study items. The right graph shows summed similarity as a function of the chroma and luminance of a probe item (assuming the same hue).\n\n\nCode\nX &lt;- matrix(\n    c(40, 60,\n      60, 50),\n    nrow = 2,\n    ncol = 2,\n    byrow = TRUE\n)\n\nsens &lt;- 0.05\n\ncolDF &lt;- tibble(h = hue, c = X[,1], l = X[,2]) %&gt;%\n    mutate(col = hcl(h, c, l))\n\nitem_cols &lt;- as.vector(colDF$col)\nnames(item_cols) &lt;- as.character(1:length(item_cols))\n\nsimDF &lt;- expand_grid(c = seq(30, 70, length.out=101), l = seq(30, 70, length.out=101), item = c(1, 2)) %&gt;%\n    mutate(d = sqrt((c - X[item, 1])^2 + (l - X[item, 2])^2)) %&gt;%\n    mutate(s = exp(-sens * d))\n\nconPlot &lt;- colDF %&gt;%\n    ggplot(aes(x = c, y = l)) +\n    geom_contour(aes(z = s, color = factor(item)), data = simDF, alpha = 2/3, breaks = seq(0, 1, by = 0.2)) +\n    geom_tile(aes(fill = col), width = 2.5, height = 2.5) +\n    scale_fill_identity(aesthetics = \"fill\") +\n    scale_color_manual(values = item_cols, aesthetics = \"color\", guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Contours of equal similarity\", caption = bquote(list(plain(hue) == .(round(hue)), c == .(signif(sens, 3)))))\n\nsumSimPlot &lt;- simDF %&gt;%\n    group_by(c, l) %&gt;%\n    summarize(summed_sim = sum(s), .groups = \"keep\") %&gt;%\n    ggplot(aes(x = c, y = l, fill = summed_sim)) +\n    geom_raster() +\n    scale_fill_viridis_c(option = \"magma\", limits = c(0, 3), guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Summed similarity\", caption = bquote(list(plain(hue) == .(round(hue)), c == .(signif(sens, 3)))))\n\nconPlot + sumSimPlot + plot_layout(nrow = 1)\n\n\n\n\n\n\n\n\n\n\n9.1.2.1 Trace strength\nIt is reasonable to believe that some memory traces are stronger than others, likely due to things like primacy and recency. In GCM/EBRW, “strength” is operationalized as a scaling factor \\(m_j\\) applied to perceived similarity: \\[\ns_{qj} = m_j \\exp \\left( -c d_{qj} \\right)\n\\]\nStronger traces have their similarity multiplied by a large value (\\(m_j\\) is large if trace \\(j\\) is strong) while weaker traces have their similarity multiplied by a small value (\\(m_j\\) is small if trace \\(j\\) is weak). This is illustrated in the pair of graphs below. A probe item does not need to be as similar to a strong item in order to evoke the same level of perceived similarity.\n\n\nCode\nX &lt;- matrix(\n    c(40, 60,\n      60, 50),\n    nrow = 2,\n    ncol = 2,\n    byrow = TRUE\n)\n\nsens &lt;- 0.05\nstrength &lt;- c(1, 2)\n\ncolDF &lt;- tibble(h = hue, c = X[,1], l = X[,2]) %&gt;%\n    mutate(col = hcl(h, c, l)) %&gt;%\n    mutate(label = c(\"1\", \"2\"))\n\nitem_cols &lt;- as.vector(colDF$col)\nnames(item_cols) &lt;- as.character(1:length(item_cols))\n\nsimDF &lt;- expand_grid(c = seq(30, 70, length.out=101), l = seq(30, 70, length.out=101), item = c(1, 2)) %&gt;%\n    mutate(d = sqrt((c - X[item, 1])^2 + (l - X[item, 2])^2)) %&gt;%\n    mutate(s = exp(-sens * d) * strength[item])\n\nconPlot &lt;- colDF %&gt;%\n    ggplot(aes(x = c, y = l)) +\n    geom_contour(aes(z = s, color = factor(item)), data = simDF, alpha = 2/3, breaks = seq(0, 2, by = 0.2)) +\n    geom_tile(aes(fill = col), width = 2.5, height = 2.5) +\n    geom_text(aes(label = label), color = \"black\") +\n    scale_fill_identity(aesthetics = \"fill\") +\n    scale_color_manual(values = item_cols, aesthetics = \"color\", guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Contours of equal similarity\", caption = bquote(list(plain(hue) == .(round(hue)), c == .(signif(sens, 3)), m[1] == .(signif(strength[1], 3)), m[2] == .(signif(strength[2], 3)))))\n\nsumSimPlot &lt;- simDF %&gt;%\n    group_by(c, l) %&gt;%\n    summarize(summed_sim = sum(s), .groups = \"keep\") %&gt;%\n    ggplot(aes(x = c, y = l, fill = summed_sim)) +\n    geom_raster() +\n    scale_fill_viridis_c(option = \"magma\", limits = c(0, 3), guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Summed similarity\", caption = bquote(list(plain(hue) == .(round(hue)), c == .(signif(sens, 3)), m[1] == .(signif(strength[1], 3)), m[2] == .(signif(strength[2], 3)))))\n\nconPlot + sumSimPlot + plot_layout(nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n9.1.2.2 Trace specificity\nThe specificity parameter \\(c\\) represents the fact that items are assumed to be encoded with some degree of error/uncertainty. Just as items may be encoded with more or less strength, it is reasonable to assume that items can be encoded in memory with more or less specificity. Thus, we add a subscript to the \\(c\\) parameter corresponding to each study item: \\[\ns_{qj} = m_j \\exp \\left( -c_j d_{qj} \\right)\n\\]\nIf an item is encoded with high specificity, then it will only be perceived as similar to the probe if the probe is very close in psychological space. This is illustrated in the pair of graphs below, where item 2 is not only stronger (\\(m_2 = 2\\) vs. \\(m_1 = 1\\)) but also more precise (\\(c_2 = 0.1\\) vs. \\(c_1 = 0.05\\)).\n\n\nCode\nX &lt;- matrix(\n    c(40, 60,\n      60, 50),\n    nrow = 2,\n    ncol = 2,\n    byrow = TRUE\n)\n\nspecificity &lt;- c(0.05, 0.1)\nstrength &lt;- c(1, 2)\n\ncolDF &lt;- tibble(h = hue, c = X[,1], l = X[,2]) %&gt;%\n    mutate(col = hcl(h, c, l)) %&gt;%\n    mutate(label = c(\"1\", \"2\"))\n\nitem_cols &lt;- as.vector(colDF$col)\nnames(item_cols) &lt;- as.character(1:length(item_cols))\n\nsimDF &lt;- expand_grid(c = seq(30, 70, length.out=101), l = seq(30, 70, length.out=101), item = c(1, 2)) %&gt;%\n    mutate(d = sqrt((c - X[item, 1])^2 + (l - X[item, 2])^2)) %&gt;%\n    mutate(s = exp(-specificity[item] * d) * strength[item])\n\nconPlot &lt;- colDF %&gt;%\n    ggplot(aes(x = c, y = l)) +\n    geom_contour(aes(z = s, color = factor(item)), data = simDF, alpha = 2/3, breaks = seq(0, 2, by = 0.2)) +\n    geom_tile(aes(fill = col), width = 2.5, height = 2.5) +\n    geom_text(aes(label = label), color = \"black\") +\n    scale_fill_identity(aesthetics = \"fill\") +\n    scale_color_manual(values = item_cols, aesthetics = \"color\", guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Contours of equal similarity\", caption = bquote(list(plain(hue) == .(round(hue)), m[1] == .(signif(strength[1], 3)), m[2] == .(signif(strength[2], 3)), c[1] == .(signif(specificity[1], 3)), c[2] == .(signif(specificity[2], 3)))))\n\nsumSimPlot &lt;- simDF %&gt;%\n    group_by(c, l) %&gt;%\n    summarize(summed_sim = sum(s), .groups = \"keep\") %&gt;%\n    ggplot(aes(x = c, y = l, fill = summed_sim)) +\n    geom_raster() +\n    scale_fill_viridis_c(option = \"magma\", limits = c(0, 3), guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Summed similarity\", caption = bquote(list(plain(hue) == .(round(hue)), m[1] == .(signif(strength[1], 3)), m[2] == .(signif(strength[2], 3)), c[1] == .(signif(specificity[1], 3)), c[2] == .(signif(specificity[2], 3)))))\n\nconPlot + sumSimPlot + plot_layout(nrow = 1)",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#making-a-recognition-decision-from-random-walk-to-diffusion",
    "href": "ebrw.html#making-a-recognition-decision-from-random-walk-to-diffusion",
    "title": "9  Exemplar of the Exemplar Based Random Walk",
    "section": "9.2 Making a recognition decision: From random walk to diffusion",
    "text": "9.2 Making a recognition decision: From random walk to diffusion\nAccording to the EBRW, each trace \\(j\\) in memory races to be retrieved at a rate proportional to its perceived similarity to the probe item, \\(s_{qj}\\). Traces race not just against one another, but against a criterion. If a memory trace wins the race, this is taken as evidence that the probe item matches something in memory, thus favoring a “yes” recognition response. If the criterion wins the race instead, this is taken as evidence that the probe item does not match anything in memory, thus favoring a “no” recognition response.\nThe idea is that, if the probe item matches something in memory, then the corresponding memory trace (or one sufficiently similar to probably match) should be able to win against the criterion. If nothing wins against the criterion, then this suggests there are no traces in memory that are a good match to the probe.\n\n9.2.1 Accumulating memory evidence\nThe outcome of each race is added to a running tally which starts at zero. The value of the tally at any given time \\(t\\), which we can denote \\(x(t)\\), constitutes the current state of evidence from memory for making a recognition decision. Assume that each race takes \\(\\Delta t\\) seconds to complete. Each time a memory trace wins the race, the tally gets incremented by one; each time the criterion wins the race, the tally gets decremented by one. Put formally, we can write this process as \\[\nx\\left( t + \\Delta t \\right) =\n\\begin{cases}\nx\\left( t \\right) + 1 & \\text{if trace wins} \\\\\nx\\left( t \\right) - 1 & \\text{if criterion wins}\n\\end{cases}\n\\] where \\(x(0) = 0\\).\n\n\n9.2.2 Step probabilities\nAlthough we have specified that whether memory evidence goes up or down depends on whether a trace or the criterion wins the race, we have not yet specified how the outcome of that race is determined. The winner of each race is random but depends on the similarity \\(s_{qj}\\) between each trace \\(j\\) and the probe \\(q\\). Specifically, trace \\(j\\) wins the race with probability: \\[\n\\frac{s_{qj}}{\\sum_{k = 1}^N s_{qk} + \\kappa}\n\\] where \\(\\kappa\\) is a nonnegative number that represents how stringent the criterion is. In other words, the equation above says that the probability that trace \\(j\\) wins the race is the similarity between trace \\(j\\) and the probe \\(q\\) divided by the summed similarity across all traces plus the criterion \\(\\kappa\\). In a sense, we can think of the criterion is like a “virtual memory trace” that races alongside the \\(N\\) actual memory traces.\nRemember that we increment memory strength whenever any trace wins the race, regardless of which one it is. Because only one trace can win each race, the probability that any trace wins is just the sum of the probabilities of winning across all \\(N\\) traces, i.e.: \\[\np = \\sum_{j = 1}^N \\frac{s_{qj}}{\\sum_{k = 1}^N s_{qk} + \\kappa} = \\frac{1}{\\sum_{k = 1}^N s_{qk} + \\kappa} \\left( \\sum_{j = 1}^Ns_{qj} \\right) = \\frac{S}{S + \\kappa}\n\\] where \\(S = \\sum_{j = 1}^N s_{qj}\\) is the summed similarity across all \\(N\\) traces. The quantity \\(p\\) is the probability that the random walk takes a step up.\nThe EBRW models the speed of decision making in terms of how many races must be run until the accumulated win advantage in favor of either a “yes” or “no” response reaches a decision boundary. To convert this to “real time”, we must say how long each race takes and allow for a residual time. Above, we used \\(\\Delta t\\) to stand for the amount of time (in seconds) each race takes to run. It will be convenient later to think instead of \\(\\nu = \\frac{1}{\\Delta t}\\), where \\(\\nu\\) is the number of races per second.\nThe figure below shows an example of how memory evidence evolves during a single trial in which \\(p = 0.55\\), \\(\\nu = 40\\), \\(B_{Upper} = 7\\), \\(B_{Lower} = -8\\), and \\(t_0 = 0.2\\). In addition, the graphs above and below the evidence trajectory illustrate the relative frequency with which, across many identical trials, each type of response would be made at each unit of time. Note that these distributions are discrete because the random walk operates in discrete time intervals, each of duration \\(\\Delta t\\) (which in this example is \\(\\Delta t = \\frac{1}{\\nu} = 0.025\\) seconds).\n\n\nCode\nset.seed(1)\n\nnu &lt;- 40\np &lt;- 0.55\n\nB &lt;- c(-8, 7)\nresid &lt;- 0.2\n\n### RT distributions\n\nY_rw &lt;- seq(B[1], B[2])\nP_rw &lt;- matrix(0, nrow = length(Y_rw), ncol = length(Y_rw))\nP_rw[cbind(2:(nrow(P_rw) - 1), 1:(ncol(P_rw) - 2))] &lt;- 1 - p\nP_rw[cbind(2:(nrow(P_rw) - 1), 3:ncol(P_rw))] &lt;- p\nP_rw[1, 1] &lt;- P_rw[nrow(P_rw), ncol(P_rw)] &lt;- 1\n\n### Simulation\n\nwhile (TRUE) {\n    winner &lt;- 0\n    x_rw &lt;- 0\n\n    while (TRUE) {\n        s &lt;- 2 * (runif(n = 1) &lt; p) - 1\n        x_rw &lt;- c(x_rw, x_rw[length(x_rw)] + s)\n        if (x_rw[length(x_rw)] &lt;= B[1]) {\n            winner &lt;- 1\n            break\n        } else if (x_rw[length(x_rw)] &gt;= B[2]) {\n            winner &lt;- 2\n            break\n        }\n    }\n    \n    if (winner == 2 & (length(x_rw) / nu) &gt; 1.5) {\n        break\n    }\n}\n\nRT_rw &lt;- matrix(0, nrow = 2, ncol = length(x_rw))\nZ_rw &lt;- 1 * c(Y_rw == 0)\n\nfor (i in 1:length(x_rw)) {\n    Z_rw &lt;- Z_rw %*% P_rw\n    RT_rw[1, i] &lt;- Z_rw[1]\n    RT_rw[2, i] &lt;- Z_rw[length(Z_rw)]\n}\n\ndRT_rw &lt;- apply(RT_rw, MARGIN = 1, FUN = diff) * nu / 2\n\nrtPlot1 &lt;- tibble(t = resid + 1:length(x_rw) / nu, p = c(0, dRT_rw[,2])) %&gt;%\n    ggplot(aes(x = t, y = p)) +\n    geom_col(fill = \"#eeb211\", color = NA, width = 1 / nu) +\n    coord_cartesian(xlim = c(0, NA), ylim = c(0, max(c(dRT_rw)))) +\n    labs(x = NULL, y = \"Pr(Respond at time t)\") +\n    theme(axis.text = element_blank(), axis.ticks.y = element_blank())\n\nrtPlot0 &lt;- tibble(t = resid + 1:length(x_rw) / nu, p = c(0, dRT_rw[,1])) %&gt;%\n    ggplot(aes(x = t, y = p)) +\n    geom_col(fill = \"#46166b\", color = NA, width = 1 / nu) +\n    labs(x = NULL, y = \"Pr(Respond at time t)\") +\n    scale_x_continuous(limits = c(0, NA)) +\n    scale_y_reverse(limits = c(max(c(dRT_rw)), 0)) +\n    theme(axis.text = element_blank(), axis.ticks.y = element_blank())\n\nrwPlot &lt;- tibble(t = resid + 1:length(x_rw) / nu, x = x_rw) %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step(linewidth = 1.5) +\n    geom_hline(yintercept = B[2], linetype = \"solid\", color = \"#eeb211\", linewidth = 2) +\n    geom_hline(yintercept = B[1], linetype = \"solid\", color = \"#46166b\", linewidth = 2) +\n    geom_vline(xintercept = resid, linetype = \"dashed\", color = \"#666666\", linewidth = 2) +\n    geom_text(data = tibble(x = 0, y = B[2], label = paste0(\"B[Upper] == \", B[2])), mapping = aes(x = x, y = y, label = label), color = \"#eeb211\", inherit.aes = FALSE, parse = TRUE, hjust = 0, vjust = 2) +\n    geom_text(data = tibble(x = 0, y = B[1], label = paste0(\"B[Lower] == \", B[1])), mapping = aes(x = x, y = y, label = label), color = \"#46166b\", inherit.aes = FALSE, parse = TRUE, hjust = 0, vjust = -1) +\n    geom_text(data = tibble(x = resid, y = 0, label = paste0(\"t[0] == \", resid)), mapping = aes(x = x, y = y, label = label), color = \"#666666\", inherit.aes = FALSE, parse = TRUE, hjust = 1.1, vjust = 1.5) +\n    coord_cartesian(xlim = c(0, NA)) +\n    labs(x = \"Retrieval time (s)\", y = \"Memory evidence\", caption = bquote(list(p == .(p), nu == .(nu))))\n\nrtPlot1 + rwPlot + rtPlot0 + plot_layout(ncol = 1, heights = c(1, 1.75, 1))\n\n\n\n\n\n\n\n\n\n\n\n9.2.3 From discrete random walk to continuous diffusion\nA random walk takes discrete-valued steps either up or down in discrete units of time. A Wiener diffusion process takes continuous-valued steps sampled from a normal distribution in infinitely small units of time, thus effectively operating in continuous time. We are going to approximate the discrete EBRW with a continuous diffusion process (so technically we should call this model the EBD for Exemplar-Based Diffusion, but we will keep calling it the EBRW for posterity).\nIn going from a random walk to a diffusion model, we are making an important psychological claim: We are saying that, instead of memory evidence arriving in discrete units at regular intervals, memory evidence is a continuous value that continually evolves as new information arrives. We can think of this as saying that, instead of only knowing the outcome of each race, you can see who is ahead and who is behind at any given time; this is the move from discrete time to continuous time. Moreover, instead of only scoring each race as a win or loss for the memory traces, the races are assigned a continuous value depending on how clear the winner is; this is the move from discrete evidence to continuous evidence.\n\n9.2.3.1 Mean and standard deviation of diffusion\nWe can write the update equation for the random walk like we did above: \\[\n\\begin{align}\nx \\left( t + \\Delta t \\right) & = \\begin{cases} x(t) + 1 & \\text{with probability } p \\\\ x(t) - 1 & \\text{with probability } 1 - p \\end{cases} \\\\\nx \\left( t + \\Delta t \\right) - x(t) & = \\begin{cases} 1 & \\text{with probability } p \\\\ -1 & \\text{with probability } 1 - p \\end{cases} \\\\\nx \\left( t + \\Delta t \\right) - x(t) & \\sim 2 \\times \\text{Bernoulli} \\left( p \\right) - 1\n\\end{align}\n\\] where we have rearranged terms and used the shorthand in the final line to emphasize the idea that each step of the random walk can be thought of as a sample from a Bernoulli distribution with parameter \\(p\\) that is then transformed from \\(\\lbrace 0, 1 \\rbrace\\) to \\(\\lbrace -1, 1 \\rbrace\\).\nTo turn this into a continuous diffusion process, we need to swap out the transformed Bernoulli distribution with a normal distribution that has the same mean and variance. The mean is \\(2 p - 1\\) and the variance is \\(4 p \\left(1 - p \\right)\\). One more thing: remember that we run \\(\\nu\\) races per second, so we need to multiply the mean and variance by \\(\\nu\\). Therefore, the mean drift rate is \\(v = \\nu \\left(2 p - 1 \\right)\\) and the variance is \\(\\sigma^2 = 4 \\nu p (1 - p)\\).\nNote that this is different from the typical diffusion model where the variance of the evidence samples is arbitrarily fixed to 1. Notice an important property of this variance: It is largest when \\(p = 0.5\\) and approaches zero as \\(p\\) approaches either 0 or 1. In other words, the more uncertain the outcome of each race, the more noise there is in the diffusion. This is illustrated below:\n\n\nCode\nexpand_grid(p = seq(0, 1, length.out = 101), nu = seq(10, 40, by = 10)) %&gt;%\n    mutate(sigma2 = 4 * nu * p * (1 - p)) %&gt;%\n    ggplot(aes(x = p, y = sigma2, color = nu, group = nu)) +\n    geom_line() +\n    labs(x = p, y = expression(sigma^2), color = expression(nu)) +\n    theme(legend.position = c(1, 1), legend.justification = c(1, 1))\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nTo summarize, the difference between the random walk and the diffusion is that we have swapped out a discrete binomial distribution of evidence increments per unit time with a continuous normal distribution of evidence increments per unit time. Everything else is the same: You still respond “yes” if and when the accumulated evidence \\(x(t)\\) reaches either the upper boundary or the lower boundary.\n\n\n9.2.3.2 Closeness of predictions\nTo illustrate how well the diffusion process approximates the random walk, the graphs below show the diffusion approximation to the same random walk example used above. The smooth lines in the upper and lower graphs are the probability of responding per unit time (i.e., the probability density function) according to the Wiener diffusion model. The open bars are the same probabilities from the random walk. The diffusion model’s predictions hew very closely to those of the random walk!\n\n\nCode\nmu &lt;- nu * (2 * p - 1)\nsigma2 &lt;- 4 * nu * p * (1 - p)\nboundsep &lt;- B[2] - B[1]\nbias &lt;- (0 - B[1]) / (B[2] - B[1])\ndelta_t &lt;- 0.001\n\nwhile (TRUE) {\n    winner_diff &lt;- NA\n    x_diff &lt;- 0\n    \n    while (TRUE) {\n        x_diff &lt;- c(x_diff, x_diff[length(x_diff)] + rnorm(n = 1, mean = mu * delta_t, sd = sqrt(sigma2 * delta_t)))\n        if (x_diff[length(x_diff)] &lt;= B[1]) {\n            winner_diff &lt;- 1\n            break\n        } else if (x_diff[length(x_diff)] &gt;= B[2]) {\n            winner_diff &lt;- 2\n            break\n        }\n    }\n    \n    if (winner == winner_diff & abs((length(x_diff) * delta_t) - (length(x_rw) / nu)) &lt; (1 / nu)) {\n        break\n    }\n}\n\nx_diff &lt;- pmax(pmin(x_diff, B[2]), B[1])\n\nt &lt;- seq(1, length(x_diff)) * delta_t\n\ndRT_diff &lt;- cbind(\n    WienerPDF(t = t, response = \"lower\", a = boundsep / sqrt(sigma2), v = mu / sqrt(sigma2), w = bias)$value,\n    WienerPDF(t = t, response = \"upper\", a = boundsep / sqrt(sigma2), v = mu / sqrt(sigma2), w = bias)$value\n)\n\nrtPlot1 &lt;- tibble(t = resid + t, p = dRT_diff[,2]) %&gt;%\n    ggplot(aes(x = t, y = p)) +\n    geom_col(data = tibble(t = resid + 1:length(x_rw) / nu, p = c(0, dRT_rw[,2])), color = \"#eeb211aa\", fill = NA, width = 1 / nu) +\n    geom_area(fill = \"#eeb21177\", color = \"#eeb211\") +\n    coord_cartesian(xlim = c(0, NA), ylim = c(0, max(c(dRT_diff)))) +\n    labs(x = NULL, y = \"Pr(Respond at time t)\") +\n    theme(axis.text = element_blank(), axis.ticks.y = element_blank())\n\nrtPlot0 &lt;- tibble(t = resid + t, p = dRT_diff[,1]) %&gt;%\n    ggplot(aes(x = t, y = p)) +\n    geom_col(data = tibble(t = resid + 1:length(x_rw) / nu, p = c(0, dRT_rw[,1])), color = \"#46166baa\", fill = NA, width = 1 / nu) +\n    geom_area(fill = \"#46166b77\", color = \"#46166b\") +\n    labs(x = NULL, y = \"Pr(Respond at time t)\") +\n    scale_x_continuous(limits = c(0, NA)) +\n    scale_y_reverse(limits = c(max(c(dRT_diff)), 0)) +\n    theme(axis.text = element_blank(), axis.ticks.y = element_blank())\n\nrwPlot &lt;- tibble(t = resid + t, x = x_diff) %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_line() +\n    geom_hline(yintercept = B[2], linetype = \"solid\", color = \"#eeb211\", linewidth = 2) +\n    geom_hline(yintercept = B[1], linetype = \"solid\", color = \"#46166b\", linewidth = 2) +\n    geom_vline(xintercept = resid, linetype = \"dashed\", color = \"#666666\", linewidth = 2) +\n    geom_text(data = tibble(x = 0, y = B[2], label = paste0(\"B[Upper] == \", B[2])), mapping = aes(x = x, y = y, label = label), color = \"#eeb211\", inherit.aes = FALSE, parse = TRUE, hjust = 0, vjust = 2) +\n    geom_text(data = tibble(x = 0, y = B[1], label = paste0(\"B[Lower] == \", B[1])), mapping = aes(x = x, y = y, label = label), color = \"#46166b\", inherit.aes = FALSE, parse = TRUE, hjust = 0, vjust = -1) +\n    geom_text(data = tibble(x = resid, y = 0, label = paste0(\"t[0] == \", resid)), mapping = aes(x = x, y = y, label = label), color = \"#666666\", inherit.aes = FALSE, parse = TRUE, hjust = 1.1, vjust = 1.5) +\n    coord_cartesian(xlim = c(0, NA)) +\n    labs(x = \"Retrieval time (s)\", y = \"Memory evidence\", caption = bquote(list(p == .(p), nu == .(nu), v == .(signif(mu, 3)), sigma == .(signif(sqrt(sigma2), 3)))))\n\nrtPlot1 + rwPlot + rtPlot0 + plot_layout(ncol = 1, heights = c(1, 1.75, 1))",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#implementation-in-r",
    "href": "ebrw.html#implementation-in-r",
    "title": "9  Exemplar of the Exemplar Based Random Walk",
    "section": "9.3 Implementation in R",
    "text": "9.3 Implementation in R\nTo find the EBRW parameters that best fit the recognition data from a participant, let’s implement the diffusion version of the EBRW in R using the WienR package. We must define a function to compute the negative log-likelihood (NLL) for a set of observed responses/RT’s, given a set of parameters. The function itself, in outline form, looks like the one below. I have written comments for the things that the function needs to accomplish to get from what is given to the function (in the parentheses following function) to what the function needs to return at the end.\nFor example purposes, this implementation doesn’t include all of the bells and whistles that the full model includes. It will not allow for varying trace strength nor will it include attention weights on each dimension. This is meant to illustrate the basic idea that we can define a diffusion model in which the drift rates are derived from a theory, rather than just estimated.\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responded \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    # 2. Calculate the log-likelihood of each observed response/RT on each trial\n    \n    # 3. Return final result\n    return(nll)\n}\n\n\nLet’s now fill in each of those sections in turn.\n\n9.3.1 Parameters\nThe vector par that is the first argument to the ebrw_nll function should be a named vector that has the following entries:\n\n\nCode\npar &lt;- c(\n    \"retrieval_rate\" = 3,     # This is the \"nu\" parameter\n    \"a\" = 2,                  # Response caution\n    \"w\" = 0.5,                # Response bias\n    \"t0\" = 0,                 # Residual time\n    \"specificity\" = 1,        # Specificity of memory representations (the \"c\" parameter)\n    \"criterion\" = 1           # Criterion (the \"kappa\" parameter)\n)\n\n\n\n\n9.3.2 Computing the mean and SD of the drift for each trial\nRecall that the drift rates depend on the distances between each of the stimulus items. Since the function is provided with stim_coords, we can make our job a little easier by using the dist function to compute the matrix of distances between all pairs of items. This saves us from “recomputing” the distances between pairs of items that occur on multiple trials:\n\n\nCode\nstim_dists &lt;- as.matrix(dist(stim_coords))\n\n\nThen, to compute the summed similarity for each trial i, we can use a for loop:\n\n\nCode\nevidence_mean &lt;- rep(0, length(probe_item))\nevidence_sd &lt;- rep(0, length(probe_item))\n\nfor (i in 1:length(probe_item)) {\n    summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n    p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n    \n    evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n    evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n}\n\n\nWe have now completed the second step of writing the ebrw_nll function, as summarized below.\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    stim_dists &lt;- as.matrix(dist(stim_coords))\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    # 2. Calculate the log-likelihood of each observed response/RT on each trial\n    \n    # 3. Return final result\n    return(nll)\n}\n\n\n\n\n9.3.3 Calculating the log-likelihood\nThis step is almost too easy. We are using the WienR package, which means we can use the WienerPDF function like we’ve seen already. There is only one thing we need to do: The WienerPDF function assumes that the standard deviation of the diffusion is always equal to one. As such, we need to standardize the drift rate and boundary separation before we send them to the WienerPDF function by dividing each by evidence_sd:\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    stim_dists &lt;- as.matrix(dist(stim_coords))\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    # 2. Calculate the log-likelihood of each observed response/RT on each trial\n    result &lt;- WienerPDF(\n        t = rt,\n        response = response,\n        a = par[\"a\"] / evidence_sd,\n        w = par[\"w\"],\n        v = evidence_mean / evidence_sd,\n        t0 = par[\"t0\"]\n    )\n    \n    # 3. Return final result\n    return(-sum(result$logvalue))\n}\n\n\n\n\n9.3.4 Error-checking\nIt is important for us to do some error checking. Sometimes, a particular combination of parameters will make it impossible for the WienerPDF function to calculate the log-likelihood. When that happens, it gives an error. In essence, such a result tells us that the model cannot work with that combination of parameters. Thus, rather than an “error”, that is really telling us that we should assign zero likelihood to that set of parameters, which is equivalent to a log-likelihood of \\(-\\infty\\).\nWe can do that kind of check in R by putting the WienerPDF function call within try(). If the WienerPDF function gives an error, then the result that gets stored in trial_wiener is also an error. Otherwise, it just gives us the log-likelihoods that we want.\nLet’s set up an “if…else” structure to do this check:\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    stim_dists &lt;- as.matrix(dist(stim_coords))\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    # 2. Calculate the log-likelihood of each observed response/RT on each trial\n    result &lt;- try(WienerPDF(\n        t = rt,\n        response = response,\n        a = par[\"a\"] / evidence_sd,\n        w = par[\"w\"],\n        v = evidence_mean / evidence_sd,\n        t0 = par[\"t0\"]\n    ))\n    \n    # 3. Return final result\n    if (class(result) == \"try-error\") {\n        return(Inf)\n    } else {\n        return(-sum(result$logvalue))\n    }\n}",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#a-worked-example",
    "href": "ebrw.html#a-worked-example",
    "title": "9  Exemplar of the Exemplar Based Random Walk",
    "section": "9.4 A worked example",
    "text": "9.4 A worked example\nThis example uses a single participant’s data from Experiment 2 of Gillespie & Cox (2024). In this experiment, each participant made similarity ratings between all pairs of eight items. Each item was an “auditory texture” constructed via Fourier synthesis. We applied Multidimensional Scaling to the similarity ratings from each participant to assign, for each participant, a set of coordinates to each item. The coordinates are such that items that are farther from one another were associated with lower similarity ratings and those that were closer to one another were assigned higher similarity ratings. Be sure to check out the paper itself for additional detail on how this was done, and how we decided that the multidimensional space in which the stimuli are represented had 3 dimensions. These coordinates will be used for the stim_coords argument of the ebrw_nll function.\nIn addition to providing similarity ratings, each participant engaged in a recognition memory task. On each trial of this task, the participant heard two auditory textures, presented sequentially. They then heard a “probe” sound and had to decide whether or not it was one of the two sounds that had just been presented. It is these recognition data that we will model with the EBRW.\nYou can grab the data yourself by running the following chunk of code to download it and load it into your R workspace:\n\n\nCode\ndownload.file(\"https://github.com/gregcox7/choice_rt_models/raw/refs/heads/main/data/ebrw_example_data.rdata\", \"blast_data.rdata\")\nload(\"ebrw_example_data.rdata\")\n\n\n\n9.4.1 Check out the data\nThe stimulus coordinates are saved in a matrix called stim_coords:\n\n\nCode\nprint(stim_coords)\n\n\n           [,1]         [,2]        [,3]\n[1,] -0.4679162 -0.463219039 -0.08056499\n[2,] -0.5549266  0.499390636  0.07099460\n[3,] -0.4742833 -0.132020672  0.43312165\n[4,] -0.5062581  0.005578674 -0.40175331\n[5,]  0.5110397 -0.222844222 -0.30381582\n[6,]  0.4578305  0.336495895 -0.27302187\n[7,]  0.5669455 -0.288366241  0.24624698\n[8,]  0.4675684  0.264984969  0.30879277\n\n\nWe can visualize them using plot_ly:\n\n\nCode\nto_plot &lt;- as.data.frame(stim_coords)\ncolnames(to_plot) &lt;- paste(\"Dimension\", 1:ncol(stim_coords))\nrownames(to_plot) &lt;- paste(\"Item\", 1:nrow(stim_coords))\n\nplot_ly(\n    data = to_plot,\n    x = ~ `Dimension 1`,\n    y = ~ `Dimension 2`,\n    z = ~ `Dimension 3`,\n    type = \"scatter3d\",\n    text = rownames(to_plot),\n    mode = \"markers+text\"\n)\n\n\n\n\n\n\nIn addition for each trial of the recognition task, the study_items matrix tells us which of the two items had been presented as part of the set to be remembered and the probe_item vector tells us what the probe item was. These numbers refer to row in the stim_coords matrix.\n\n\nCode\nprint(study_items)\n\n\n      [,1] [,2]\n [1,]    3    5\n [2,]    3    6\n [3,]    8    1\n [4,]    1    7\n [5,]    2    4\n [6,]    7    2\n [7,]    6    7\n [8,]    8    3\n [9,]    3    4\n[10,]    5    6\n[11,]    6    8\n[12,]    1    4\n[13,]    4    8\n[14,]    7    4\n[15,]    6    1\n[16,]    1    3\n[17,]    1    2\n[18,]    3    2\n[19,]    6    4\n[20,]    8    2\n[21,]    3    7\n[22,]    4    5\n[23,]    5    2\n[24,]    1    5\n[25,]    7    8\n[26,]    2    6\n[27,]    7    5\n[28,]    5    8\n\n\nCode\nprint(probe_item)\n\n\n [1] 4 2 1 7 2 7 7 2 6 6 6 1 3 4 4 3 8 3 8 4 7 8 4 2 6 2 3 5\n\n\nFinally, the rt and response vectors record the response time (in seconds) and the response (where 2 is “yes” and 1 is “no”) produced by this participant on each trial.\n\n\n9.4.2 Finding optimal parameters\nThe version of the EBRW that we applied in our paper is a bit more complex than the one we will use here, which only has six free parameters. We will use R’s built-in nlminb function to find the best-fitting values of these parameters. To do this, we need to specify initial values for each parameter in a named vector, as shown below. These initial values don’t necessarily need to be anything in particular as long as they don’t cause the ebrw_nll function to return an error or a value of Inf.\n\n\nCode\ninit_par &lt;- c(\n    \"retrieval_rate\" = 3,     # This is the \"nu\" parameter\n    \"a\" = 2,                  # Response caution\n    \"w\" = 0.5,                # Response bias\n    \"t0\" = 0,                 # Residual time\n    \"specificity\" = 1,        # Specificity of memory representations (the \"c\" parameter)\n    \"criterion\" = 1           # Criterion (the \"kappa\" parameter)\n)\n\n\nWe also need to specify the upper and lower values that each of these parameters could possibly take, as shown below:\n\n\nCode\nlower &lt;- c(\n    \"retrieval_rate\" = 0,     # This is the \"nu\" parameter\n    \"a\" = 0,                  # Response caution\n    \"w\" = 0,                  # Response bias\n    \"t0\" = 0,                 # Residual time\n    \"specificity\" = 0,        # Specificity of memory representations (the \"c\" parameter)\n    \"criterion\" = 0           # Criterion (the \"kappa\" parameter)\n)\n\nupper &lt;- c(\n    \"retrieval_rate\" = Inf,     # This is the \"nu\" parameter\n    \"a\" = Inf,                  # Response caution\n    \"w\" = 1,                    # Response bias\n    \"t0\" = min(rt),             # Residual time\n    \"specificity\" = Inf,        # Specificity of memory representations (the \"c\" parameter)\n    \"criterion\" = Inf           # Criterion (the \"kappa\" parameter)\n)\n\n\nNote that these upper and lower values can be Infinite if necessary!\nFinally, let’s use the nlminb function, which we need to provide with each of the ingredients we prepared above. We will save the result as fit:\n\n\nCode\nfit &lt;- nlminb(\n    start = init_par,          # Need to provide initial guess of parameter values\n    objective = ebrw_nll,      # Tell R the name of the function to optimize\n    lower = lower,             # The lower bounds on each parameter\n    upper = upper,             # The upper bounds on each parameter\n    stim_coords = stim_coords, # The coordinates of each stimulus\n    rt = rt,                   # The vector of RT's on each trial\n    response = response,       # The vector of responses on each trial\n    study_items = study_items, # The study items on each trial\n    probe_item = probe_item    # The probe item on each trial\n)\n\n\nWarning in nlminb(start = init_par, objective = ebrw_nll, lower = lower, :\nNA/NaN function evaluation\n\n\nAnd the final result!\n\n\nCode\nfit\n\n\n$par\nretrieval_rate              a              w             t0    specificity \n     1.2474191      2.1459586      0.4488126      0.6884095      5.6470633 \n     criterion \n     0.1035233 \n\n$objective\n[1] 21.59716\n\n$convergence\n[1] 0\n\n$iterations\n[1] 35\n\n$evaluations\nfunction gradient \n      56      243 \n\n$message\n[1] \"relative convergence (4)\"\n\n\nIt may seem like a lot of work for such a simple result—six numbers! But it helps to take a step back and realize what we have just done. We had a participant provide similarity ratings for pairs of unusual auditory stimuli that they had never heard before. We found a way to represent how that participant perceives those stimuli in terms of coordinates in a multidimensional space. We could then infer, from their choices and response times in a recognition task, how well this participant could remember these stimuli and how they used perceived similarity to make their choices. We have explained why this participant did what they did not just in terms of accumulating evidence for a decision, but in terms of how they got that evidence from their memory for novel auditory stimuli.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#predictions-by-simulation-and-math",
    "href": "ebrw.html#predictions-by-simulation-and-math",
    "title": "9  Exemplar of the Exemplar Based Random Walk",
    "section": "9.5 Predictions by simulation and math",
    "text": "9.5 Predictions by simulation and math\nAs impressive as it is to have “reverse-engineered” the contents of a participant’s memory from their behavior, there are still some pieces we have left out of the puzzle. Specifically, although we developed a function to calculate the NLL for the purpose of estimating parameters, we haven’t yet built a function to simulate data using the EBRW. That is the purpose of this section. We will also see some explicit mathematical formulae for predicting average performance without the need to run many simulations. In the end, this will enable us to (a) verify the quality of our model fit by comparing its predictions to observations; and (b) make predictions for novel experimental conditions that we may not have observed.\nFinally, this section demonstrates a general technique that is often useful in programming a model: Having a single function that can return either the NLL or predictions. Since the model parameters have the same meaning regardless of whether we are calculating likelihoods or making predictions, we can just tell the function what we want it to do with those parameters. The value of this approach is that we won’t accidentally write simulation code that differs in some important way from our likelihood code. For example, if we change the parameters for our likelihood code, we want to make sure that our simulation code respects that change; the approach we take in this section forces us to do this and thereby keep our model consistent.\n\n9.5.1 Simulating individual trials\nFirst, we will see how we can modify our ebrw_nll function to return simulated behavior instead of just the negative log-likelihood of observed choices and RT’s. The chunk of code below gives an outline of our approach, which involves adding a new argument to the function called n_sims. By default, n_sims = 0, which indicates that the function should not do any simulation but should instead return the NLL, as it did above. As you might anticipate, we will need an if...else structure to check whether n_sims &gt; 0 or not.\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\n# n_sims: this indicates whether or not simulations are meant to be returned instead of the NLL\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt, n_sims = 0) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    stim_dists &lt;- as.matrix(dist(stim_coords))\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    if (n_sims &gt; 0) {\n        # 2a. Simulate a response for each trial\n        \n        # SIMULATION CODE GOES HERE\n        \n        # 3a. Return final result\n        return(result)\n    } else {\n        # 2b. Calculate the log-likelihood of each observed response/RT on each trial\n        result &lt;- try(WienerPDF(\n            t = rt,\n            response = response,\n            a = par[\"a\"] / evidence_sd,\n            w = par[\"w\"],\n            v = evidence_mean / evidence_sd,\n            t0 = par[\"t0\"]\n        ))\n        \n        # 3b. Return final result\n        if (class(result) == \"try-error\") {\n            return(Inf)\n        } else {\n            return(-sum(result$logvalue))\n        }\n    }\n}\n\n\nIt is worth pointing out that, aside from the if...else structure, nothing else has changed about the function: Other than n_sims it takes exactly the same arguments, meaning it will still use the provided parameters to compute the drift rates for each trial of an experiment based on the similarity between the studied items and the probe.\nYou may wonder at this point why n_sims is a number—why didn’t we use a boolean like do_simulation which could be either TRUE or FALSE? This is because we may want to simulate multiple replications of a given trial. For example, we may want to simulate an experiment in which the same study/test items are repeated. Alternatively, we may want to use a large number of simulations so that we can estimate the entire distribution of possible responses and response times that a participant might produce on a given trial. In any case, by allowing n_sims to be a number rather than just TRUE or FALSE, we are making our function more flexible—something for which our future selves will thank our current selves.\nBack to the business at hand, we need to fill in the blank where it says SIMULATION CODE HERE. Recall that we are using the WienR package, which includes a sampWiener function for simulating choices and RT’s. We used this to build our diffusion simulation code and we can use it again here. The only challenge is that, unlike WienerPDF, sampWiener can only take a single value for the diffusion model parameters (e.g., a, v, w, etc.) at a time. So we will use a for loop to simulate behavior on each individual trial, as shown in the code below (as usual, there are alternative approaches that are more efficient than a for loop, but our aim at the moment is for transparency):\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\n# n_sims: this indicates whether or not simulations are meant to be returned instead of the NLL\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt, n_sims = 0) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    stim_dists &lt;- as.matrix(dist(stim_coords))\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    if (n_sims &gt; 0) {\n        # 2a. Simulate a response for each trial\n        result &lt;- c()\n        \n        # For each trial...\n        for (i in 1:length(probe_item)) {\n            # Simulate `n_sims` replications of this trial, using its corresponding parameters\n            trial_sim &lt;- sampWiener(\n                N = n_sims,\n                a = par[\"a\"] / evidence_sd[i],\n                w = par[\"w\"],\n                v = evidence_mean[i] / evidence_sd[i],\n                t0 = par[\"t0\"]\n            )\n            \n            # Append the new set of simulations to our `result`, including an indicator of which trial we are simulating and an index for each simulation\n            result &lt;- rbind(\n                result,\n                tibble(trial = i, sim_index = 1:n_sims, rt = trial_sim$q, response = trial_sim$response)\n            )\n        }\n        \n        # 3a. Return final result\n        return(result)\n    } else {\n        # 2b. Calculate the log-likelihood of each observed response/RT on each trial\n        result &lt;- try(WienerPDF(\n            t = rt,\n            response = response,\n            a = par[\"a\"] / evidence_sd,\n            w = par[\"w\"],\n            v = evidence_mean / evidence_sd,\n            t0 = par[\"t0\"]\n        ))\n        \n        # 3b. Return final result\n        if (class(result) == \"try-error\") {\n            return(Inf)\n        } else {\n            return(-sum(result$logvalue))\n        }\n    }\n}\n\n\nThe code above uses our old trick of creating an “empty” tibble (results &lt;- c()) and then appending each new set of simulations to the results tibble before returning it. Let’s take the new code out for a spin!\nFirst, let’s see how easy it is to simulate data that comes from exactly the same trials that our participant experienced:\n\n\nCode\nsim_trials &lt;- ebrw_nll(\n    par = fit$par,\n    stim_coords = stim_coords,\n    study_items = study_items,\n    probe_item = probe_item,\n    n_sims = 1\n)\n\nprint(sim_trials)\n\n\n# A tibble: 28 × 4\n   trial sim_index    rt response\n   &lt;int&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   \n 1     1         1 1.11  lower   \n 2     2         1 2.88  lower   \n 3     3         1 1.71  upper   \n 4     4         1 2.42  upper   \n 5     5         1 2.48  upper   \n 6     6         1 1.41  upper   \n 7     7         1 0.979 upper   \n 8     8         1 0.824 lower   \n 9     9         1 1.50  lower   \n10    10         1 1.26  upper   \n# ℹ 18 more rows\n\n\nNotice that for the par argument, we used the fitted parameter values we found earlier, which are in fit$par. We also used the exact same stim_coords, study_items, and probe_item arguments as we used when fitting this participant’s data. So essentially what we just did is simulate an imaginary but plausible dataset that this participant could have produced, assuming that the EBRW is a good model of their performance.\nWhile simulating a single dataset has its value—like with the parameter recovery exercises we did with diffusion models—we might be more interested in the distribution of responses and response times that this participant might produce. This is one reason why it is handy to be able to set n_sims to a large value, as in the chunk of code below. We can then estimate, for example, the probability of making a “yes” response in a particular trial, or the mean correct response time. That said, the next section will show us how we can obtain predictions for response probabilities and mean RT’s mathematically. However, mathematical formulae won’t always be available, so we will often fall back on simulations.\n\n\nCode\n# Simulate many trials\nsim_trials &lt;- ebrw_nll(\n    par = fit$par,\n    stim_coords = stim_coords,\n    study_items = study_items,\n    probe_item = probe_item,\n    n_sims = 1000\n)\n\n# Obtain mean predictions for each trial\nsim_means &lt;- sim_trials %&gt;%\n    group_by(trial) %&gt;%\n    summarize(\n        sim_p_upper = mean(response == \"upper\"),\n        sim_mean_rt_yes = mean(rt[response == \"upper\"]),\n        sim_mean_rt_no = mean(rt[response == \"lower\"])\n    )\n\nprint(sim_means)\n\n\n# A tibble: 28 × 4\n   trial sim_p_upper sim_mean_rt_yes sim_mean_rt_no\n   &lt;int&gt;       &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n 1     1       0.001            1.37           1.63\n 2     2       0.041            1.95           1.72\n 3     3       0.991            1.86           1.38\n 4     4       0.986            1.84           1.57\n 5     5       0.989            1.87           2.00\n 6     6       0.994            1.88           1.97\n 7     7       0.993            1.81           1.44\n 8     8       0.028            1.94           1.72\n 9     9       0              NaN              1.52\n10    10       0.988            1.81           1.77\n# ℹ 18 more rows\n\n\nYou may already notice one issue with using simulation to obtain average model predictions. According to the fitted model, there is almost no chance that this participant gives a “yes” response on trial 9. Even in 1000 simulations, the model never produced a “yes” response on that trial. As a result, we would need to simulate many trials before we got a stable estimate of the mean RT for a “yes” response on the trial (though, given how unlikely it is, maybe we wouldn’t be too interested in that prediction anyway).\nOut of curiousity, let’s see what the study items and probe were for trial 9:\n\n\nCode\nstudy_items[9,]\n\n\n[1] 3 4\n\n\nCode\nprobe_item[9]\n\n\n[1] 6\n\n\nIf you refer back to the MDS plot above, we see that items 3 and 4 (the studied items on trial 9) both fall on one end of “Dimension 1” whereas item 6 (the probe item) falls on the other side. In other words, the probe is pretty far from both studied items, so it makes sense that the EBRW would predict a very low probability that this participant incorrectly says “yes” on that trial.\n\n\n9.5.2 Mathematical expressions for response probability and mean RT’s\nMany cognitive models are sufficiently complex that there aren’t mathematical expressions for their predictions. However, the Wiener diffusion model (i.e., without any extra trial-by-trial variability) actually has mathematical expressions for the mean response probability and mean response times (for each response). These formulae aren’t exactly pretty, but they only involve elementary functions that are built into R, so we can translate them into code.\n\\[\\begin{align}\n\\hat{p}_i & = \\frac{1 - \\exp \\left( -\\frac{2 a v_i w}{s_i^2} \\right)}{1 - \\exp\\left(-\\frac{2 a v_i}{s_i^2} \\right)} \\\\\n\\widehat{RT}_{Y,i} & = t_0 + \\frac{a}{v_i} \\left[ \\frac{1}{\\tanh \\left( \\frac{a v_i}{s_i^2} \\right)} - \\frac{w}{\\tanh \\left( \\frac{a v_i w}{s_i^2} \\right)} \\right] \\\\\n\\widehat{RT}_{N,i} & = t_0 + \\frac{a}{v_i} \\left[ \\frac{1}{\\tanh \\left( \\frac{a v_i}{s_i^2} \\right)} - \\frac{1 - w}{\\tanh \\left( \\frac{a v_i \\left(1 - w \\right)}{s_i^2} \\right)} \\right]\n\\end{align}\\]\nIt may help to keep in mind the following “Rosetta stone” that relates the mathematical notation to things that we recognize from our code:\n\n\\(\\hat{p}_i\\): Probability of giving a “yes” response on trial \\(i\\).\n\\(\\widehat{RT}_{Y,i}\\): Mean RT for “yes” responses on trial \\(i\\).\n\\(\\widehat{RT}_{N,i}\\): Mean RT for “no” responses on trial \\(i\\).\n\\(a\\): Boundary separation, par[\"a\"].\n\\(w\\): Response bias, par[\"w\"].\n\\(t_0\\): Residual time, par[\"t0\"].\n\\(v_i\\): Mean drift rate on trial \\(i\\), evidence_mean[i].\n\\(s_i\\): Standard deviation of drift on trial \\(i\\), evidence_sd[i].\n\nFinally, \\(\\tanh\\) is the hyperbolic tangent function, which is built into R as tanh(). This function is defined by \\[\n\\tanh \\left(x \\right) = \\frac{\\exp \\left( x \\right) - \\exp \\left(-x \\right)}{\\exp \\left( x \\right) + \\exp \\left(-x \\right)}\n\\] and it looks like this:\n\n\nCode\ntibble(x = seq(-3, 3, length.out = 201)) %&gt;%\n    mutate(y = tanh(x)) %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_line() +\n    labs(x = \"x\", y = \"tanh(x)\")\n\n\n\n\n\n\n\n\n\nArmed with these mathematical expressions, we can readily translate them into R code. We’ll first do that and then see how we can insert that code into our ebrw_nll function. In the following, we are again using a for loop over each trial, using i as the index for the trial. Like we did with the simulations, we will first create an empty result and then append the predicted response probabilities and mean RT’s for each trial. Also, since the term \\(s_i^2\\) appears many times across all three formulae, we will want to compute it once and then refer back to it again later. The result is stored as the vector evidence_var.\n\n\nCode\nresult &lt;- c()\n\n# For convenience, we compute this first so we can refer to it each time we need it later\nevidence_var &lt;- evidence_sd^2\n\nfor (i in 1:length(probe_item)) {\n    trial_p_yes &lt;- (1 - exp(-2 * par[\"a\"] * evidence_mean[i] * par[\"w\"] / evidence_var[i])) / (1 - exp(-2 * par[\"a\"] * evidence_mean[i] / evidence_var[i]))\n    trial_mean_rt_yes &lt;- par[\"t0\"] + (par[\"a\"] / evidence_mean[i]) * (1 / tanh(par[\"a\"] * evidence_mean[i] / evidence_var[i]) - par[\"w\"] / tanh(par[\"a\"] * evidence_mean[i] * par[\"w\"] / evidence_var[i]))\n    trial_mean_rt_no &lt;- par[\"t0\"] + (par[\"a\"] / evidence_mean[i]) * (1 / tanh(par[\"a\"] * evidence_mean[i] / evidence_var[i]) - (1 - par[\"w\"]) / tanh(par[\"a\"] * evidence_mean[i] * (1 - par[\"w\"]) / evidence_var[i]))\n    \n    result &lt;- rbind(\n        result,\n        tibble(trial = i, p_yes = trial_p_yes, mean_rt_yes = trial_mean_rt_yes, mean_rt_no = trial_mean_rt_no)\n    )\n}\n\n\nNow we just need to figure out how to get the chunk of code above into our ebrw_nll function. There are a number of reasonable ways to do this. For example, we could add a new argument called something like return_math_pred which we could set to be TRUE or FALSE depending on whether we wanted the mathematically-computed mean predictions. However, since we already have the n_sims argument we added earlier, we can do something a bit clever. Since the mean predictions are based on what we would observe over an infinite number of simulations, we can allow a user to set n_sims = Inf, where Inf is R’s special shorthand for “infinity”. R uses the is.infinite function to check whether a number is infinite or not.\nAs such, in the code below, we “daisy-chain” the if...else construction we built in the previous section. We first check if (is.infinite(n_sims)) and, if so, run the chunk of code above. We then check for else if (n_sims &gt; 0) and, if so, run the specified but finite number of simulations per trial. Finally, the else statement computes the negative log-likelihoods.\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\n# n_sims: this indicates whether or not simulations are meant to be returned instead of the NLL\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt, n_sims = 0) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    stim_dists &lt;- as.matrix(dist(stim_coords))\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    if (is.infinite(n_sims)) {\n        # 2a. Compute predicted response probabilities and mean RT's for each trial\n        result &lt;- c()\n        \n        evidence_var &lt;- evidence_sd^2\n        \n        for (i in 1:length(probe_item)) {\n            trial_p_yes &lt;- (1 - exp(-2 * par[\"a\"] * evidence_mean[i] * par[\"w\"] / evidence_var[i])) / (1 - exp(-2 * par[\"a\"] * evidence_mean[i] / evidence_var[i]))\n            trial_mean_rt_yes &lt;- par[\"t0\"] + (par[\"a\"] / evidence_mean[i]) * (1 / tanh(par[\"a\"] * evidence_mean[i] / evidence_var[i]) - par[\"w\"] / tanh(par[\"a\"] * evidence_mean[i] * par[\"w\"] / evidence_var[i]))\n            trial_mean_rt_no &lt;- par[\"t0\"] + (par[\"a\"] / evidence_mean[i]) * (1 / tanh(par[\"a\"] * evidence_mean[i] / evidence_var[i]) - (1 - par[\"w\"]) / tanh(par[\"a\"] * evidence_mean[i] * (1 - par[\"w\"]) / evidence_var[i]))\n            \n            result &lt;- rbind(\n                result,\n                tibble(trial = i, p_yes = trial_p_yes, mean_rt_yes = trial_mean_rt_yes, mean_rt_no = trial_mean_rt_no)\n            )\n        }\n        \n        # 3a. Return final result\n        return(result)\n    } else if (n_sims &gt; 0) {\n        # 2b. Simulate a response for each trial\n        result &lt;- c()\n        \n        # For each trial...\n        for (i in 1:length(probe_item)) {\n            # Simulate `n_sims` replications of this trial, using its corresponding parameters\n            trial_sim &lt;- sampWiener(\n                N = n_sims,\n                a = par[\"a\"] / evidence_sd[i],\n                w = par[\"w\"],\n                v = evidence_mean[i] / evidence_sd[i],\n                t0 = par[\"t0\"]\n            )\n            \n            # Append the new set of simulations to our `result`, including an indicator of which trial we are simulating and an index for each simulation\n            result &lt;- rbind(\n                result,\n                tibble(trial = i, sim_index = 1:n_sims, rt = trial_sim$q, response = trial_sim$response)\n            )\n        }\n        \n        # 3b. Return final result\n        return(result)\n    } else {\n        # 2c. Calculate the log-likelihood of each observed response/RT on each trial\n        result &lt;- try(WienerPDF(\n            t = rt,\n            response = response,\n            a = par[\"a\"] / evidence_sd,\n            w = par[\"w\"],\n            v = evidence_mean / evidence_sd,\n            t0 = par[\"t0\"]\n        ))\n        \n        # 3c. Return final result\n        if (class(result) == \"try-error\") {\n            return(Inf)\n        } else {\n            return(-sum(result$logvalue))\n        }\n    }\n}\n\n\nAgain, let’s see how this works. The code below computes the predicted mean response probabilities and RT’s for each trial:\n\n\nCode\nmath_pred &lt;- ebrw_nll(\n    par = fit$par,\n    stim_coords = stim_coords,\n    study_items = study_items,\n    probe_item = probe_item,\n    n_sims = Inf\n)\n\nmath_pred\n\n\n# A tibble: 28 × 4\n   trial       p_yes mean_rt_yes mean_rt_no\n   &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1     1 0.00420            1.84       1.64\n 2     2 0.0359             1.89       1.70\n 3     3 0.990              1.84       1.63\n 4     4 0.990              1.84       1.63\n 5     5 0.991              1.83       1.63\n 6     6 0.990              1.84       1.63\n 7     7 0.990              1.84       1.63\n 8     8 0.0364             1.89       1.70\n 9     9 0.000000120        1.71       1.52\n10    10 0.992              1.83       1.62\n# ℹ 18 more rows\n\n\nLet’s see how the mathematical predictions compare to those we derived from our 1000 simulations earlier, at least for the first few trials:\n\n\nCode\nknitr::kable(full_join(sim_means, math_pred, by = \"trial\")[1:12,], digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrial\nsim_p_upper\nsim_mean_rt_yes\nsim_mean_rt_no\np_yes\nmean_rt_yes\nmean_rt_no\n\n\n\n\n1\n0.001\n1.367\n1.629\n0.004\n1.842\n1.636\n\n\n2\n0.041\n1.948\n1.721\n0.036\n1.888\n1.695\n\n\n3\n0.991\n1.861\n1.384\n0.990\n1.836\n1.631\n\n\n4\n0.986\n1.839\n1.572\n0.990\n1.836\n1.630\n\n\n5\n0.989\n1.867\n2.003\n0.991\n1.833\n1.628\n\n\n6\n0.994\n1.875\n1.967\n0.990\n1.836\n1.631\n\n\n7\n0.993\n1.807\n1.444\n0.990\n1.835\n1.629\n\n\n8\n0.028\n1.938\n1.725\n0.036\n1.888\n1.695\n\n\n9\n0.000\nNaN\n1.521\n0.000\n1.710\n1.520\n\n\n10\n0.988\n1.807\n1.765\n0.992\n1.830\n1.625\n\n\n11\n0.989\n1.807\n1.597\n0.992\n1.831\n1.625\n\n\n12\n0.991\n1.809\n1.789\n0.992\n1.831\n1.625\n\n\n\n\n\nAs we can see, the simulated means tend to be close to the mathematical predictions, but can be off when it comes to predicting mean RT’s for rare responses, since those predictions are based on only a few simulated trials.\n\n\n9.5.3 Comparing predicted to observed\nAs noted above, being able to compute predictions from the EBRW is important to verify that it is doing a good job fitting the data. A common way to do this is to make a plot with observed values on the horizontal axis and predicted value on the vertical axis. To the extent that the points in the plot cluster around the diagonal, we have reason to believe that the model’s predictions are closely aligned with what was observed. Relatedly, we can report the \\(R^2\\) value, i.e., the proportion of variance explained by the model; this is just the square of the Pearson correlation coefficient.\nIn the present application, we have two observed values on each trial: the response and the RT. Thus, we’ll need to make two plots, one for each measure. But to do that, we’ll first need to get both the observed and predicted data into the same data structure. That’s what the chunk of code below does.\n\n\nCode\n# This tibble contains the observed data for each trial\nobs_data &lt;- tibble(\n    trial = 1:length(probe_item),\n    rt = rt,\n    response = response - 1 # The \"- 1\" is because \"response\" is coded as 2 (\"yes\") and 1 (\"no\"), but it'll be easier to instead code this as 1 (\"yes\") and 0 (\"no\").\n)\n\n# Since both the observed and predicted data contain a \"trial\" column, use that to join them together\nobs_pred &lt;- full_join(obs_data, math_pred, by = \"trial\")\n\n\nNow let’s try to naively plot the observed and predicted data against one another, as we just described. Note that this will not really work as intended!\n\n\nCode\nresp_plot &lt;- obs_pred %&gt;%\n    ggplot(aes(x = response, y = p_yes, color = factor(response))) +\n    geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n    geom_point() +\n    coord_equal() +\n    scale_color_manual(values = c(\"1\" = \"#377eb8\", \"0\" = \"#e41a1c\")) +\n    labs(x = \"Observed response\", y = \"Predicted P(\\\"Yes\\\")\", color = \"Response\")\n\nrt_plot &lt;- obs_pred %&gt;%\n    mutate(pred_rt = if_else(response == 1, mean_rt_yes, mean_rt_no)) %&gt;%\n    ggplot(aes(x = rt, y = pred_rt, color = factor(response))) +\n    geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n    geom_point() +\n    coord_equal() +\n    scale_color_manual(values = c(\"1\" = \"#377eb8\", \"0\" = \"#e41a1c\")) +\n    labs(x = \"Observed RT\", y = \"Predicted mean RT\", color = \"Response\")\n\nresp_plot + rt_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nIn the plots above, each point represents a single trial. While we can see in the left plot (barely) that the model generally tends to predict a high probability of saying “yes” on trials where the participant actually said “yes”, it is much harder to see what is going on in the right plot that shows RT’s. The issue is that the model is making a prediction about the mean RT whereas each point represents a sample from the participant’s RT distribution.\nTherefore, a more informative way to judge whether the model is making sensible predictions is to compare its mean predictions against average observed performance. Since this is a recognition memory task, it makes the most sense to average performance depending on whether the probe item was a target (i.e., was one of the studied items) or a foil (i.e., was not one of the studied items). The code below first defines a new column in the obs_pred tibble that indicates the type of trial before aggregating both observed and predicted performance by probe type, then making plots similar to those above.\n\n\nCode\nobs_pred$probe_type &lt;- NA\n\nfor (i in 1:length(probe_item)) {\n    # Note the use of \"%in%\"\n    if (probe_item[i] %in% study_items[i,]) {\n        obs_pred$probe_type[i] &lt;- \"Target\"\n    } else {\n        obs_pred$probe_type[i] &lt;- \"Foil\"\n    }\n}\n\nobs_pred$probe_type &lt;- factor(obs_pred$probe_type, levels = c(\"Target\", \"Foil\"))\n\nresp_plot &lt;- obs_pred %&gt;%\n    group_by(probe_type) %&gt;%\n    summarize(mean_obs = mean(response), mean_pred = mean(p_yes)) %&gt;%\n    ggplot(aes(x = mean_obs, y = mean_pred, color = probe_type)) +\n    geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n    geom_point() +\n    coord_equal() +\n    scale_color_manual(values = c(\"Target\" = \"#377eb8\", \"Foil\" = \"#e41a1c\")) +\n    labs(x = \"Observed response proportion\", y = \"Predicted P(\\\"Yes\\\")\", color = NULL)\n\nrt_plot &lt;- obs_pred %&gt;%\n    mutate(pred_rt = if_else(response == 1, mean_rt_yes, mean_rt_no)) %&gt;%\n    group_by(probe_type) %&gt;%\n    summarize(mean_obs = mean(rt), mean_pred = mean(pred_rt)) %&gt;%\n    ggplot(aes(x = mean_obs, y = mean_pred, color = probe_type)) +\n    geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n    geom_point() +\n    coord_equal() +\n    scale_color_manual(values = c(\"Target\" = \"#377eb8\", \"Foil\" = \"#e41a1c\")) +\n    labs(x = \"Observed mean RT\", y = \"Predicted mean RT\", color = NULL)\n\nresp_plot + rt_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nThe above graphs are not only easier to see, they allow us to more fairly critique the model. The model does a great job predicting response probabilities as well as mean RT for targets. It looks like it predicts longer mean RT to foils than was observed, but that could also reflect sampling variability. In general, the plots above suggest that the model is generally good at fitting this participant’s data, though it may be missing something about the RT’s. That misfit in part justifies the more complex version of the EBRW that Gillespie & Cox (2024) fit to these data.\n\n\n9.5.4 Making novel predictions\nNow that we have the ability to quickly extract predictions from the EBRW, it is worth pointing out that we can now make predictions about how this participant would respond in a trial they did not experience, perhaps with a different set of study items or a different probe item.\nTo go even broader, we could imagine a new experiment in which the auditory stimuli were degraded by noise. In that case, we might expect the participant’s sensitivity parameter to be lower in that condition. We could simulate how this would affect their recognition performance with this stimuli (though it is likely that, in such an experiment, the participant would also adjust their response caution and response bias to compensate).\nThe point is that the EBRW permits us to make predictions regarding particular experimental manipulations, which can then help us design future experiments. This is because the EBRW’s predictions depend upon the specifics of how a participant represents the items in the experiment and contains parameters with meanings that are clearly linked to cognitive constructs like “sensitivity”. This was not necessarily the case with the “generic” decision models we explored earlier in the course.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#putting-it-all-together",
    "href": "ebrw.html#putting-it-all-together",
    "title": "9  Exemplar of the Exemplar Based Random Walk",
    "section": "9.6 Putting it all together",
    "text": "9.6 Putting it all together\nThe chunk of code below puts all our EBRW work in one place. There are blanks left in certain areas that could either be filled by observed results or by your own imagination. In addition, a complete script that simulates data using the EBRW, fits the EBRW to the simulated data, and compares the simulated to the fitted data is provided here.\n\n\nCode\n# This could be derived using MDS\nstim_coords &lt;- ...\n\n# These could be actual trials in an experiment or used to construct imaginary trials for simulation\nstudy_items &lt;- ...\nprobe_item &lt;- ...\n\n# These could be observed or simulated; they may not even be needed if you are just doing simulation\nresponse &lt;- ...\nrt &lt;- ...\n\n# This could be from a model fit or specified manually to do simulations\npar &lt;- ...\n\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\n# n_sims: this indicates whether or not simulations are meant to be returned instead of the NLL\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt, n_sims = 0) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    stim_dists &lt;- as.matrix(dist(stim_coords))\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    if (is.infinite(n_sims)) {\n        # 2a. Compute predicted response probabilities and mean RT's for each trial\n        result &lt;- c()\n        \n        evidence_var &lt;- evidence_sd^2\n        \n        for (i in 1:length(probe_item)) {\n            trial_p_yes &lt;- (1 - exp(-2 * par[\"a\"] * evidence_mean[i] * par[\"w\"] / evidence_var[i])) / (1 - exp(-2 * par[\"a\"] * evidence_mean[i] / evidence_var[i]))\n            trial_mean_rt_yes &lt;- par[\"t0\"] + (par[\"a\"] / evidence_mean[i]) * (1 / tanh(par[\"a\"] * evidence_mean[i] / evidence_var[i]) - par[\"w\"] / tanh(par[\"a\"] * evidence_mean[i] * par[\"w\"] / evidence_var[i]))\n            trial_mean_rt_no &lt;- par[\"t0\"] + (par[\"a\"] / evidence_mean[i]) * (1 / tanh(par[\"a\"] * evidence_mean[i] / evidence_var[i]) - (1 - par[\"w\"]) / tanh(par[\"a\"] * evidence_mean[i] * (1 - par[\"w\"]) / evidence_var[i]))\n            \n            result &lt;- rbind(\n                result,\n                tibble(trial = i, p_yes = trial_p_yes, mean_rt_yes = trial_mean_rt_yes, mean_rt_no = trial_mean_rt_no)\n            )\n        }\n        \n        # 3a. Return final result\n        return(result)\n    } else if (n_sims &gt; 0) {\n        # 2b. Simulate a response for each trial\n        result &lt;- c()\n        \n        # For each trial...\n        for (i in 1:length(probe_item)) {\n            # Simulate `n_sims` replications of this trial, using its corresponding parameters\n            trial_sim &lt;- sampWiener(\n                N = n_sims,\n                a = par[\"a\"] / evidence_sd[i],\n                w = par[\"w\"],\n                v = evidence_mean[i] / evidence_sd[i],\n                t0 = par[\"t0\"]\n            )\n            \n            # Append the new set of simulations to our `result`, including an indicator of which trial we are simulating and an index for each simulation\n            result &lt;- rbind(\n                result,\n                tibble(trial = i, sim_index = 1:n_sims, rt = trial_sim$q, response = trial_sim$response)\n            )\n        }\n        \n        # 3b. Return final result\n        return(result)\n    } else {\n        # 2c. Calculate the log-likelihood of each observed response/RT on each trial\n        result &lt;- try(WienerPDF(\n            t = rt,\n            response = response,\n            a = par[\"a\"] / evidence_sd,\n            w = par[\"w\"],\n            v = evidence_mean / evidence_sd,\n            t0 = par[\"t0\"]\n        ))\n        \n        # 3c. Return final result\n        if (class(result) == \"try-error\") {\n            return(Inf)\n        } else {\n            return(-sum(result$logvalue))\n        }\n    }\n}",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#exercises",
    "href": "ebrw.html#exercises",
    "title": "9  Exemplar of the Exemplar Based Random Walk",
    "section": "9.7 Exercises",
    "text": "9.7 Exercises\n\nThis exercise makes use of the EBRW example script mentioned in the main text. It replicates the recognition memory task described in the chapter, assuming two items per study list followed by a single probe item. With this script, you can specify stim_coords and, by default, it will simulate one trial for each possible combination of study items and probe item (i.e., all possible 2-item lists, followed by all possible probe items). The script then fits the EBRW to the simulated data using exactly the same code as in the chapter. Finally, the script also produces plots of predicted vs. observed performance just like in the chapter.\n\nReplace the stim_coords matrix in the example file with one of your own devising. As a reminder, each row should correspond to a different item with each column giving its coordinate along the corresponding dimension. You might define your stim_coords based on pre-existing knowledge of psychologically-relevant stimulus dimensions (like with the color example at the beginning of the chapter). You could also find a matrix of coordinates obtained from a published scaling study, or one you carry out yourself. Finally, you could come up with stim_coords that represent a meaningful hypothesis about how some set of items is represented psychologically. In any case, provide your matrix and explain what the items are and what each dimension is.\nModify the simulation parameters in sim_par until the model’s simulated performance resembles what you would expect to see in a real experiment using your stimuli.\nDescribe an experimental manipulation that you expect would influence the EBRW’s sensitivity parameter. Simulate performance using a lower value of the sensitivity parameter than you used in part (b) and comment on how the model’s predicted performance is affected.\nFind values for the other parameters in the model (e.g., boundary separation, drift bias) that allow it to achieve the same speed and accuracy with lower sensitivity (part c) as it achieved with higher sensitivity (part d).\n\nYou may have noticed that our implementation of the EBRW assumes that each list of studied items is the same length. Describe how to modify our ebrw_nll function so that study_items can include NA values, thus allowing for lists to vary in length, similar to how we allowed our race model to be robust to sets of different size. Hint: All this involves is putting a na.rm = TRUE somewhere in our code!\nThis is a follow-up to the previous exercise. In applications of the EBRW to experiments that vary the number of items, it is typical to assume that the “criterion” varies linearly with list length (Nosofsky et al., 2011, 2014). Modify our ebrw_nll function to accommodate this version of the model. The chunk of code below gives a suggestion about how this might be done in such a way that preserves the functionality of the code we used throughout this chapter by keeping the “criterion” parameter and adding a “criterion_slope” parameter.\n\n\n\nCode\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt, n_sims = 0) {\n    ...\n    \n    if (is.na(par[\"criterion_slope\"])) {\n        criterion_slope &lt;- 0\n    } else {\n        criterion_slope &lt;- par[\"criterion_slope\"]\n    }\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        \n        # HINT: You will need to find the value of `list_length` on each trial---how will you do this?  (You might think about the fact that we are using `NA` values to indicate when `study_items[i,]` has fewer items than the number of columns in `study_items`).\n        this_criterion &lt;- par[\"criterion\"] + criterion_slope * list_length\n        \n        p &lt;- summed_sim / (summed_sim + this_criterion)\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    ...\n}\n\n\n\n\n\n\nGarner, W. R., & Felfoldy, G. L. (1970). Integrality of stimulus dimensions in various types of information processing. Cognitive Psychology, 1, 225–241.\n\n\nGillespie, N. F., & Cox, G. E. (2024). Perception and memory for novel auditory stimuli: Similarity, serial position, and list homogeneity. PsyArXiv. https://doi.org/10.31234/osf.io/n294a\n\n\nNosofsky, R. M. (1986). Attention, similarity, and the identification-categorization relationship. Journal of Experimental Psychology: General, 115(1), 39–57.\n\n\nNosofsky, R. M. (1992). Similarity scaling and cognitive process models. Annual Review of Psychology, 43, 25–53.\n\n\nNosofsky, R. M., Cox, G. E., Cao, R., & Shiffrin, R. M. (2014). An exemplar-familiarity model predicts short-term and long-term probe recognition across diverse forms of memory search. Journal of Experimental Psychology: Learning, Memory, and Cognition, 40(6), 1524–1539.\n\n\nNosofsky, R. M., Little, D. R., Donkin, C., & Fific, M. (2011). Short-term memory scanning viewed as exemplar-based categorization. Psychological Review, 118(2), 280–315.\n\n\nNosofsky, R. M., & Palmeri, T. J. (1997). An exemplar-based random walk model of speeded classification. Psychological Review, 104(2), 266–300.\n\n\nShepard, R. N. (1962a). The analysis of proximities: Multidimensional scaling with an unknown distance function. I. Psychometrika, 27(2), 125–140. https://doi.org/https://doi.org/10.1007/BF02289630\n\n\nShepard, R. N. (1962b). The analysis of proximities: Multidimensional scaling with an unknown distance function. II. Psychometrika, 27(3), 219–246. https://doi.org/https://doi.org/10.1007/BF02289621",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "vector_reps.html",
    "href": "vector_reps.html",
    "title": "10  Distributed representations, similarity, and recognition",
    "section": "",
    "text": "10.1 Representations\nIn the previous chapter, we were introduced to the Exemplar-Based Random Walk (EBRW) model. We saw how it could be used to model behavior in a recognition memory task in which a participant studies a number of items and is then presented with a “probe” item and must decide whether or not the probe was among the items that were studied. The EBRW explained such decisions in terms of summed similarity between the probe and each studied item. In the EBRW, similarity was a function of the distance between representations of the probe and study items. The probe and study items were represented as points in a multidimensional space. Each dimension of this space corresponds to a “feature” or “attribute” that an item could have, with different coordinates corresponding to different values of that feature/attribute. Often, these features/attributes can be identified with particular physical characteristics of an item (like hue, brightness, roughness, etc.).\nThe EBRW is a great example of how a computational cognitive model can help explain behavior in terms of latent representations that someone forms of their experience. It is also an example of a distributed representation of an item, in that an item is characterized by the particular distribution or configuration of values it has across a number of features/attributes. In this chapter, we explore more general forms of distributed representations and more general ways that we can model similarity between representations. We will continue to model representations of items as sets of numbers—that is, as a vector—but we will abandon the notion that each element of the vector has a direct relationship with physical characteristics of the item.\nWhile it is beyond the scope of our course to delve into the philosophy behind the idea of “representation”, it is worth spending some time to think about what a “representation” is in the context of a cognitive model, so that we can at least arrive at a reasonable operational definition of the term. For additional discussion, see [].\nFor our purposes, we can define a representation as a formal entity posited by a cognitive model that enables certain kinds of processes that, when applied to that representation, enable the model to make predictions about performance in a particular task context. You may notice that this definition of a representation is tied to both the processes posited by a model as well as the kinds of predictions the model is expected to make. This is because a representation only has meaning in the context of these other aspects of the model.\nWe can think of a representation as a kind of “code” that conveys information that can be “read” by a suitable process. To take a concrete, if not exactly cognitive, example, consider the task of sorting mail. Imagine that we are working in a distribution center that receives packages that are intended to go to different parts of the country. Our job is to route those packages to the post office nearest their final destinations. We can accomplish this task by reading (processing) the ZIP codes on each package. The ZIP code is a numerical representation of the geographical area for which a package is destined, which when processed by an appropriate reader enables them to route the package to the correct office. This example illustrates the essential characteristics of a representation:\nWe can see how these principles manifested in the random walk/diffusion/race models we explored earlier. As we discussed, these models represent a decision-maker’s current state of mind in terms of one or more numbers that represent the degree to which the decision-maker favors each option they are choosing between. Representing evidence as a number enables an evidence accumulation process that can be modeled via the mathematical operation of addition. These models do not necessarily claim that the numbers that represent accumulated evidence are “implemented” in any particular way. That said, as we will see later in this course, it is possible to relate representations of accumulated evidence to scalp (Philiastides et al., 2006) and single-neuron (Purcell et al., 2010; Purcell et al., 2012; Shadlen & Newsome, 2001) electrophysiology as well as fMRI BOLD signals (Turner et al., 2013).",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributed representations, similarity, and recognition</span>"
    ]
  },
  {
    "objectID": "vector_reps.html#representations",
    "href": "vector_reps.html#representations",
    "title": "10  Distributed representations, similarity, and recognition",
    "section": "",
    "text": "A ZIP code only serves as a representation of geographical area when processed appropriately. If you don’t have sensory organs to see the numbers or you do not know how to interpret the numbers, the ZIP code is meaningless. As noted above, a representation only has meaning in the context of the processes applied to it to accomplish a task.\nThere isn’t necessarily just one way to represent something. We could write ZIP codes with Roman numerals instead of Arabic numerals—while this would entail changing the kind of process applied to the ZIP code (because the two formats must be read in different ways), both are legitimate ways of representing geographical regions in the context of the mail-sorting task. We could even use a graphical representation, like a map with a dot indicating the destination. Again, such a representation would need to be processed appropriately.\nThe structure of a representation may or may not enable multiple kinds of processes to be applied to it, potentially to serve different tasks. For example, ZIP codes do not just serve as labels for different regions—nearby regions have similar ZIP codes. This is because the earlier digits indicate broad geographic regions while later digits represent narrower subdivisions of those larger regions. Therefore, one could apply a comparison process to these ZIP codes to determine not just whether two packages were going to the same region, but whether they were going to nearby regions. This would enable sorting to be robust to certain kinds of contingencies. For example, if the post office in one region were closed, the comparison process could allow you to determine the nearest open office to send the package to. A graphical representation of the postal destination would allow for even finer gradations of similarity.\nPositing a representation does not necessarily commit to any particular way that such a representation may be implemented. Here, I use the term “implementation” in the same way as in the three levels of description posited by Marr (1982). A ZIP code can be “implemented” as ink on a page, as pixels on a screen, as an etching on a tablet, as a sound wave (if spoken aloud), etc. While these different implementations of a ZIP code would entail different implementations of how they were processed, they do not alter the form or content of the ZIP code. The broader point is that the kinds of representations posited by cognitive models are defined not by their physical forms, but rather by their abstract structure and the kinds of processes they support.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributed representations, similarity, and recognition</span>"
    ]
  },
  {
    "objectID": "vector_reps.html#types-of-vector-representations",
    "href": "vector_reps.html#types-of-vector-representations",
    "title": "10  Distributed representations, similarity, and recognition",
    "section": "10.2 Types of vector representations",
    "text": "10.2 Types of vector representations\nMany cognitive models make use of representations that take the form of vectors, that is, as ordered collections of numbers. We saw one kind of vector representation in the EBRW. In the EBRW, each item was represented as a vector of coordinates that specified that item’s position within a multidimensional space. That representation enabled the EBRW to operationalize similarity as a function of distance between vector representations of items. We now consider other broad classes of vector representations and, in the next section, we see how different kinds of representations enable other operationalizations of similarity. Finally, we see how similarity can be used to model performance in different kinds of tasks.\nIn the examples below, to keep things concrete, let’s imagine a participant is engaged in a task that involves making judgments about different concepts. There are eight concepts, each of which is a living thing from a particular category (these concepts are the same ones used in the examples from Rogers & McClelland (2004)):\n\nPine\nOak\nRose\nDaisy\nRobin\nCanary\nSunfish\nSalmon\n\nYou may notice that these concepts can be grouped together in different ways. For example, we could divide them up into plants and animals; into trees, flowers, birds, and fish; into things that are red, green, or yellow; etc. The kinds of vector representations we consider below will illustrate how these relations can be encoded in the vectors.\n\n10.2.1 Localist representations\nPerhaps the simplest and most direct way to represent each of the eight concepts above is to use a localist representation. The term “localist” refers to the idea that there is a one-to-one mapping between each concept and its “location” within a vector. Specifically, with a localist representation, each vector has as many entries as there are things to be represented. So if we have eight concepts, each of them will be represented with an eight-dimensional vector. Each of those vectors will contain zeros except for a “1” in exactly one entry in the vector. The “location” of the 1 is what determines which concept the vector represents.\nThe matrix below gives an example of a set of localist representations of each of the eight concepts above. Each row is the representation of a different concept.\n\n\nCode\nconcept_names &lt;- c(\"Pine\", \"Oak\", \"Rose\", \"Daisy\", \"Robin\", \"Canary\", \"Sunfish\", \"Salmon\")\ncategory_names &lt;- rep(c(\"Tree\", \"Flower\", \"Bird\", \"Fish\"), each = 2)\n\nrep_localist &lt;- diag(length(concept_names))\nrownames(rep_localist) &lt;- concept_names\n\nprint(rep_localist)\n\n\n        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\nPine       1    0    0    0    0    0    0    0\nOak        0    1    0    0    0    0    0    0\nRose       0    0    1    0    0    0    0    0\nDaisy      0    0    0    1    0    0    0    0\nRobin      0    0    0    0    1    0    0    0\nCanary     0    0    0    0    0    1    0    0\nSunfish    0    0    0    0    0    0    1    0\nSalmon     0    0    0    0    0    0    0    1\n\n\nNote that there is nothing special about having the 1’s on the diagonal—or even about using 1’s in the first place! For example, the matrix below is another example of localist representations of the same set of concepts:\n\n\nCode\nalt_rep_localist &lt;- rep_localist %*% diag(rpois(n = nrow(rep_localist), lambda = 5) + 1)\nalt_rep_localist &lt;- alt_rep_localist[sample(nrow(alt_rep_localist)),]\nrownames(alt_rep_localist) &lt;- concept_names\n\nprint(alt_rep_localist)\n\n\n        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\nPine       0   12    0    0    0    0    0    0\nOak        0    0    0    0    0    0    0    7\nRose       0    0    0    0    0    0    4    0\nDaisy      0    0    0    5    0    0    0    0\nRobin      0    0    0    0    0    5    0    0\nCanary     0    0    5    0    0    0    0    0\nSunfish    0    0    0    0    2    0    0    0\nSalmon     4    0    0    0    0    0    0    0\n\n\nWe could even replace all the 0’s with NAs or -99 or any other value that we agree to interpret as a kind of “background”. Ultimately, what matters about a localist representation is that what a vector represents is indicated by which entry in the vector is “not background”.\n\n\n10.2.2 Separable distributed representations\nAnother defining characteristic of a localist representation is that it does not allow for gradations of similarity. Either two representations refer to the same thing or they refer to different things. Distributed vector representations, like those used in the EBRW, allow for representations to be partially similar to one another.\nAlthough it is not necessarily standard terminology in the field, I think it is important to distinguish between separable and integral distributed representations. The difference between them is that, with a separable distributed representation, it is possible to identify the elements of the vector with particular attributes/features/dimensions of the items being represented. With an integral distributed representation, the individual elements of the vector have no interpretation on their own—instead, different items are represented by different patterns of values across all the elements of a vector. This distinction probably seems pretty abstract, so let’s dig into some concrete examples.\nReturning to the set of concepts above, one way to construct a set of separable distributed representations is to consider different attributes that each concept may or may not have. Each element of the vectors corresponds to a different attribute. For each concept, its representation will have a value of 1 in the entries corresponding to attributes that the concept possesses and a value of 0 in the entries corresponding to attributes that the concept lacks. This is illustrated below:\n\n\nCode\nattribute_names &lt;- c(\"can_grow\", \"is_living\", \"has_roots\", \"has_bark\", \"is_big\", \"has_branches\", \"is_green\", \"has_leaves\", \"has_petals\", \"is_pretty\", \"is_red\", \"is_yellow\", \"can_move\", \"has_feathers\", \"can_fly\", \"has_wings\", \"can_sing\", \"can_swim\", \"has_gills\", \"has_scales\")\n\nrep_distrib_sep1 &lt;- matrix(c(\n    1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n    1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n    1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n    1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n    1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1\n), nrow = 8, ncol = length(attribute_names), byrow = TRUE, dimnames = list(concept_names, attribute_names))\n\nknitr::kable(rep_distrib_sep1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncan_grow\nis_living\nhas_roots\nhas_bark\nis_big\nhas_branches\nis_green\nhas_leaves\nhas_petals\nis_pretty\nis_red\nis_yellow\ncan_move\nhas_feathers\ncan_fly\nhas_wings\ncan_sing\ncan_swim\nhas_gills\nhas_scales\n\n\n\n\nPine\n1\n1\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOak\n1\n1\n1\n1\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nRose\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nDaisy\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nRobin\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n0\n0\n0\n0\n\n\nCanary\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n0\n0\n\n\nSunfish\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n1\n1\n\n\nSalmon\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n1\n1\n\n\n\n\n\nAlthough we will consider similarity more deeply in the next section, note that the distributed representations above make it possible to judge similarity based on the extent to which two concepts’ representations do or do not share attributes. Indeed, the different categories (plants/animals) and subcategories (trees/flowers/birds/fish) to which the concepts belong are implicit in which attributes are shared between concepts.\nIn the example above, each entry in the vector corresponded to a distinct attribute. It is also possible to construct separable distributed representations where attributes are encoded with sectors of a vector. For example, rather than treating is_red, is_yellow, and is_green as different attributes, we might assign a “sector” consisting of several entries to represent the color associated with a concept. Maybe that sector consists of three entries, for hue, chroma, and lightness, as in the example at the beginning of the EBRW chapter.\nRelatedly, distributed representations can use continuous values—they are not restricted to binary indicators of the presence/absence of an attribute. For example, instead of an attribute called is_big, we could have a size attribute which corresponds to the size of the concept being represented.\nFor example, here is how one might represent color and size for each of the concepts in our running example:\n\n\nCode\nalt_attribute_names &lt;- c(\"size\", \"hue\", \"chroma\", \"lightness\")\n\nrep_distrib_sep2 &lt;- matrix(c(\n    50, 131, 36, 27,\n    40, 20, 60, 16,\n    0.1, 3, 58, 79,\n    0.2, 52, 100, 50,\n    0.3, 9, 63, 38,\n    0.1, 60, 100, 63,\n    0.5, 32, 100, 59,\n    1, 5, 99, 71\n), nrow = 8, ncol = length(alt_attribute_names), byrow = TRUE, dimnames = list(concept_names, alt_attribute_names))\n\nknitr::kable(rep_distrib_sep2)\n\n\n\n\n\n\nsize\nhue\nchroma\nlightness\n\n\n\n\nPine\n50.0\n131\n36\n27\n\n\nOak\n40.0\n20\n60\n16\n\n\nRose\n0.1\n3\n58\n79\n\n\nDaisy\n0.2\n52\n100\n50\n\n\nRobin\n0.3\n9\n63\n38\n\n\nCanary\n0.1\n60\n100\n63\n\n\nSunfish\n0.5\n32\n100\n59\n\n\nSalmon\n1.0\n5\n99\n71\n\n\n\n\n\nWith a separable distributed representation, we can increase the dimensionality of the vectors as much as we want. For example, a separable distributed representation might consist of hundreds of dimensions. However, it is unlikely that all of these dimensions would be relevant to any particular task. There may also be capacity limits on the number of dimensions that someone could make use of at any particular time. This highlights one final, but important, aspects of a separable distributed representation: Because different kinds of information about an item are represented in different elements of the vector, separable distributed representations enable us to model attention to particular kinds of information. Specifically, as we shall see below, we can assign weights to each dimension that represent the degree to which someone attends to that dimension within a given task context. A separable distributed representation can thus support performance in many different tasks by assuming that some tasks entail attention to different elements of that representation.\n\n\n10.2.3 Integral distributed representations\nThe final kind of vector representation we will consider are integral distributed representations. The difference between integral and separable distributed representations is that the vector elements of an integral representation cannot be identified with distinct features/attributes of the items. Rather, an item’s representation consists of the complete pattern of elements across the vector. An integral representation can thus be thought of as a “holistic” or “configural” representation, because its individual elements can only be understood as part of the complete pattern of entries in the vector.\nAn analogy may help clarify this admittedly abstract issue: Consider baking a cake. Before you bake the cake, its ingredients are separable: You have a pile of flour, a carton of eggs, a jug of milk, etc. One could imagine constructing a separable distributed representation of the pre-baked cake in which each element corresponded to a particular ingredient and the entries specified the quantities of each ingredient. However, once the ingredients are mixed and the cake is baked, it is no longer reducible to its component ingredients. Those same ingredients, in those same quantities, would not necessarily result in the same cake—it is the particular way in which the ingredients are combined and prepared that results in the final product. The way any individual ingredient contributes to the baked cake depends on its relationship to all the other ingredients in the cake. In that sense, the baked cake is best represented using an integral as opposed to separable representation.\nUnlike separable representations, then, integral representations make it impossible to model giving different amounts of weight/attention to different attributes of an item. This is an important distinction, and is why I adopted the terms “integral” and “separable” to distinguish between these kinds of representations. Garner & Felfoldy (1970) used those terms to distinguish between cases in which a participant could selectively attend to one feature while ignoring others (separable) and cases in which a participant could not ignore other features (integral).\nIt is still possible to conceive of similarity between integral representations. It is just that similarity depends not on sharing specific elements of a vector, but instead on having similar patterns of values across elements. To visualize this, we can graph a vector representation with the index of the elements along the horizontal axis and the value on the vertical axis. Integral representations are similar to the extent that the resulting graphs have similar shapes. This is illustrated below.\n\n\nCode\nx &lt;- rnorm(n = 10)\nx &lt;- (x - mean(x)) / sd(x)\n\ns_vals &lt;- round(seq(0, 1, length.out = 6), 2)\n\ntoPlot &lt;- c()\n\nfor (s in s_vals) {\n    if (s &lt; 1) {\n        while (TRUE) {\n            y &lt;- rnorm(n = 10)\n            y &lt;- (y - mean(y)) / sd(y)\n            \n            if (round(cor(x, y), 2) == s) break\n        }\n    } else {\n        y &lt;- x\n    }\n    \n    toPlot &lt;- rbind(\n        toPlot,\n        tibble(sim_factor = paste(\"Similarity =\", s), rep = \"A\", i = 1:length(x), val = x),\n        tibble(sim_factor = paste(\"Similarity =\", s), rep = \"B\", i = 1:length(y), val = y)\n    )\n}\n\ntoPlot %&gt;%\n    ggplot(aes(x = i, y = val, color = rep, linetype = rep)) +\n    geom_line() +\n    facet_wrap(\"sim_factor\") +\n    labs(x = \"Vector index\", y = \"Value\", color = \"Representation\", linetype = \"Representation\", title = \"Similarity between integral representations\")\n\n\n\n\n\n\n\n\n\nIt will not have escaped your notice (especially if you look at the code for the above chunk) that similarity between integral representations can be modeled in terms of their correlation. We will explore this more below.\nFor now, we can return to our running example to see what integral distributed representations of our eight concepts might look like. In fact, the example representations below are based on ones derived from a statistical model of word co-occurrence called Latent Semantic Analysis (Landauer & Dumais, 1997). As noted below, many machine learning models make use of integral distributed representations, and some of these are even plausible cognitive models of learning, which we will explore in the next chapter.\n\n\nCode\nload(\"lsa_reps_examples.rdata\")\nrownames(lsa_reps_examples) &lt;- concept_names\n\nknitr::kable(lsa_reps_examples)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nV29\nV30\nV31\nV32\nV33\nV34\nV35\nV36\nV37\nV38\nV39\nV40\nV41\nV42\nV43\nV44\nV45\nV46\nV47\nV48\nV49\nV50\nV51\nV52\nV53\nV54\nV55\nV56\nV57\nV58\nV59\nV60\nV61\nV62\nV63\nV64\nV65\nV66\nV67\nV68\nV69\nV70\nV71\nV72\nV73\nV74\nV75\nV76\nV77\nV78\nV79\nV80\nV81\nV82\nV83\nV84\nV85\nV86\nV87\nV88\nV89\nV90\nV91\nV92\nV93\nV94\nV95\nV96\nV97\nV98\nV99\nV100\nV101\nV102\nV103\nV104\nV105\nV106\nV107\nV108\nV109\nV110\nV111\nV112\nV113\nV114\nV115\nV116\nV117\nV118\nV119\nV120\nV121\nV122\nV123\nV124\nV125\nV126\nV127\nV128\nV129\nV130\nV131\nV132\nV133\nV134\nV135\nV136\nV137\nV138\nV139\nV140\nV141\nV142\nV143\nV144\nV145\nV146\nV147\nV148\nV149\nV150\nV151\nV152\nV153\nV154\nV155\nV156\nV157\nV158\nV159\nV160\nV161\nV162\nV163\nV164\nV165\nV166\nV167\nV168\nV169\nV170\nV171\nV172\nV173\nV174\nV175\nV176\nV177\nV178\nV179\nV180\nV181\nV182\nV183\nV184\nV185\nV186\nV187\nV188\nV189\nV190\nV191\nV192\nV193\nV194\nV195\nV196\nV197\nV198\nV199\nV200\nV201\nV202\nV203\nV204\nV205\nV206\nV207\nV208\nV209\nV210\nV211\nV212\nV213\nV214\nV215\nV216\nV217\nV218\nV219\nV220\nV221\nV222\nV223\nV224\nV225\nV226\nV227\nV228\nV229\nV230\nV231\nV232\nV233\nV234\nV235\nV236\nV237\nV238\nV239\nV240\nV241\nV242\nV243\nV244\nV245\nV246\nV247\nV248\nV249\nV250\nV251\nV252\nV253\nV254\nV255\nV256\nV257\nV258\nV259\nV260\nV261\nV262\nV263\nV264\nV265\nV266\nV267\nV268\nV269\nV270\nV271\nV272\nV273\nV274\nV275\nV276\nV277\nV278\nV279\nV280\nV281\nV282\nV283\nV284\nV285\nV286\nV287\nV288\nV289\nV290\nV291\nV292\nV293\nV294\nV295\nV296\nV297\nV298\nV299\nV300\nV301\n\n\n\n\nPine\n57.307893\n-33.412359\n16.2100709\n44.705718\n5.4170146\n-77.312664\n-14.9572611\n-27.9986255\n-32.299533\n-14.5052352\n-4.5012929\n-14.7266570\n-16.1968384\n-2.7994879\n22.4571901\n15.729012\n11.851067\n24.6211851\n-9.1846293\n2.2328819\n-50.0163069\n-3.0823218\n33.7986198\n3.119457\n-17.114383\n13.4889538\n4.2553778\n-1.5470188\n-0.0345441\n-18.7060762\n-5.5927648\n-3.3022175\n-8.9941651\n-1.8332407\n22.5889364\n13.926796\n-9.5462912\n0.7528040\n20.7099540\n8.977873\n-18.0528367\n13.0251756\n5.1726928\n12.223609\n1.9835759\n16.4832762\n14.7923809\n-2.818340\n9.3103527\n-0.0929313\n-2.4702942\n-25.6337961\n14.202371\n-11.0013780\n-15.1385398\n20.0835240\n-0.0384213\n-17.011600\n14.5557277\n-3.6187865\n-8.1690392\n9.0190558\n6.2733016\n-6.4742631\n-21.835881\n7.759060\n9.4504609\n-9.293384\n2.3812821\n13.5068984\n2.3493417\n6.492563\n11.2623955\n-7.2021843\n4.4342641\n4.404997\n-3.6185602\n4.560521\n-5.6883207\n0.3942039\n-3.103478\n-0.2551315\n-7.4488074\n1.048294\n-1.7003834\n-14.8334574\n-7.3806372\n21.785405\n-6.706319\n-14.7996623\n-14.8057805\n4.2287031\n-6.268469\n-9.6626014\n3.6745613\n-25.201119\n5.2614205\n-0.3076832\n13.4564618\n-0.2724789\n-0.7343194\n3.7806412\n-4.0303417\n6.906578\n-16.8387944\n-13.0173042\n2.5265882\n-2.1689518\n3.9887857\n-7.5873560\n13.2403690\n-6.8157585\n-5.555756\n-9.843402\n6.9697738\n1.149107\n-0.7261884\n13.9617517\n-1.0806520\n-0.7193217\n-4.611446\n0.6665858\n-6.824130\n-10.490425\n-0.4190986\n6.6339878\n7.8565984\n0.0106521\n2.7727858\n0.2248497\n-3.6600560\n-0.4946363\n-13.6349074\n-11.496180\n-10.7151125\n-9.0431384\n8.1208173\n-2.457928\n-6.4496081\n-3.1466380\n0.1797440\n-10.4487262\n-3.0656779\n1.3713914\n7.7817654\n2.2848116\n3.4939900\n-4.0449003\n-0.4560466\n4.2031594\n-6.6603234\n-6.8989429\n0.6610100\n-2.7081900\n1.3808937\n-8.3187305\n4.1863844\n-8.4271535\n9.202663\n13.9739600\n-5.6569982\n8.1442349\n-0.4116793\n-2.4405695\n14.0171390\n1.2796313\n-4.2112780\n-5.6927812\n-1.065173\n-14.3548774\n-0.6837453\n-8.4956908\n4.7990248\n-13.9831855\n6.1447592\n-6.668499\n-1.9669608\n8.8483279\n-9.9708037\n-1.5526994\n-2.3869305\n2.7068987\n-5.8647411\n3.9524734\n4.062503\n-3.9309064\n-7.8845501\n7.1579054\n0.1585556\n-5.5554894\n9.0703253\n-4.0248275\n-6.9605902\n-9.8685993\n-0.3081633\n0.4980695\n3.7748428\n1.9719101\n5.0909790\n0.1292172\n-3.226018\n9.6870012\n-1.3666593\n2.211289\n-0.8944688\n-0.8549064\n3.0816563\n4.9719423\n4.114711\n-14.281108\n-7.9345617\n-1.6437555\n-7.9618872\n-8.0368606\n3.4011152\n-2.7222087\n2.8836967\n0.2547526\n-2.3780604\n-1.1224880\n-11.5821939\n4.0609673\n-8.6903496\n-2.8427766\n-2.4340404\n4.2274566\n-8.3796828\n5.6594842\n2.7963812\n0.2636938\n-9.7927961\n9.5555720\n9.4439495\n1.522367\n-6.7097545\n-0.3524096\n-0.8597147\n6.5798253\n2.2472588\n8.0601951\n-5.9025702\n-2.5059219\n2.0198455\n-12.8518396\n2.291976\n1.1620025\n6.2732416\n2.8002452\n-1.6263764\n2.6704350\n-10.1605403\n-2.7630097\n12.7147504\n5.715203\n1.8822445\n7.2482598\n2.3230484\n-5.3064914\n8.0907059\n3.4526805\n-8.4392930\n4.3845380\n-2.7214840\n13.0781151\n4.9657986\n4.3581377\n-7.6347574\n4.2915669\n1.3878334\n3.9236583\n2.0851763\n-9.9306471\n-2.457062\n11.5000748\n6.9552347\n-0.2530878\n10.3370600\n-5.9768839\n-8.2311500\n-3.3333605\n7.1425790\n2.4735721\n0.0097459\n2.8663022\n-3.1668199\n1.2567023\n-0.6361538\n-6.8877328\n11.1474688\n-3.9729353\n-1.8673939\n-1.0748327\n6.478546\n12.9997228\n10.3136090\n-5.5700471\n-9.9853885\n9.1411329\n0.2742870\n-1.6717776\n\n\nOak\n80.828697\n-43.005475\n26.7473579\n33.209697\n10.2143428\n-88.688440\n-26.7698885\n-36.4809668\n-32.689389\n-28.1914601\n5.8729877\n-20.1327581\n-26.6134246\n16.4039523\n15.2154146\n13.858923\n-16.996326\n-0.0919365\n10.3958523\n19.4280717\n-48.1713132\n-10.4005373\n16.1410747\n-2.868989\n-28.312146\n2.4767914\n-8.9061867\n-5.9204197\n4.4135992\n-17.6132436\n-24.8680966\n11.0406067\n-22.7716040\n-1.1670161\n12.7757840\n5.076753\n4.1300678\n-5.0944729\n47.6587007\n6.015303\n-19.0976943\n1.9509617\n-4.9462611\n4.171989\n3.3572475\n20.1556112\n10.8333810\n-7.790407\n2.5772871\n-20.7119218\n5.0779574\n-19.1404229\n22.226832\n-22.0977128\n-8.9922374\n18.7208032\n-4.4950088\n-15.808647\n22.7718473\n-7.4013208\n10.9007914\n13.8910809\n24.5008176\n0.2974784\n-6.888492\n-4.040368\n17.0215756\n-21.543589\n6.4547801\n1.7278160\n6.0658790\n11.427325\n13.5814514\n-7.5986843\n-0.4017347\n12.109434\n-11.1031793\n3.234420\n11.4574224\n-10.0867276\n-10.302973\n-18.1461301\n-5.2251757\n18.880036\n-8.3391278\n-6.6289327\n-0.6804034\n10.682985\n-9.124614\n-9.7054687\n-4.9089148\n-16.4950604\n1.187639\n-4.1632332\n-2.1032415\n-18.275406\n2.3416492\n-7.5896618\n24.0136987\n-3.2429851\n6.9687686\n14.4519253\n-0.2791838\n7.115823\n-11.9696475\n-1.5205516\n-8.2146305\n7.9105197\n14.0550096\n-2.9324309\n22.6483777\n10.3098669\n-2.724509\n2.098667\n4.5523924\n-10.880036\n0.5708622\n9.9182861\n-9.6121785\n-8.8592534\n0.124189\n16.0831198\n-7.280793\n-7.470394\n15.9237948\n7.6834953\n12.3278255\n-3.5486672\n-11.9662029\n-4.9252451\n-2.1536257\n1.8985313\n-7.5979244\n-7.383110\n-19.0073754\n1.9448647\n2.0579126\n-17.412922\n-11.1693916\n-14.6267846\n-4.1773881\n-12.7152303\n8.6602983\n8.8907915\n3.0503019\n-0.0037553\n-9.8574560\n4.4583720\n-3.1306854\n-9.1026034\n-6.1067185\n-10.6015973\n-5.7251255\n-10.5043480\n0.8605427\n-12.5711306\n3.9891822\n-14.5223856\n2.131734\n8.6427481\n-4.4826514\n3.5473972\n-1.4149334\n-9.7085635\n16.2090817\n-0.1036665\n-10.3853703\n-6.5985241\n-5.582096\n-5.1657445\n-8.3063971\n-18.5980656\n12.2062214\n-6.8818838\n5.1119193\n7.313527\n-6.2993178\n9.0798866\n-13.2630747\n4.6537671\n3.7365609\n-1.4956865\n-12.0706154\n0.9103531\n-1.686001\n-2.8324344\n-6.7086971\n12.9250006\n11.1620524\n-9.8040349\n12.2273438\n6.4498272\n-6.0560375\n-8.9560154\n11.3814081\n17.0410777\n-0.3908397\n12.1629579\n10.2282912\n13.1237576\n8.497552\n7.8198934\n-11.0312620\n-1.332901\n-13.9372947\n6.5161129\n1.5215513\n-5.0862092\n-5.332131\n-8.985470\n-3.0928814\n-11.8218450\n-8.3140759\n-9.1598165\n0.7849052\n-1.6985827\n4.3136719\n-1.6546882\n1.9974886\n-9.4080705\n-19.5365685\n-0.6712322\n-17.2954168\n0.4360413\n1.9430895\n16.7224478\n-12.1301834\n13.3619001\n-9.1417729\n-4.3495371\n-20.9839851\n3.0265527\n21.3448865\n20.042162\n-3.9763363\n-7.7227427\n-4.3354595\n1.5552491\n3.5565718\n7.3571507\n4.9196287\n-6.7190032\n15.4551298\n-18.3392176\n-7.329798\n19.9773636\n3.9836644\n-9.9018899\n-2.4813813\n0.8674818\n-10.0296373\n5.1682120\n14.7502598\n-2.908291\n3.3245893\n-1.7032331\n13.9013417\n3.4384251\n7.8857765\n-8.5167114\n-18.8396904\n5.5098333\n-2.5232538\n12.7119670\n-2.0198069\n11.5662823\n-8.4465002\n12.6920782\n8.5115727\n5.0622426\n1.0143331\n-10.7399950\n-5.023741\n13.8779947\n7.6182755\n-14.4531521\n5.6044493\n-2.8049467\n-11.8018653\n4.2484054\n-3.9074604\n-7.9172866\n-3.5020140\n8.1817810\n3.6104357\n0.2792197\n2.6597445\n-10.6930430\n19.5662373\n-11.0043755\n6.7610438\n1.8715504\n5.841162\n12.3294527\n-0.5637173\n1.4770438\n-13.3556489\n5.2794818\n3.9390143\n-5.9196026\n\n\nRose\n169.793825\n-98.482982\n7.9087297\n-33.526768\n-15.3870192\n-12.804220\n-14.4783884\n-15.5809105\n-25.410565\n-19.4996026\n-27.4623533\n-5.6971808\n-8.5937710\n28.3714423\n29.5112569\n-7.504715\n-12.272873\n-14.9184604\n48.8658440\n1.8716615\n-14.6257486\n-29.9462025\n-2.6105517\n-18.737203\n5.911870\n24.5004368\n3.5533200\n27.8024060\n-2.2104456\n-27.5325946\n5.3811664\n14.3092581\n-4.2505634\n6.2236096\n30.3394288\n8.797142\n4.6484107\n4.6574179\n10.1908145\n-5.267450\n-19.6150691\n-23.6457392\n29.0449034\n-28.486535\n7.6362859\n-0.1406000\n-5.7477637\n-2.528637\n23.5909689\n-14.3507716\n18.2506426\n-9.4865387\n12.052128\n-15.1619448\n14.9628981\n9.3703101\n-11.4976291\n3.435361\n7.0759125\n-1.4774759\n-0.0685737\n-16.2138107\n-11.4504387\n-11.0340333\n-8.777626\n10.978064\n31.6275187\n-17.394848\n1.5170053\n-12.4252284\n3.1297145\n9.534743\n25.7245968\n1.5104692\n-12.8455362\n21.783901\n-33.4809698\n22.691487\n15.3734504\n7.3714504\n1.814490\n-0.1285942\n8.4247191\n19.642460\n-4.7169768\n-0.1141606\n7.3560021\n14.891125\n6.643697\n-12.9119349\n3.8057530\n11.4236129\n-7.610132\n22.4005712\n-11.1136624\n-14.408629\n-3.2702556\n10.5295792\n4.7816807\n13.0524770\n0.2997899\n6.3874410\n10.2777419\n14.919367\n15.9114001\n2.6025231\n11.2475550\n10.1245306\n-4.3021472\n-14.1799770\n29.2487634\n15.3426564\n7.777509\n5.093669\n-26.2353297\n7.290247\n-1.3080207\n14.7794721\n-7.3412600\n1.5924531\n10.437176\n-7.2708055\n3.825308\n-4.408311\n22.2402751\n-9.7766258\n12.9946723\n5.5923931\n-6.1653314\n-14.2313198\n19.3671305\n19.4241150\n4.6512561\n-5.752324\n-3.0811970\n0.7296660\n-11.6086048\n3.939982\n-9.5809516\n-1.1101148\n11.4602614\n7.3187249\n-0.9660889\n-11.2094730\n5.8177400\n-1.7164509\n12.0703200\n2.6721368\n-13.8695715\n-8.8573739\n-2.0379172\n-0.0437098\n-6.8562632\n-10.4266609\n1.9711819\n-17.1435723\n-21.0114836\n-3.0988737\n-16.277689\n5.6989771\n17.1905151\n7.3003084\n-13.0027768\n-6.8507316\n13.4156697\n2.5991769\n6.6928880\n0.9784581\n-15.818938\n5.1605969\n-7.9594016\n8.7603460\n-9.5441003\n6.5391132\n3.2680039\n7.200926\n8.5513808\n-12.7086585\n7.3750603\n-14.1497674\n6.1342401\n-0.4422350\n2.8880373\n-13.0100570\n-14.632730\n15.6580307\n-19.2117003\n-6.3291255\n-9.8000420\n1.1788047\n2.0219291\n-6.1350549\n-7.5339776\n4.4261359\n12.7294918\n14.7959570\n14.2741003\n1.3313340\n-3.6815732\n13.1639548\n6.393202\n3.3457355\n-0.3390432\n2.370055\n-26.2276702\n1.8357318\n0.4190367\n2.3285473\n1.788032\n-1.154643\n10.6305695\n-6.9046237\n-2.4252594\n8.7147069\n-10.0202057\n12.2915791\n-4.8637219\n-8.3308660\n14.7143327\n-5.9119868\n-3.5836339\n-5.9821050\n3.3685229\n-0.5273490\n7.4853536\n0.8879983\n-1.6465533\n6.7061540\n2.4660995\n-17.8014362\n3.0484707\n-8.8926470\n-5.8612257\n-10.381385\n1.3308735\n-1.7600527\n-6.8335712\n2.2206121\n-5.4044090\n-0.9646472\n-13.1956251\n-0.5255635\n6.2242313\n-5.5270536\n-3.950202\n10.8664688\n-13.9663561\n0.0951356\n-1.8206412\n-8.7879439\n-22.3673848\n1.2170017\n-14.5392483\n5.334749\n-0.3670962\n-1.2099986\n-6.3474973\n22.8696146\n2.1016206\n-12.2466947\n-3.1836825\n-13.9297601\n-19.3750175\n-5.5130141\n7.4440775\n4.6176661\n9.0544032\n17.0450127\n1.3686618\n3.7586278\n-7.5690573\n-13.8882015\n-2.576893\n11.9090520\n11.9572000\n-1.8513407\n-1.2740561\n2.4682909\n3.4724126\n-7.5428997\n-9.2694222\n-6.5640788\n-9.9982253\n0.4708680\n-2.7110109\n2.2204115\n-10.2996062\n-1.0598588\n11.1593595\n11.8381842\n5.0702978\n13.6659472\n-13.532357\n12.3769665\n-3.2224983\n-4.2846200\n7.7764724\n-5.4564864\n-1.6851974\n-1.2794527\n\n\nDaisy\n24.100221\n-16.222531\n-9.3980192\n3.055284\n4.6509376\n-2.064403\n3.9375922\n-7.1785296\n-3.427622\n-2.5476127\n-3.1881560\n-3.0890079\n2.8138486\n8.4137179\n8.8892470\n3.930178\n4.484171\n-5.4619650\n3.4778137\n-0.6353814\n-15.1974376\n-1.0285566\n-1.0310829\n-5.313575\n6.470959\n1.2715653\n-2.6302464\n-0.2207977\n1.3919182\n-2.1731995\n-0.9797795\n-0.1544892\n1.0353388\n4.5132537\n-0.5794254\n-3.784247\n7.4763496\n1.1541796\n1.8444969\n4.896767\n0.4737606\n0.9662965\n-0.8882757\n2.829369\n2.2048359\n5.3551305\n1.7500490\n-2.990434\n7.6930077\n-3.2830856\n1.9311926\n-1.5081203\n6.648616\n-8.3339914\n2.1998600\n9.1336778\n2.9894294\n-2.443459\n2.7925253\n1.1114895\n3.0950249\n-6.0302616\n-1.7250197\n2.7722301\n-2.009590\n-2.873179\n5.3776300\n-3.190599\n-1.2582892\n-1.1636990\n-0.5336351\n4.735501\n3.0734966\n1.4201545\n-7.2246440\n1.439099\n-4.7856932\n2.419344\n4.3053005\n-0.6889198\n2.566885\n0.1682647\n1.8888079\n3.535584\n-2.7770019\n0.7089246\n4.0487488\n2.837982\n2.183721\n-0.2072544\n-2.7926343\n1.6429740\n-4.428295\n7.5298984\n0.7520422\n-3.652488\n2.1818781\n4.1139360\n2.6437537\n2.8562345\n1.6740233\n-5.6797268\n1.4680887\n5.022289\n0.0013254\n0.9708033\n-0.0881402\n-0.3067673\n-1.4011147\n-0.0301427\n1.7268919\n0.2298842\n1.751551\n2.619544\n0.1792596\n2.262902\n0.5520544\n2.2583938\n-3.4603345\n-1.4617958\n1.765299\n-3.4695657\n-1.789427\n-1.291499\n-0.9980570\n-0.8156097\n2.2177832\n-1.2410956\n0.1199769\n1.2092455\n0.5854205\n0.6172075\n1.0071439\n-1.830293\n0.6156090\n-1.5444117\n0.3443324\n2.931205\n-0.5220600\n-0.3023792\n-2.6408510\n0.9592554\n-0.6381676\n-2.1317337\n1.6269074\n0.7034337\n0.7778252\n-0.6087897\n1.0196182\n-0.5708336\n0.3365156\n-0.4899099\n0.1691228\n-2.2193951\n0.6475840\n-1.3154586\n-1.5273491\n1.9772685\n-1.502573\n0.5563318\n-0.9540241\n-0.9424389\n-0.1309414\n0.5641224\n4.9536121\n-1.6481216\n0.0152478\n0.0713423\n-2.476908\n-1.2880931\n-2.2792060\n-1.5237614\n1.8059194\n-1.0313243\n-1.8179311\n1.719301\n1.4999382\n-0.8877665\n-1.1326820\n-2.6514381\n-2.1634048\n0.2419982\n4.1322335\n-1.7894117\n-1.879960\n-0.9275659\n-2.8482647\n0.0662491\n-1.2305098\n2.5804608\n0.1833774\n0.1125451\n-2.4019557\n1.3005379\n-1.7227692\n0.2017959\n2.4216225\n2.5192043\n-0.0032864\n-0.4333918\n0.878485\n-0.1422516\n1.5043564\n1.427500\n-1.4644374\n1.8093708\n-1.3059904\n1.9247803\n2.011994\n-0.836282\n-0.0305426\n2.6002665\n-0.4163487\n-1.1505782\n0.5197413\n-0.3288488\n2.0073724\n-3.3995149\n0.3591165\n-0.8033242\n2.3073012\n-2.4522559\n1.0381770\n0.5889167\n-1.2192055\n-2.2046264\n-0.0626758\n-2.2200307\n1.7759482\n-1.4519378\n1.4079847\n4.1210806\n-1.1196130\n-1.105506\n1.3710091\n-0.5470366\n-0.4197090\n-1.8541823\n0.8415489\n-3.7318444\n2.5166220\n3.6061232\n4.2544035\n1.8787912\n2.367761\n-0.3487660\n0.2295378\n-0.2703449\n-0.4056041\n-0.8336008\n-0.9515302\n-0.8328531\n-0.2374453\n2.148024\n-1.5675426\n1.2949289\n1.6991554\n2.7862258\n-0.6770487\n0.0717748\n2.0054809\n-1.4472863\n0.4194559\n-1.0473439\n1.0471419\n-0.8462998\n-0.7288768\n2.1141282\n0.2206433\n0.9258962\n-3.9254093\n-1.4988314\n1.123979\n-0.3680056\n2.0287794\n-4.2838177\n0.5503843\n-0.7195955\n1.3857401\n-0.7845520\n-1.6141708\n0.4472853\n-0.7486416\n0.7764512\n-0.1349593\n1.3673770\n-1.5620602\n-1.1320222\n-1.6498093\n2.6569546\n-0.3293581\n-0.7622562\n1.200270\n-0.4821121\n-1.1639723\n-0.9182759\n-0.4077662\n-1.6968441\n-2.0445169\n0.4338182\n\n\nRobin\n87.209927\n-39.421553\n-13.2598933\n-19.255405\n12.6966848\n13.649532\n1.5221174\n-14.5741633\n-12.210516\n-0.5938838\n-18.5349484\n2.2500446\n-21.2421595\n24.6352050\n13.8045869\n32.620392\n17.944238\n-36.4383255\n7.9661383\n-5.6953062\n-11.4241949\n11.2369978\n0.6669709\n-8.619645\n13.561825\n6.9819243\n18.0589694\n2.5570000\n-0.6621710\n6.6970804\n-8.7458538\n-0.9545062\n0.0350254\n10.0853483\n-12.8431858\n-1.320653\n2.0786364\n-1.4873606\n8.4382774\n14.977747\n-12.4116775\n10.7415860\n-4.5315644\n13.927279\n-0.7128135\n9.6521416\n-15.8944102\n-10.277005\n1.9953862\n-4.9111056\n13.1432152\n5.2965675\n6.968865\n-10.1453840\n6.4075715\n23.5236111\n8.0962490\n1.989658\n11.7967562\n7.6318221\n-3.2844709\n4.1211137\n-14.3982188\n-10.9720064\n-6.372903\n4.284889\n-2.0337362\n-1.238032\n-2.4289796\n-5.6503061\n-5.9904350\n5.792373\n6.6458773\n1.3018887\n10.0427329\n5.481997\n2.8851058\n-5.935497\n-7.9150823\n3.4489882\n9.693554\n-10.6514120\n3.4651870\n-16.072585\n-4.8761024\n0.4401328\n2.3540835\n2.018782\n-2.763589\n-11.4212434\n-2.3363155\n-7.5630419\n-8.602159\n-4.2029689\n7.3109632\n-11.829872\n-0.2461956\n-2.6658087\n3.1757153\n-5.1620742\n14.5710273\n-10.9124843\n-1.3028343\n1.692319\n-0.7856966\n-18.7663053\n1.2938884\n7.7773864\n-7.3547104\n-3.8452535\n3.5982890\n-2.2197566\n-2.964379\n-1.904294\n-2.5573230\n3.325042\n6.1901865\n2.2946508\n-8.6880534\n-12.0032768\n10.386573\n-12.8842522\n-6.411623\n-7.892069\n-2.3083960\n-5.8206563\n-0.6115528\n-4.2427814\n2.2741640\n6.5603657\n1.4889978\n-7.4718740\n-4.2095390\n1.977323\n8.5250592\n-3.7560782\n1.0639200\n7.170260\n-0.5884419\n-4.8995161\n-0.3838779\n5.7217020\n2.7197784\n-0.1464315\n4.8567756\n-3.7900789\n-12.7395744\n-6.5280637\n4.6569942\n-5.3193175\n3.3383670\n-6.3656664\n6.2351808\n-7.2392041\n2.8795302\n-13.1776304\n-3.7389721\n3.4776007\n4.965016\n-0.1876198\n-4.3859840\n4.6497057\n-2.7225727\n0.0782279\n5.5174978\n-1.8113893\n-3.6797836\n2.8278217\n3.474013\n6.2143381\n4.6871694\n-3.9044960\n-3.4702816\n1.1007603\n-6.5268288\n2.856452\n-1.4037336\n-4.9919524\n-3.4320218\n-1.6192452\n-5.3155054\n-3.1168445\n-2.8154924\n-4.2488728\n1.439867\n3.6809335\n-3.6949757\n3.2758540\n0.3023899\n2.8939321\n5.8187221\n-0.6670275\n3.0347652\n-0.0887428\n-1.5831982\n2.3446869\n2.5552661\n7.5736351\n-0.7490191\n4.2969524\n-1.069244\n8.1020301\n1.3164267\n-4.474278\n-0.9060467\n3.3468269\n-5.6638199\n7.6678717\n2.111567\n-5.039989\n0.6724506\n5.3588127\n-2.4365683\n3.9894087\n1.8377403\n-1.5928317\n0.2032277\n-0.2400587\n-7.0711414\n-3.5950192\n0.8553665\n5.4162505\n-1.4317203\n3.0724923\n-1.0762675\n2.2648095\n2.4612144\n-1.0634334\n-0.5904171\n5.8193823\n0.7524095\n3.8338898\n6.2357706\n-2.316103\n-1.3868023\n-4.9612493\n-0.7907146\n2.9447086\n-0.2930757\n-0.4749651\n2.5210642\n6.3847007\n11.3807706\n10.8388429\n-4.556101\n-0.2353506\n1.0616370\n1.8722904\n4.2907091\n-2.4827531\n-5.7486118\n3.2385323\n3.4846426\n1.298343\n2.8100805\n4.2519181\n-1.8560000\n4.9559298\n3.5121412\n-3.1172622\n-4.8691203\n-7.0946331\n-7.5602092\n-3.0801421\n-5.8546164\n-2.2928990\n-1.4201750\n-2.1490260\n-2.0374218\n5.2978722\n4.1143296\n2.4964750\n-1.707420\n1.1130026\n-1.4592996\n-3.5824727\n-5.0246880\n-1.5280913\n5.3745849\n-3.4539094\n-4.4424357\n-7.0144081\n2.4888395\n-3.4601753\n5.7682678\n-0.5639159\n1.7719846\n1.8693498\n0.3915249\n4.3354162\n0.6329579\n-4.7893166\n2.355965\n-2.9944984\n0.9746733\n2.1831488\n1.8503339\n5.7226143\n-3.5455370\n-8.8605653\n\n\nCanary\n28.017936\n-9.711131\n1.8152580\n4.152567\n4.6799648\n-18.097735\n-9.7935685\n-5.7242762\n-9.117048\n-6.4045831\n-7.1737961\n3.7290718\n-4.2616369\n-1.8423486\n9.0168055\n2.552417\n10.890162\n2.2268742\n3.5813905\n-10.7686105\n0.9729619\n3.9552726\n9.7763952\n4.992721\n9.647722\n3.4270408\n4.9922738\n-3.0303735\n0.5653330\n4.8307165\n2.0812984\n-1.3337595\n5.0110176\n3.8795221\n1.9729774\n3.114877\n5.6659330\n-0.3845149\n-6.0508167\n-2.505897\n3.2621366\n-8.5092034\n0.2695424\n8.999274\n6.0097419\n-8.0452811\n-6.2318201\n-2.956672\n-8.0912288\n10.2832121\n-8.5757066\n2.0593362\n5.641815\n-1.6180890\n8.4030088\n7.7012418\n12.3991068\n1.103800\n-2.8683962\n0.4302879\n-1.0158241\n-3.5594815\n-4.2587670\n-6.4801574\n5.638990\n6.314367\n-0.0669196\n-1.348402\n7.5563033\n-10.8832919\n3.7249564\n5.189974\n-11.1306659\n8.7928820\n2.8876223\n4.541898\n-2.3243570\n-3.231420\n-0.4968872\n3.5529275\n2.423911\n-3.5899662\n-3.6125600\n-8.406667\n1.0438244\n-3.3558637\n-2.0500128\n-3.263322\n-1.852730\n9.0978358\n8.7565410\n-1.1946464\n4.118265\n1.7952097\n-4.1285179\n-1.918781\n0.3452547\n-0.7890635\n0.3350979\n1.2202142\n4.4832191\n-6.2956360\n1.7460564\n6.924928\n4.3928950\n-1.8558423\n4.9384971\n-3.5288743\n3.8039616\n-8.2919066\n3.7334474\n2.7497758\n3.204420\n2.333834\n-0.1390349\n5.446273\n-0.7796235\n1.3121761\n2.4666003\n1.0968973\n-1.472856\n-0.4516848\n-1.731479\n1.297061\n7.8931945\n-7.8552598\n-0.9213307\n-5.1829622\n1.4432436\n-3.8580089\n4.6614600\n-0.4314485\n2.3888149\n-6.777383\n-0.9086408\n-0.2654383\n0.5617019\n4.655535\n0.0676956\n5.3238419\n-1.3010668\n-1.0045344\n2.7937679\n-1.5368508\n-0.3815586\n-0.8088542\n-2.0564964\n-0.5237565\n3.4549695\n4.1438910\n2.4115136\n3.6730358\n2.9372325\n0.8475813\n-0.5392527\n-5.5385215\n-0.2500064\n7.1198768\n3.138690\n5.7205305\n-3.2763185\n-1.6993905\n0.0370527\n4.1311516\n5.1184535\n-2.2756837\n0.6948147\n0.1812992\n6.260929\n1.2352646\n2.8668889\n2.6751753\n1.6192647\n2.6606578\n0.5162005\n3.506697\n-0.3387404\n4.1307986\n-3.5253067\n2.5762092\n0.8551627\n2.6942940\n0.2475203\n-3.6053549\n3.826843\n-1.4177310\n1.6054711\n-1.7910695\n-6.5983420\n-0.3165268\n-2.0444003\n3.5619750\n4.1132019\n-1.5378804\n-6.9456183\n7.5385905\n8.1937005\n4.2524765\n-1.7145109\n-1.0758415\n-1.455281\n6.5756390\n5.8934214\n-5.505482\n1.5139850\n4.7889436\n1.2251997\n3.1443918\n1.363844\n5.646660\n-4.3012001\n8.7334368\n-1.7210364\n-0.5361291\n1.1349723\n-5.3463749\n-4.1545847\n-4.0710196\n1.3033721\n-0.4549559\n2.4057862\n4.3525330\n0.9562371\n1.3003306\n-0.9387649\n-1.5592181\n1.0699544\n-0.3185847\n2.1227863\n2.1451704\n1.2627861\n-1.8852066\n-0.8324722\n4.130666\n0.0788831\n-2.3861298\n3.3290750\n2.4536828\n-0.7881028\n6.8388531\n2.8891909\n2.3766424\n3.7569247\n-2.4993799\n1.846241\n-1.8720644\n3.6760059\n-0.0793287\n0.9553124\n3.5302088\n2.8229766\n1.9054433\n2.0903841\n2.126011\n-1.8324137\n-2.5411060\n2.2986315\n4.5185081\n2.6298880\n-1.7034878\n0.1103810\n2.1441119\n-2.3426218\n-1.4942959\n6.0660064\n0.2770576\n-4.2644216\n4.4382470\n-4.1272113\n1.4978426\n-4.3379426\n-2.3860744\n-2.743482\n8.6801296\n0.4912353\n-3.4577271\n-4.6427416\n-6.0232588\n-1.2000095\n1.2915031\n1.7547299\n0.5286464\n-3.7662894\n4.7920103\n1.9458703\n0.2371975\n-1.0947628\n-0.3974943\n-2.5085937\n-0.5398389\n0.2735771\n1.5247094\n-3.624901\n2.3300559\n-2.0270701\n1.2641799\n-0.9181785\n9.4161516\n4.1023113\n-1.0459135\n\n\nSunfish\n2.671976\n-1.634745\n1.0092547\n1.971743\n-0.0659806\n-4.485483\n-0.6157394\n-0.4446398\n-2.135539\n-0.1855337\n-0.3653675\n-0.9757169\n0.9833536\n-0.4373506\n-0.6857746\n1.395239\n3.526766\n0.5293000\n0.4209848\n-3.4104846\n-3.9299972\n-0.4856075\n1.1505664\n2.247154\n1.291273\n0.4333016\n0.5757962\n0.7421109\n1.3637568\n0.4675282\n-1.1996607\n-1.1199203\n-0.5715238\n0.7483034\n-0.2462562\n1.295735\n-2.1481787\n2.2357764\n-0.1000319\n-1.334179\n-0.4678468\n0.6883993\n0.7824726\n1.986898\n0.1823848\n0.5900189\n-0.6196374\n-1.197890\n-0.6397997\n1.8739895\n-0.6986206\n0.3264926\n-1.106825\n-0.7986667\n-0.7942869\n0.3534946\n1.5704717\n1.099842\n-0.4712961\n-1.9296982\n-1.2794992\n-0.6377849\n-0.4841883\n-0.7619484\n-1.848736\n1.861377\n-3.0369215\n1.040914\n-0.8948211\n0.5964522\n-2.1729557\n-2.847560\n0.3661112\n0.7198424\n-0.2592527\n-1.005021\n0.9121147\n-1.991298\n-4.0090396\n0.9573294\n0.837238\n1.5041342\n0.5979597\n-1.329158\n0.1356189\n0.1137635\n0.5712501\n1.144023\n-2.901016\n1.7090371\n0.1723399\n0.7930372\n-1.302795\n-0.1576198\n0.6288434\n1.809818\n0.9966588\n0.1851948\n-0.9818111\n-0.5018535\n0.6628568\n0.2434079\n0.3056738\n-1.179871\n1.0023197\n1.9586813\n-0.2391551\n-0.5478282\n0.4052252\n0.1050347\n0.0608304\n0.6921589\n0.090562\n1.737157\n-0.1224932\n1.382547\n-0.0669635\n0.6275334\n1.1357830\n0.7212117\n1.332176\n-0.1708254\n1.631695\n1.046881\n2.6477299\n-0.7323800\n0.2450768\n1.0407605\n-0.0638638\n0.0362075\n-0.7602249\n-0.4514443\n1.0970385\n-2.058717\n-0.4772066\n0.6891971\n-0.5252515\n1.320807\n2.0855522\n-0.5308727\n-0.5414442\n0.8132779\n0.8060741\n-0.7820840\n-0.0974855\n-1.1516975\n-0.4544318\n-0.1437671\n1.0149376\n-0.7442971\n0.2365992\n0.5018176\n-0.0623840\n0.7175603\n-1.4370608\n2.1816251\n-1.1740033\n-0.1587521\n-1.308936\n1.0963224\n0.9416279\n0.6446517\n-0.7079844\n-1.3376836\n0.8534419\n-0.1928021\n-0.4729082\n-0.6972626\n-0.595974\n0.9460619\n-1.1913831\n0.1201412\n0.0858613\n-0.0684709\n0.8437920\n-0.956026\n0.9757848\n-0.0287358\n0.2485583\n0.2431551\n-1.2416189\n0.0169046\n-0.5596322\n0.2480171\n-0.637270\n-0.4266425\n-0.8717179\n2.0318199\n0.2985858\n-0.8279377\n0.2846029\n0.8176092\n0.2886955\n-1.0247478\n1.3453494\n-1.0061067\n-0.0779308\n-0.2040882\n-0.9363782\n0.6747263\n1.141463\n-1.2624069\n-0.8103713\n1.212891\n0.0998399\n0.9855938\n0.7333348\n-0.8901834\n1.282947\n-2.092871\n-0.3857082\n-0.4879458\n0.3354572\n0.5785333\n-0.8967905\n1.6569907\n-0.3487997\n-0.9604216\n0.8301921\n0.1801073\n-2.2939194\n-1.8618764\n-0.5685490\n-1.1015050\n0.8148482\n0.4927997\n-1.7475145\n2.6695286\n-1.1713861\n-1.5447938\n-1.7871431\n0.0911204\n-0.0722668\n-1.870259\n-0.3034667\n0.4476403\n-2.0470996\n0.2271746\n-1.2681026\n-0.3287547\n-0.2580326\n-0.7302849\n-0.7232476\n-0.1447716\n1.142121\n0.0259262\n0.9386625\n0.0660591\n2.2029548\n0.5374430\n0.2409948\n0.9285480\n-0.3375535\n-1.890040\n0.5197283\n0.5158098\n0.5181227\n0.6476375\n1.0858086\n0.2762813\n-0.1479446\n0.2331174\n-0.1243531\n0.7791777\n-0.6929186\n0.3517946\n1.4635512\n0.2301916\n1.3337702\n0.2979985\n-0.4120448\n-0.3105052\n1.233750\n-1.7594159\n0.6044447\n0.8377204\n-1.4883090\n-1.3467319\n0.2149829\n0.6559219\n-0.8600177\n0.6556712\n-0.1650155\n1.9970446\n-0.4196055\n0.4368345\n-1.2926978\n-0.8387044\n-0.4728054\n0.0600773\n-0.8078840\n-0.8101973\n1.032789\n0.0913317\n-0.2479691\n1.2400970\n-0.2620524\n-0.4653997\n-0.6517328\n0.2916859\n\n\nSalmon\n45.366639\n-14.246932\n0.1114514\n19.613487\n2.7339021\n-37.359271\n-15.1653864\n-15.6177393\n-28.997250\n-3.1877566\n-6.6190532\n-15.9110693\n-0.7991651\n-1.2207791\n25.7337867\n18.073711\n18.250084\n10.8358750\n23.4028315\n-6.8574182\n-28.0333926\n-6.6615900\n11.7924924\n-2.439194\n15.361486\n2.6333636\n-18.3236103\n20.7517419\n-38.2821673\n-21.7261770\n-8.9726795\n-3.8014499\n23.8414364\n-10.6582933\n-32.0685345\n10.280295\n0.1439898\n-5.1115406\n7.0460971\n-12.619112\n-15.9090216\n5.6932943\n0.0775648\n6.647813\n-15.5498536\n-11.8216543\n-9.0334169\n-1.171663\n3.2620234\n11.2080167\n-0.9551670\n2.5643337\n-17.841022\n-7.5145103\n-21.0402972\n0.2247946\n-2.1811709\n6.188044\n2.9438778\n-5.0685389\n-15.7523157\n16.3098186\n11.8173905\n-9.4073700\n-6.414177\n10.251040\n-12.0886915\n3.541542\n-10.0117884\n1.7279323\n-13.3995545\n-24.430764\n14.0453707\n-10.6955862\n8.0365074\n-1.883903\n12.6877520\n-6.894571\n-22.7189243\n4.7472505\n4.395364\n-2.1284084\n5.7227501\n-2.650241\n9.1897555\n2.1506669\n9.4490531\n2.812082\n-20.816413\n8.5460037\n0.2949648\n0.7946690\n-10.656108\n9.9514356\n-12.6217186\n16.278604\n0.8662642\n2.8323808\n5.5195939\n0.5749495\n-13.1863747\n2.6751923\n-13.9624171\n-11.584639\n-15.9321708\n12.5074162\n-3.9394292\n-3.4320179\n3.9509412\n8.7203613\n-0.9916812\n4.4278251\n-5.484492\n5.542930\n5.9595888\n-1.846622\n-2.2214505\n-9.5353523\n0.6262246\n-2.1146753\n6.978190\n2.4015731\n-2.978400\n23.274266\n10.1745679\n-12.6547850\n8.5752611\n20.8520893\n-4.7983872\n3.8165092\n1.6916782\n-4.5252281\n0.3917101\n-9.628118\n-10.0832436\n16.4615853\n-2.9438570\n4.273337\n9.3487661\n-9.0246737\n-8.1324818\n9.3575397\n-5.6498191\n3.4268754\n-7.4079411\n-5.2582906\n-3.8441174\n5.5187426\n2.6453734\n-6.7113518\n6.7072824\n-7.0429745\n3.1472481\n-0.6532618\n-5.3207453\n0.4442541\n-4.4238417\n-8.6722421\n-20.286674\n16.6801413\n6.2967667\n12.9163128\n2.5236117\n-16.1339405\n3.9601234\n7.9584559\n-3.5261174\n-4.7358231\n-6.021531\n1.3814217\n-9.8024166\n5.5291360\n5.4079389\n-6.4259406\n7.3904302\n-6.307354\n1.5624471\n9.8609787\n-3.2719547\n-3.8165110\n-6.0230060\n2.2525619\n-9.1581446\n0.5150933\n-3.109665\n-12.3073288\n0.1718954\n8.2760865\n1.0782718\n-4.5277518\n3.5575192\n8.7853295\n-7.3308001\n-10.3578341\n3.8029453\n-0.7960860\n-4.5135825\n2.4909701\n-5.2377565\n2.8056909\n4.187349\n2.1199821\n-14.5778237\n-5.288184\n0.7636763\n1.0684937\n11.4446640\n-3.8809239\n-1.471044\n-5.762816\n9.2540044\n-3.1231925\n3.6703762\n-2.7950426\n-7.2013671\n4.9203092\n-2.9570233\n-5.5224954\n5.8207086\n1.3965562\n-22.8025385\n-9.1081463\n-7.9102714\n-4.8906529\n-4.1812692\n12.2521930\n-21.5515863\n11.7752264\n-6.4152903\n-4.5245436\n-15.0166304\n4.7185851\n-2.0311982\n-15.313263\n1.9812127\n3.7187549\n-19.9333908\n-2.4657861\n-10.5151205\n5.2766498\n-0.7492269\n-4.5585811\n5.6919522\n3.2917922\n14.842447\n1.6301066\n2.8435282\n1.9368151\n10.4675541\n-1.6156167\n3.1438421\n3.1220410\n-7.2900231\n-11.184279\n7.0010860\n6.1452208\n1.4225875\n9.5060338\n7.1725843\n10.3827746\n-4.0648451\n1.0313897\n-2.1608972\n5.0647489\n4.5756690\n7.6646325\n14.0237157\n5.6951812\n9.2584501\n4.4598100\n-15.8358166\n-3.1563305\n1.917434\n-17.5611094\n17.6697035\n1.3399241\n-2.5608145\n-11.7418063\n3.3866112\n4.1455907\n-7.2857001\n12.7853620\n-0.8977347\n9.5451436\n-5.8540317\n-1.8019472\n-7.1901684\n-7.3223563\n10.1887605\n-1.8333372\n2.3858847\n6.8776140\n13.609219\n3.4711277\n2.3469568\n12.4018618\n-3.9063538\n-5.5250340\n-0.2188451\n0.2033760\n\n\n\n\n\nOf course, the raw numbers alone are not especially easy to interpret. The graph below may make it a bit easier to see how items in the same category have vector representations with similar “shapes” whereas items from different categories have different “shapes”. That said, since these vectors have 300 dimensions, I only plot the first 30 entries!\n\n\nCode\ntoPlot &lt;- c()\nfor (i in 1:nrow(lsa_reps_examples)) {\n    toPlot &lt;- rbind(\n        toPlot,\n        tibble(item = rownames(lsa_reps_examples)[i], index = 1:30, val = lsa_reps_examples[i,1:30])\n    )\n}\n\ntoPlot %&gt;%\n    mutate(item = factor(item, levels = concept_names)) %&gt;%\n    mutate(category = factor(item, levels = concept_names, labels = category_names)) %&gt;%\n    mutate(exemplar = factor(item, levels = concept_names, labels = rep(c(\"A\", \"B\"), length(concept_names) / 2))) %&gt;%\n    ggplot(aes(x = index, y = val, color = item)) +\n    geom_line() +\n    facet_wrap(\"category\")\n\n\n\n\n\n\n\n\n\nOf course, this is all just based on our visual impressions—we will now explore different ways of operationalizing similarity between representations. We will then see how to incorporate similarity into models of behavior.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributed representations, similarity, and recognition</span>"
    ]
  },
  {
    "objectID": "vector_reps.html#similarity-between-distributed-representations",
    "href": "vector_reps.html#similarity-between-distributed-representations",
    "title": "10  Distributed representations, similarity, and recognition",
    "section": "10.3 Similarity between distributed representations",
    "text": "10.3 Similarity between distributed representations\nAs noted earlier, different kinds of representations enable different ways of operationalizing the similarity between those representations. This is important because similarity is a core construct in many cognitive models, like we saw with the EBRW. Indeed, the EBRW’s approach to similarity is the first we will consider.\n\n10.3.1 Transformed distance\nRecall that the EBRW operationalized similarity as an exponential function of distance between items. Specifically, the similarity \\(s_{ij}\\) between representations of items \\(i\\) and \\(j\\) was defined as \\[\ns_{ij} = \\exp \\left(-c d_{ij} \\right)\n\\] where \\(c\\) was a sensitivity parameter and \\(d_{ij}\\) is the distance between \\(\\mathbf{x_i}\\), the vector representing item \\(i\\), and \\(\\mathbf{x_j}\\), the vector representing item \\(j\\): \\[\nd_{ij} = \\left( \\sum_{k = 1}^D w_k \\left| x_{ik} - x_{jk} \\right|^p \\right)^{\\frac{1}{p}}\n\\] where \\(D\\) is the number of dimensions in each representation, \\(w_k\\) is the weight given to dimension \\(k\\), and \\(p\\) is a new parameter that we just added for this chapter. The new parameter \\(p\\) makes the distance formula more general. When \\(p = 2\\), we get the Euclidean distance we used in the last chapter. But \\(p\\) can, in principle, be any nonnegative number; this more general distance formula is known as a (Minkowski distance)[https://en.wikipedia.org/wiki/Minkowski_distance].\nThe choice of \\(p\\) reflects how differences in different dimensions contribute to the overall distance. When \\(p = 2\\), corresponding to Euclidean distance, the distance (and therefore similarity) between representations only depends on how far apart their vector representations are, not their orientation relative to the dimensions of the space. Another common choice of \\(p\\) in cognitive models is \\(p = 1\\), corresponding to the “city block” or “taxicab” distance. When \\(p = 1\\), the distance is the sum of the absolute differences between vector representations. As a result, distance (and therefore similarity) depends on how the two items are oriented with respect to the dimensions of the space.\nWe can visualize the effect of different choices of \\(p\\) by drawing contours of equal distance. We can imagine assigning item \\(i\\) to have a vector representation of \\((0, 0)\\) and then consider the all the possible positions of item \\(j\\) that would result in a distance \\(d_{ij} = 1\\). That’s what is shown in the graph below:\n\n\nCode\nexpand_grid(p = c(0.25, 0.5, 1, 2, 4, 8), theta = seq(0, 2*pi, length.out = 501)) %&gt;%\n    mutate(x = cos(theta)) %&gt;%\n    mutate(y = sign(sin(theta)) * (1 - abs(x)^p)^(1 / p)) %&gt;%\n    ggplot(aes(x = x, y = y, color = factor(p), group = p, linewidth = factor(p))) +\n    geom_path() +\n    coord_equal() +\n    scale_linewidth_manual(values = c(\"0.25\" = 0.5, \"0.5\" = 0.5, \"1\" = 1.5, \"2\" = 1.5, \"4\" = 0.5, \"8\" = 0.5)) +\n    labs(x = expression(x[j1]), y = expression(x[j2]), color = \"p\", linewidth = \"p\")\n\n\n\n\n\n\n\n\n\nWhen \\(p = 2\\), the contour of equal distance is a circle—distance is irrespective of the orientation relative to the two dimensions of the space. When \\(p = 1\\), the contour of equal distance is a diamond—distance is the sum of the differences on each dimension. As noted above, most cognitive models adopt either \\(p = 1\\) or \\(p = 2\\), but other values of \\(p\\) are entirely possible. When \\(p\\) gets really big, the contour of equal distance approaches a square—distance depends on the maximum difference. When \\(p\\) gets really small, the contour of equal distance approaches a “star”—distance depends on the minimum difference.\nA major conceptual point to take from the preceding discussion about the Minkowski distance parameter \\(p\\) is that, whenever \\(p \\neq 2\\), we need to take the dimensions of the space seriously. When \\(p \\neq 2\\), distance depends on which dimensions exhibit which differences. The resulting distance is only interpretable if those dimensions are interpretable, in turn. As a result, this approach to similarity is best suited to separable distributed representations.\nFor similar reasons, attention weights \\(w_k\\) only make sense to apply to separable distributed representations. If the elements of a distributed representation cannot be interpreted as referring to different attributes/features, as with an integral distributed representation, it makes little sense to assign those elements different attention weights. Of course, there is nothing stopping you from building such a model—it just will not have a very clear cognitive interpretation.\nTo give a concrete example of how the transformed distance approach to similarity can be implemented in R, we can adapt some of the code we wrote for the EBRW. This example uses the size and color representations for the eight concepts used in the examples in the previous section.\n\n\nCode\n# List names of items and attributes\nconcept_names &lt;- c(\"Pine\", \"Oak\", \"Rose\", \"Daisy\", \"Robin\", \"Canary\", \"Sunfish\", \"Salmon\")\nsep_attribute_names &lt;- c(\"size\", \"hue\", \"chroma\", \"lightness\")\n\n# Define matrix of representations\nseparable_distributed_reps &lt;- matrix(c(\n    50, 131, 36, 27,\n    40, 20, 60, 16,\n    0.1, 3, 58, 79,\n    0.2, 52, 100, 50,\n    0.3, 9, 63, 38,\n    0.1, 60, 100, 63,\n    0.5, 32, 100, 59,\n    1, 5, 99, 71\n), nrow = 8, ncol = length(alt_attribute_names), byrow = TRUE, dimnames = list(concept_names, alt_attribute_names))\n\n# The \"c\" parameter representing sensitivity\nsensitivity &lt;- 0.01\n\n# The \"p\" parameter representing the Minkowski parameter\nminkowski_p &lt;- 1\n\n# These are the weights given to each of the four dimensions.\nattention_weight &lt;- c(1, 1, 1, 1)\n# By convention, these weights are constrained to sum to 1.\nattention_weight &lt;- attention_weight / sum(attention_weight)\n\n# First, define an empty distance matrix\ndistance &lt;- matrix(0, nrow = nrow(separable_distributed_reps), ncol = nrow(separable_distributed_reps), dimnames = list(concept_names, concept_names))\n\n# Fill in each entry in the distance matrix \nfor (i in 1:nrow(separable_distributed_reps)) {\n    for (j in 1:nrow(separable_distributed_reps)) {\n        distance[i, j] &lt;- sum(attention_weight * abs(separable_distributed_reps[i,] - separable_distributed_reps[j,])^minkowski_p)^(1 / minkowski_p)\n    }\n}\n\nsimilarity &lt;- exp(-sensitivity * distance)\n\nprint(round(similarity, 2))\n\n\n        Pine  Oak Rose Daisy Robin Canary Sunfish Salmon\nPine    1.00 0.68 0.53  0.58  0.59   0.58    0.54   0.49\nOak     0.68 1.00 0.74  0.69  0.83   0.66    0.71   0.69\nRose    0.53 0.74 1.00  0.74  0.88   0.75    0.80   0.88\nDaisy   0.58 0.69 0.74  1.00  0.79   0.95    0.93   0.84\nRobin   0.59 0.83 0.88  0.79  1.00   0.75    0.82   0.83\nCanary  0.58 0.66 0.75  0.95  0.75   1.00    0.92   0.85\nSunfish 0.54 0.71 0.80  0.93  0.82   0.92    1.00   0.90\nSalmon  0.49 0.69 0.88  0.84  0.83   0.85    0.90   1.00\n\n\nGo ahead, try it out yourself! You may notice that the sensitivity parameter is pretty small, see what happens if you increase it. In fact, that will be an exercise for us later!\n\n\n10.3.2 Dot product\nThe dot product or inner product is a way of directly computing the similarity between two vectors, without first computing a distance between them. While transformed distance is the most common approach to modeling similarity in the GCM/EBRW, dot products have been used a lot in other models of memory, notably the Theory Of Distributed Associative Memory (TODAM; Murdock (1982)).\nThe dot product between two vectors is sum of the products of their elements. To get formal about it, the dot product between vector representations \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_j}\\) is often written as \\(\\mathbf{x_i} \\cdot \\mathbf{x_j}\\) (can you guess why this is called a dot product?). The dot product is defined as: \\[\n\\mathbf{x_i} \\cdot \\mathbf{x_j} = \\sum_{k = 1}^D x_{ik} x_{jk}\n\\]\nIntuitively, the dot product quantifies the amount of overlap between two representations. Any time the two vectors have elements that are both large and both of the same sign, their dot product will be larger. That’s why the dot product is a useful way to operationalize similarity.\nTo make this concrete, let’s consider a pair of two-dimensional vector representations (so \\(D = 2\\) in this example). We will assume that \\(x_{11} = x_{21} = 1\\), i.e., that the first element of each vector representation equals 1. The second elements of each representation (\\(x_{12}\\) and \\(x_{22}\\)) will vary between -2 and 2. The colors in the graph below indicates the value of the dot product for each combination of values of $x_{21}The chunk of code below varies the elements of these vectors so you can see how their dot product is influenced by those changes.\n\n\nCode\nexpand_grid(x11 = 1, x12 = seq(-2, 2, length.out = 11), x21 = 1, x22 = seq(-2, 2, length.out = 11)) %&gt;%\n    mutate(dot_product = x11 * x21 + x12 * x22) %&gt;%\n    ggplot(aes(x = x12, y = x22, fill = dot_product)) +\n    geom_raster() +\n    scale_fill_gradient2() +\n    coord_equal() +\n    labs(x = expression(x[12]), y = expression(x[22]), fill = expression(bold(x[1]) %.% bold(x[2])))\n\n\n\n\n\n\n\n\n\nNotice that the dot product is large and positive (i.e., very blue) whenever \\(x_{12}\\) and \\(x_{22}\\) are large and of the same sign, which happens in the upper right and lower left quadrants of the graph above. To the extent that \\(x_{12}\\) and \\(x_{22}\\) are large but of different signs, the dot product becomes negative (the red regions in the upper left and lower right quadrants). The dot product thus quantifies the degree to which the elements of each representation have the same sign, weighted by the magnitude of the elements of each representation.\nWhile dot products can be computed with continuous values, they can also be computed for binary values just as well. For any element that contains a zero in the vectors being compared, that element will contribute zero to the dot product. Therefore, the dot product between representations that consist of ones and zeros is the number of elements for which both representations contain a one.\nFor example, using the concept features from earlier, the following is a matrix where each entry gives the dot product between the representations of the items in the corresponding rows/columns.\n\n\nCode\nconcept_names &lt;- c(\"Pine\", \"Oak\", \"Rose\", \"Daisy\", \"Robin\", \"Canary\", \"Sunfish\", \"Salmon\")\n\nattribute_names &lt;- c(\"can_grow\", \"is_living\", \"has_roots\", \"has_bark\", \"is_big\", \"has_branches\", \"is_green\", \"has_leaves\", \"has_petals\", \"is_pretty\", \"is_red\", \"is_yellow\", \"can_move\", \"has_feathers\", \"can_fly\", \"has_wings\", \"can_sing\", \"can_swim\", \"has_gills\", \"has_scales\")\n\nconcept_attributes &lt;- matrix(c(\n    1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n    1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n    1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n    1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n    1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1\n), nrow = 8, ncol = length(attribute_names), byrow = TRUE, dimnames = list(concept_names, attribute_names))\n\ntcrossprod(concept_attributes)\n\n\n        Pine Oak Rose Daisy Robin Canary Sunfish Salmon\nPine       7   6    3     3     2      2       2      2\nOak        6   7    4     4     2      2       2      2\nRose       3   4    7     6     3      2       2      3\nDaisy      3   4    6     7     2      3       3      2\nRobin      2   2    3     2     7      6       3      4\nCanary     2   2    2     3     6      8       4      3\nSunfish    2   2    2     3     3      4       7      6\nSalmon     2   2    3     2     4      3       6      7\n\n\nIt is worth taking a look at the code for this one, since R makes it easy to compute the dot products between many vectors simultaneously using the tcrossprod function, as illustrated in the final line of the code chunk above. R has two functions that implement particular forms of (matrix multiplication)[https://en.wikipedia.org/wiki/Matrix_multiplication], crossprod and tcrossprod. Specifically, tcrossprod returns the cross-product of an \\(N \\times M\\) matrix \\(A\\) with the transpose of a \\(K \\times M\\) matrix \\(B\\). The “transpose” of a matrix is what you get when you swap its rows and columns. So the transpose of matrix \\(B\\) has dimension \\(M \\times K\\). The result is an \\(N \\times K\\) matrix where the entry in the \\(i\\)th row and \\(j\\)th column is the dot product of the \\(i\\)th row of \\(A\\) and the \\(j\\)th row of \\(B\\). This is illustrated below.\n\n\nCode\nA &lt;- matrix(rnorm(n = 15), nrow = 3, ncol = 5)\nB &lt;- matrix(rnorm(n = 20), nrow = 4, ncol = 5)\n\nprint(A)\n\n\n          [,1]       [,2]       [,3]       [,4]       [,5]\n[1,] -1.042958 -0.4140553  1.3762569 -1.2603923  0.1210855\n[2,] -1.070853 -0.3916844 -0.9313825  0.7158927 -2.6305672\n[3,]  0.379089  1.5611897  1.9640299  0.7494422 -0.8950186\n\n\nCode\nprint(B)\n\n\n           [,1]       [,2]       [,3]      [,4]       [,5]\n[1,] -1.1570559 -0.1226359  0.7488086 -2.455725 0.80124750\n[2,]  0.5232695  1.7441242 -0.8286818 -1.545715 0.05190826\n[3,]  0.1259115 -1.7034857  1.8937240 -0.436462 0.24459708\n[4,] -1.0263656 -1.2446303  1.0119758  1.201336 0.54842031\n\n\nCode\ntcrossprod(A, B)\n\n\n          [,1]        [,2]       [,3]       [,4]\n[1,]  5.480287 -0.45389825  3.7599981 1.53079186\n[2,] -3.276126 -1.71478563 -2.1872745 0.06142366\n[3,] -1.716958  0.08883569  0.5615761 0.06484820\n\n\nWhen you call tcrossprod with just a single matrix, R uses that same matrix for both \\(A\\) and \\(B\\). For example:\n\n\nCode\ntcrossprod(A)\n\n\n           [,1]       [,2]      [,3]\n[1,]  4.7565358 -1.2236171 0.6082522\n[2,] -1.2236171  9.6000030 0.0442214\n[3,]  0.6082522  0.0442214 7.8011570\n\n\nCode\ntcrossprod(B)\n\n\n           [,1]      [,2]      [,3]       [,4]\n[1,]  8.5871127  2.397573  2.749072 -0.4127539\n[2,]  2.3975730  6.394423 -3.787157 -5.3749163\n[3,]  2.7490719 -3.787157  6.754235  3.5171862\n[4,] -0.4127539 -5.374916  3.517186  5.3705980\n\n\nSo to get back to talking about dot products between vector representations, tcrossprod(concept_attributes) returns a matrix where the entry in the \\(i\\)th row and \\(j\\)th column is the dot product between the \\(i\\)th row of concept_attributes and the \\(j\\)th row of concept_attributes. Convenient!\nAs we saw, the dot product can be used to quantify similarity between separable distributed vector representations. It can also be used to quantify similarity between integral distributed vector representations. For example, this is the matrix of dot-product similarities between the integral representations of the concepts in our running example:\n\n\nCode\nknitr::kable(tcrossprod(lsa_reps_examples))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPine\nOak\nRose\nDaisy\nRobin\nCanary\nSunfish\nSalmon\n\n\n\n\nPine\n38573.968\n37331.6943\n18080.1644\n3954.26442\n8252.5993\n4161.7246\n1012.64213\n12580.369\n\n\nOak\n37331.694\n59365.5215\n31061.6833\n5111.06152\n11119.6447\n3805.2614\n703.56230\n14579.626\n\n\nRose\n18080.164\n31061.6833\n83666.0058\n9616.77767\n22586.7398\n6677.3640\n544.37714\n12069.413\n\n\nDaisy\n3954.264\n5111.0615\n9616.7777\n3147.26980\n5174.1907\n1266.2499\n92.93532\n1729.109\n\n\nRobin\n8252.599\n11119.6447\n22586.7398\n5174.19071\n25216.0414\n4542.8128\n307.62160\n4887.937\n\n\nCanary\n4161.725\n3805.2614\n6677.3640\n1266.24994\n4542.8128\n6624.3994\n277.54518\n1760.961\n\n\nSunfish\n1012.642\n703.5623\n544.3771\n92.93532\n307.6216\n277.5452\n405.72158\n2192.350\n\n\nSalmon\n12580.369\n14579.6264\n12069.4125\n1729.10943\n4887.9368\n1760.9615\n2192.34977\n33261.839\n\n\n\n\n\n\n\n10.3.3 Cosine similarity\nAs noted above, the dot product is sensitive to the magnitude of the values in the vector representations. This could be important if magnitude represents something important in the context of our model. For example, just like the EBRW allows for a “strength” parameter that can multiplicatively scale the similarity between two representations, we could imagine that the magnitude of the entries in a vector represents the “strength” or “salience” of the item that vector represents.\nIn some models, though, the magnitudes are irrelevant—instead an item is represented by the pattern of relative values in the vector. We can still use the dot product to quantify similarity, but we will first need to normalize each vector representation. Specifically, we will normalize the vectors such that their dot products with themselves are equal to 1. Referring back to the definition of the dot product above, the dot product of a vector with itself is the sum of the squares of the entries in the vector. So if we divide each entry in a vector by the square root of the sum of the squared entries, we obtain our normalized vector representation.\nWe can see how this works by imagining that we have two integral distributed vector representations, which the code below randomly generates and then shows.\n\n\nCode\nx_1 &lt;- rnorm(n = 10, mean = 0, sd = 1)\nx_2 &lt;- rnorm(n = 10, mean = 0, sd = 1)\n\nknitr::kable(rbind(x_1, x_2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx_1\n1.4623004\n-0.4582554\n-1.3126406\n-0.7356256\n0.6091427\n0.1927802\n-0.0306855\n0.5532889\n0.7257781\n1.088871\n\n\nx_2\n-0.9597082\n0.6364972\n0.4518392\n-0.3443309\n-0.1859800\n-1.0846488\n0.3763883\n-1.6267809\n0.6824552\n-1.126632\n\n\n\n\n\nTo create normalized versions of these representations, we divide them by the square root of the sum of their squared entries:\n\n\nCode\nx_1_norm &lt;- x_1 / sqrt(sum(x_1^2))\nx_2_norm &lt;- x_2 / sqrt(sum(x_2^2))\n\nknitr::kable(rbind(x_1_norm, x_2_norm))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx_1_norm\n0.5511184\n-0.1727094\n-0.4947139\n-0.2772459\n0.2295764\n0.0726559\n-0.0115649\n0.2085260\n0.2735345\n0.4103787\n\n\nx_2_norm\n-0.3532000\n0.2342491\n0.1662897\n-0.1267236\n-0.0684459\n-0.3991817\n0.1385216\n-0.5987017\n0.2511630\n-0.4146329\n\n\n\n\n\nTo see the effect of this normalization on the dot products, let’s compute the dot products between the unnormalized vectors and for the normalized ones:\n\n\nCode\n# Dot product between unnormalized x_1 and itself\nsum(x_1 * x_1)\n\n\n[1] 7.040174\n\n\nCode\n# Dot product between unnormalized x_2 and itself\nsum(x_2 * x_2)\n\n\n[1] 7.383073\n\n\nCode\n# Dot product between unnormalized x_1 and x_2\nsum(x_1 * x_2)\n\n\n[1] -4.000327\n\n\nCode\n# Dot product between normalized x_1 and itself\nsum(x_1_norm * x_1_norm)\n\n\n[1] 1\n\n\nCode\n# Dot product between normalized x_2 and itself\nsum(x_2_norm * x_2_norm)\n\n\n[1] 1\n\n\nCode\n# Dot product between normalized x_1 and x_2\nsum(x_1_norm * x_2_norm)\n\n\n[1] -0.5548623\n\n\nBy normalizing the vectors so that the dot product with themselves is 1, we can treat the result as a measure of relative similarity between vector representations. Since similarity is greatest between a representation and itself, the dot product between normalized vector representations has an upper bound of 1. The dot product also has a lower bound of -1, which is achieved when the two vectors have exactly opposite entries:\n\n\nCode\nsum(x_1_norm * (-x_1_norm))\n\n\n[1] -1\n\n\nJust for completeness, let’s see how we can use the tcrossprod trick in this example:\n\n\nCode\n# Matrix of dot products for unnormalized representations\ntcrossprod(rbind(x_1, x_2))\n\n\n          x_1       x_2\nx_1  7.040174 -4.000327\nx_2 -4.000327  7.383073\n\n\nCode\n# Matrix of dot products for normalized representations\ntcrossprod(rbind(x_1_norm, x_2_norm))\n\n\n           x_1_norm   x_2_norm\nx_1_norm  1.0000000 -0.5548623\nx_2_norm -0.5548623  1.0000000\n\n\nThis might remind you of something—the Pearson correlation coefficient, which also ranges between 1 and -1 and measures the degree of correspondence between two set of numbers. Indeed, the normalized dot product can be interpreted in exactly the same way as the correlation coefficient! They are not strictly identical, however, as the correlation coefficient involves subtracting out the mean of each set of numbers, which we do not do to get the normalized dot product.\nThe normalized dot product has another, potentially even more evocative interpretation: It is the cosine of the angle between two vector representations. Specifically, we can treat a vector representation as specifying the end point of a line segment that starts at the origin (i.e., the point where all vector elements are equal to zero). The line segment thus goes in a particular direction from the origin. The normalized dot product between two vectors is the cosine of the angle between the directions specified by each vector. For that reason, the normalized dot product is often called cosine similarity.\nThe basic idea of cosine similarity is illustrated in the graph below. The graph shows different vector representations \\(\\mathbf{x_j}\\) that are at different angles \\(\\theta_{ij}\\) relative to another vector representation \\(\\mathbf{x_i}\\). When the two vectors point generally in the same direction (to the left), the cosine similarity \\(s_{ij} = \\cos \\left( \\theta_{ij} \\right)\\) is greater than zero. When the two vectors point generally in the opposite direction (i.e., when \\(\\mathbf{x_j}\\) points more to the right than the left), cosine similarity is less than zero. Finally, when the two vectors are orthogonal, cosine similarity is zero.\n\n\nCode\nplot_angles &lt;- seq(0, 2 * pi - pi / 6, by = pi / 6)\nangle_ticks &lt;- seq(0, pi, by = pi / 4)\nangle_labels &lt;- c(expression(0), expression(pi / 4), expression(pi / 2), expression(3 * pi / 4), expression(pi))\n\ncircPlotDF &lt;- tibble(x = 0, y = 0, xend = -cos(plot_angles), yend = sin(plot_angles), theta = acos(cos(plot_angles)))\n\ncircPlot &lt;- circPlotDF %&gt;%\n    ggplot(aes(x = x, y = y, xend = xend, yend = yend, color = theta)) +\n    geom_segment(arrow = arrow(length = unit(0.04, \"npc\"))) +\n    annotate(geom = \"segment\", x = 0, y = 0, xend = -1, yend = 0, arrow = arrow(length = unit(0.04, \"npc\"), type = \"closed\"), color = \"black\", linewidth = 1) +\n    annotate(geom = \"curve\", x = -0.7, y = 0, xend = -0.7 * cos(plot_angles[2]), yend = 0.7 * sin(plot_angles[2]), linetype = \"dashed\", color = \"black\", curvature = -1/6) +\n    annotate(geom = \"text\", x = -1.05, y = 0, label = \"bold(x[i])\", parse = TRUE, hjust = 1, vjust = 0.5, color = \"black\") +\n    annotate(geom = \"text\", x = -1.05 * cos(plot_angles[2]), y = 1.05 * sin(plot_angles[2]), label = \"bold(x[j])\", parse = TRUE, hjust = 1, vjust = 0.5, color = color(\"sunset\")(length(plot_angles))[2]) +\n    annotate(geom = \"text\", x = -0.75 * cos(plot_angles[2] / 2), y = 0.75 * sin(plot_angles[2] / 2), label = \"theta[ij]\", parse = TRUE, hjust = 1, vjust = 0.5) +\n    scale_color_sunset(range = c(0, 1), midpoint = pi / 2, guide = \"none\") +\n    coord_equal(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n    theme_void()\n\ncosPlotDF &lt;- tibble(theta = seq(0, pi, length.out = 101), s = cos(theta))\n\ncosPlot &lt;- cosPlotDF %&gt;%\n    ggplot(aes(x = theta, y = s, color = theta)) +\n    geom_line() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n    scale_color_sunset(range = c(0, 1), midpoint = pi / 2, guide = \"none\") +\n    labs(x = expression(theta[ij]), y = expression(s[ij] == plain(cos)*(theta[ij]))) +\n    scale_x_continuous(breaks = angle_ticks, labels = angle_labels)\n\nprint(circPlot + cosPlot + plot_layout(ncol = 1))\n\n\n\n\n\n\n\n\n\nCosine similarity between vector representations \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_j}\\) can be formally specified in terms of the entries in each vector, without needing to explicitly specify the angle \\(\\theta_{ij}\\): \\[\ns_{ij} = \\cos \\left( \\theta_{ij} \\right) = \\frac{\\sum_{k = 1}^D x_{ik} x_{jk}}{\\left( \\sqrt{\\sum_{k = 1}^D x_{ik}^2} \\right) \\left( \\sqrt{\\sum_{k = 1}^D x_{jk}^2} \\right)}\n\\] where the numerator is the dot product and the denominator is the result of normalizing each vector.\nAbove, we saw how to use the tcrossprod trick to quickly compute the dot products between a set of vector representations that are stored in a matrix, with one row per item. We can use the same trick to compute the cosine similarities between those vectors if we first normalize them. One way to do that is to use a for loop to create a new matrix of normalized representations, like we did with x_1 and x_2 above. The code below does this with lsa_reps_examples, the integral distributed representations for our eight living things. It then uses tcrossprod with the normalized representations (reps_normalized) to get the cosine similarities for each pair of items.\n\n\nCode\nreps_normalized &lt;- lsa_reps_examples\n\nfor (i in 1:nrow(reps_normalized)) {\n    reps_normalized[i,] &lt;- lsa_reps_examples[i,] / sqrt(sum(lsa_reps_examples[i,]^2))\n}\n\nknitr::kable(tcrossprod(reps_normalized))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPine\nOak\nRose\nDaisy\nRobin\nCanary\nSunfish\nSalmon\n\n\n\n\nPine\n1.0000000\n0.7801236\n0.3182590\n0.3588819\n0.2646093\n0.2603472\n0.2559734\n0.3512150\n\n\nOak\n0.7801236\n1.0000000\n0.4407409\n0.3739185\n0.2873990\n0.1918864\n0.1433578\n0.3280999\n\n\nRose\n0.3182590\n0.4407409\n1.0000000\n0.5926361\n0.4917457\n0.2836333\n0.0934354\n0.2287908\n\n\nDaisy\n0.3588819\n0.3739185\n0.5926361\n1.0000000\n0.5808139\n0.2773187\n0.0822431\n0.1689983\n\n\nRobin\n0.2646093\n0.2873990\n0.4917457\n0.5808139\n1.0000000\n0.3514901\n0.0961755\n0.1687773\n\n\nCanary\n0.2603472\n0.1918864\n0.2836333\n0.2773187\n0.3514901\n1.0000000\n0.1692959\n0.1186324\n\n\nSunfish\n0.2559734\n0.1433578\n0.0934354\n0.0822431\n0.0961755\n0.1692959\n1.0000000\n0.5967915\n\n\nSalmon\n0.3512150\n0.3280999\n0.2287908\n0.1689983\n0.1687773\n0.1186324\n0.5967915\n1.0000000\n\n\n\n\n\nWe can also get a bit fancy and use some more matrix algebra to do the normalization without using a for loop. Admittedly, unless you are used to linear algebra, this may not be too intuitive. However, the main idea is to multiply the matrix of representations by a diagonal matrix that normalizes each row:\n\n\nCode\nreps_normalized &lt;- diag(1 / sqrt(rowSums(lsa_reps_examples^2))) %*% lsa_reps_examples\n\nknitr::kable(tcrossprod(reps_normalized))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0000000\n0.7801236\n0.3182590\n0.3588819\n0.2646093\n0.2603472\n0.2559734\n0.3512150\n\n\n0.7801236\n1.0000000\n0.4407409\n0.3739185\n0.2873990\n0.1918864\n0.1433578\n0.3280999\n\n\n0.3182590\n0.4407409\n1.0000000\n0.5926361\n0.4917457\n0.2836333\n0.0934354\n0.2287908\n\n\n0.3588819\n0.3739185\n0.5926361\n1.0000000\n0.5808139\n0.2773187\n0.0822431\n0.1689983\n\n\n0.2646093\n0.2873990\n0.4917457\n0.5808139\n1.0000000\n0.3514901\n0.0961755\n0.1687773\n\n\n0.2603472\n0.1918864\n0.2836333\n0.2773187\n0.3514901\n1.0000000\n0.1692959\n0.1186324\n\n\n0.2559734\n0.1433578\n0.0934354\n0.0822431\n0.0961755\n0.1692959\n1.0000000\n0.5967915\n\n\n0.3512150\n0.3280999\n0.2287908\n0.1689983\n0.1687773\n0.1186324\n0.5967915\n1.0000000\n\n\n\n\n\nAs you can see, the result is the same set of cosine similarities. The linear algebra approach can be more efficient than the for loop if we have a large number of representations to deal with, but for the purposes of this class I recommend whichever approach you are most comfortable with.\nFinally, while cosine similarity can be computed using either separable or integral distributed representations, it makes a bit more sense to apply with integral distributed representations. That’s because the cosine similarity is a measure of the extent to which the entries in two vector representations exhibit the same patterns. This is a “holistic” measure of overall similarity which does not depend on the entries in each vector having any particular interpretation.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributed representations, similarity, and recognition</span>"
    ]
  },
  {
    "objectID": "vector_reps.html#modeling-choice-and-response-time-with-similarity",
    "href": "vector_reps.html#modeling-choice-and-response-time-with-similarity",
    "title": "10  Distributed representations, similarity, and recognition",
    "section": "10.4 Modeling choice and response time with similarity",
    "text": "10.4 Modeling choice and response time with similarity\nIn the last chapter, we saw how the EBRW used similarity between representations of items to explain choice and response time in recognition tasks. The previous application did not include attention weights and only allowed for transformed distance as a measure of similarity. The chunk of code below shows can we can modify our EBRW code to accommodate different types of similarity and to allow for attention weights on transformed distances. This function is embedded in a simplified working example in this script. The changes are explained in comments.\n\n\nCode\n# To be more general, the function now asks for \"stim_reps\", which is a matrix of vector representations of each stimulus.  These could be any kind of vector representation (localist, separable, integral).\n# The type of similarity to use is specified by the `sim_type` argument.  This can be \"transformed_distance\", \"cosine\", or \"dot_product\".\nextended_ebrw_nll &lt;- function(par, stim_reps, study_items, probe_item, response, rt, n_sims = 0, sim_type = \"transformed_distance\") {\n    if (sim_type == \"transformed_distance\") {\n        # If transformed distance similarity is specified, the parameter vector can also optionally specify parameters for *attention weights*.\n        # If there are D columns in stim_reps, there are D - 1 free attention weights, since they must sum to one.\n        # The attention weight parameters need only be nonnegative numbers, since they will be normalized anyway.\n        if (is.na(par[\"attention_weight[1]\"])) {\n            attention_weight &lt;- rep(1, ncol(stim_reps))\n        } else {\n            attention_weight &lt;- c(par[paste0(\"attention_weight[\", 1:(ncol(stim_reps) - 1), \"]\")], 1)\n        }\n        attention_weight &lt;- attention_weight / sum(attention_weight)\n        \n        # You can also optionally specify a Minkowski parameter, but it will be assumed to be 2 if you don't\n        if (is.na(par[\"minkowski_p\"])) {\n            minkowski_p &lt;- 2\n        } else {\n            minkowski_p &lt;- par[\"minkowski_p\"]\n        }\n        \n        # Finally, compute the matrix of stimulus similarities\n        stim_sims &lt;- diag(nrow(stim_reps))\n        \n        for (i in 2:nrow(stim_reps)) {\n            for (j in 1:(i - 1)) {\n                stim_sims[i, j] &lt;- stim_sims[j, i] &lt;- exp(-par[\"specificity\"] * sum(attention_weight * abs(stim_reps[i,] - stim_reps[j,])^minkowski_p)^(1 / minkowski_p))\n            }\n        }\n    } else {\n        # If similarity is *not* transformed distance, then dot product is assumed.\n        if (sim_type == \"cosine\") {\n            # If cosine similarity is specified, then first normalize the representations.\n            stim_reps &lt;- diag(1 / sqrt(rowSums(stim_reps^2))) %*% stim_reps\n        }\n        \n        # Finally, compute the matrix of stimulus similarities\n        stim_sims &lt;- tcrossprod(stim_reps)^par[\"specificity\"]\n    }\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(stim_sims[probe_item[i], study_items[i,]], na.rm = TRUE)\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    if (is.infinite(n_sims)) {\n        # 2a. Compute predicted response probabilities and mean RT's for each trial\n        result &lt;- c()\n        \n        evidence_var &lt;- evidence_sd^2\n        \n        for (i in 1:length(probe_item)) {\n            trial_p_yes &lt;- (1 - exp(-2 * par[\"a\"] * evidence_mean[i] * par[\"w\"] / evidence_var[i])) / (1 - exp(-2 * par[\"a\"] * evidence_mean[i] / evidence_var[i]))\n            trial_mean_rt_yes &lt;- par[\"t0\"] + (par[\"a\"] / evidence_mean[i]) * (1 / tanh(par[\"a\"] * evidence_mean[i] / evidence_var[i]) - par[\"w\"] / tanh(par[\"a\"] * evidence_mean[i] * par[\"w\"] / evidence_var[i]))\n            trial_mean_rt_no &lt;- par[\"t0\"] + (par[\"a\"] / evidence_mean[i]) * (1 / tanh(par[\"a\"] * evidence_mean[i] / evidence_var[i]) - (1 - par[\"w\"]) / tanh(par[\"a\"] * evidence_mean[i] * (1 - par[\"w\"]) / evidence_var[i]))\n            \n            result &lt;- rbind(\n                result,\n                tibble(trial = i, p_yes = trial_p_yes, mean_rt_yes = trial_mean_rt_yes, mean_rt_no = trial_mean_rt_no)\n            )\n        }\n        \n        # 3a. Return final result\n        return(result)\n    } else if (n_sims &gt; 0) {\n        # 2b. Simulate a response for each trial\n        result &lt;- c()\n        \n        # For each trial...\n        for (i in 1:length(probe_item)) {\n            # Simulate `n_sims` replications of this trial, using its corresponding parameters\n            trial_sim &lt;- sampWiener(\n                N = n_sims,\n                a = par[\"a\"] / evidence_sd[i],\n                w = par[\"w\"],\n                v = evidence_mean[i] / evidence_sd[i],\n                t0 = par[\"t0\"]\n            )\n            \n            # Append the new set of simulations to our `result`, including an indicator of which trial we are simulating and an index for each simulation\n            result &lt;- rbind(\n                result,\n                tibble(trial = i, sim_index = 1:n_sims, rt = trial_sim$q, response = trial_sim$response)\n            )\n        }\n        \n        # 3b. Return final result\n        return(result)\n    } else {\n        # 2c. Calculate the log-likelihood of each observed response/RT on each trial\n        result &lt;- try(WienerPDF(\n            t = rt,\n            response = response,\n            a = par[\"a\"] / evidence_sd,\n            w = par[\"w\"],\n            v = evidence_mean / evidence_sd,\n            t0 = par[\"t0\"]\n        ))\n        \n        # 3c. Return final result\n        if (class(result) == \"try-error\") {\n            return(Inf)\n        } else {\n            return(-sum(result$logvalue))\n        }\n    }\n}",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributed representations, similarity, and recognition</span>"
    ]
  },
  {
    "objectID": "vector_reps.html#exercises",
    "href": "vector_reps.html#exercises",
    "title": "10  Distributed representations, similarity, and recognition",
    "section": "10.5 Exercises",
    "text": "10.5 Exercises\n\nWhat kinds of items do you think are best modeled using separable representations and what kinds of items do you think are best modeled using integral representations? Are there particular characteristics of items that make separable or integral representations more appropriate?\n\n\n\n\n\nGarner, W. R., & Felfoldy, G. L. (1970). Integrality of stimulus dimensions in various types of information processing. Cognitive Psychology, 1, 225–241.\n\n\nLandauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2), 211–240.\n\n\nMarr, D. (1982). Vision: A computational investigation into the human representation and processing of visual information. W.H. Freeman.\n\n\nMurdock, B. B. (1982). A theory for the storage and retrieval of item and associative information. Psychological Review, 89(3), 609–626.\n\n\nPhiliastides, M. G., Ratcliff, R., & Sajda, P. (2006). Neural representation of task difficulty and decision making during perceptual categorization: A timing diagram. Journal of Neuroscience, 26(35), 8965–8975. https://doi.org/10.1523/JNEUROSCI.1655-06.2006\n\n\nPurcell, B. A., Heitz, R. P., Cohen, J. Y., Schall, J. D., Logan, G. D., & Palmeri, T. J. (2010). Neurally constrained modeling of perceptual decision making. Psychological Review, 117(4), 1113–1143. https://doi.org/10.1037/a0020311\n\n\nPurcell, B. A., Schall, J. D., Logan, G. D., & Palmeri, T. J. (2012). From salience to saccades: Multiple-alternative gated stochastic accumulator model of visual search. Journal of Neuroscience, 32(10), 3433–3446. https://doi.org/10.1523/JNEUROSCI.4622-11.2012\n\n\nRogers, T. T., & McClelland, J. L. (2004). Semantic cognition: A parallel distributed processing approach. MIT Press.\n\n\nShadlen, M. N., & Newsome, W. T. (2001). Neural basis of a perceptual decision in the parietal cortex (area LIP) of the rhesus monkey. Journal of Neurophysiology, 86(4), 1916–1936. https://doi.org/10.1152/jn.2001.86.4.1916\n\n\nTurner, B. M., Forstmann, B. U., Wagenmakers, E.-J., Brown, S. D., Sederberg, P. B., & Steyvers, M. (2013). A bayesian framework for simultaneously modeling neural and behavioral data. NeuroImage, 72, 193–206. https://doi.org/10.1016/j.neuroimage.2013.01.048",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributed representations, similarity, and recognition</span>"
    ]
  },
  {
    "objectID": "learning.html",
    "href": "learning.html",
    "title": "11  Associative Learning",
    "section": "",
    "text": "11.1 Hebbian learning\nIn the previous chapter, we saw several different examples of how computational cognitive models can represent the sorts of things we do cognition about: words, concepts, objects, events, etc. The representations were treated as given, either from a statistical procedure like Multidimensional Scaling or machine learning or from our own knowledge/expertise regarding the physical and conceptual features of items (like color, size, shape, etc.). Although one can view storage of new exemplars in memory as a kind of learning (see the Exercises), this chapter focuses on the kind of associative learning that is thought to underlie a variety of phenomena in cognition. Associative learning refers to the formation of functional links/connections/associations between elements that frequently co-occur. This form of learning is important for learning how words and referents go together, what properties objects have, and how to make predictions about what may happen next.\nAn important distinction in this form of learning is between supervised and unsupervised learning. In unsupervised learning, the learner does not have an explicit goal; as such, unsupervised learning often amounts to forming a representation of the patterns of correlation between elements in the learner’s environment. In supervised learning, the learner has a particular goal they are trying to achieve; in supervised learning, the learner gets feedback either from a “teacher” or from the environment itself about how well they achieved their goal. In supervised learning, the learner tries to form a representation that minimizes their “error”, that is, the discrepancy between the learner’s goal and the feedback they received.\nThe representations we explored in the last chapter all took the form of vectors which characterized the properties of items that someone might encounter. Since associative learning involves learning how properties go with one another, we will introduce another type of representation in this chapter: A matrix of associative weights. All the forms of learning we will examine in this chapter amount to making adjustments to the entries in one or more of these matrices. We’ll first see how this principle plays out in the context of a simple but powerful form of unsupervised learning: Hebbian learning.\nAlthough this form of learning takes its name from Hebb (1949), the basic idea has been around since at least the ancient Greeks. The idea is that things that are experienced at the same time become associated with one another. Eventually, experiencing one thing “activates” or “evokes” or “retrieves” other things that were frequently encountered alongside it. In more recent times, the principle has been applied to neurons, giving rise to the dictum that “neurons that fire together wire together”.\nTo model Hebbian learning, we assume that the learner experiences discrete learning “events”. These “events” are analogous to the kinds of memory traces we used in the EBRW: each event is represented as a vector with \\(D\\) dimensions. These representations can be localist or either separable or integral distributed. Learning is modeled by making adjustments in a \\(D \\times D\\) matrix, where the entry in the \\(i\\)th row and \\(j\\)th column is the associative weight between element \\(i\\) and element \\(j\\) of the event representations. These weights get updated each time an event occurs. As a result, the weights eventually come to represent the ways that different event element co-vary with one another.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Associative Learning</span>"
    ]
  },
  {
    "objectID": "learning.html#hebbian-learning",
    "href": "learning.html#hebbian-learning",
    "title": "11  Associative Learning",
    "section": "",
    "text": "11.1.1 Representing learning events\nLet’s make this concrete by thinking back to the Blast cell example. Recall that those data involved participants with varying degrees of expertise in identifying whether a cell was a potentially cancerous “blast” cell or not, just by looking at a picture of the cell. Let’s imagine a related but somewhat simpler scenario, based on those used by Medin et al. (1982), that puts us in the position of being a novice gradually learning to become an expert.\nImagine that we are new doctors reading case reports that describe the presence or absence of four symptoms in each patient. For the moment, we are only interested in how these different symptoms may or may not co-vary with one another. The four symptoms are swollen eyelids, splotchy ears, discolored gums, and nosebleed. We have eight patient reports, and we can represent each report using a separable distributed vector with \\(D = 4\\) dimensions. As shown below, we can represent the presence of a symptom with a 1 and its absence with a 0.\n\n\nCode\nevent &lt;- matrix(c(\n    0, 0, 1, 1,\n    1, 1, 1, 1,\n    0, 1, 0, 0,\n    1, 1, 1, 1,\n    1, 0, 1, 1,\n    1, 1, 0, 0,\n    0, 1, 1, 1,\n    1, 0, 0, 0\n), nrow = 8, ncol = 4, byrow = TRUE,\ndimnames = list(\n    c(\"EM\", \"RM\", \"JJ\", \"LF\", \"AM\", \"JS\", \"ST\", \"SE\"),\n    c(\"swollen_eyelids\", \"splotchy_ears\", \"discolored_gums\", \"nosebleed\")\n))\n\nknitr::kable(event)\n\n\n\n\n\n\nswollen_eyelids\nsplotchy_ears\ndiscolored_gums\nnosebleed\n\n\n\n\nEM\n0\n0\n1\n1\n\n\nRM\n1\n1\n1\n1\n\n\nJJ\n0\n1\n0\n0\n\n\nLF\n1\n1\n1\n1\n\n\nAM\n1\n0\n1\n1\n\n\nJS\n1\n1\n0\n0\n\n\nST\n0\n1\n1\n1\n\n\nSE\n1\n0\n0\n0\n\n\n\n\n\n\n\n11.1.2 Updating associative weights\nBefore reading any of the patient reports, we are a tabula rasa, a “blank slate”. That means we don’t know anything about how any of the symptoms go together. This state of initial ignorance is represented by an associative weight matrix that is filled with zeros. This is illustrated below.\n\n\nCode\nassc_weights &lt;- matrix(0, nrow = ncol(event), ncol = ncol(event), dimnames = list(colnames(event), colnames(event)))\n\nknitr::kable(assc_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\nswollen_eyelids\nsplotchy_ears\ndiscolored_gums\nnosebleed\n\n\n\n\nswollen_eyelids\n0\n0\n0\n0\n\n\nsplotchy_ears\n0\n0\n0\n0\n\n\ndiscolored_gums\n0\n0\n0\n0\n\n\nnosebleed\n0\n0\n0\n0\n\n\n\n\n\nNow we read the first patient report:\n\n\nCode\nprint(event[1,])\n\n\nswollen_eyelids   splotchy_ears discolored_gums       nosebleed \n              0               0               1               1 \n\n\nHow should we update our matrix of associative weights on the basis of this report? Since discolored gums and nosebleed were both present, we should increment the cells of the matrix corresponding to the combination of those symptoms. Moreover, since discolored gums and nosebleed are both present with themselves, we might as well update those cells of the matrix too. The result is that our new matrix might look something like this:\n\n\nCode\nknitr::kable(assc_weights + outer(event[1,], event[1,]))\n\n\n\n\n\n\n\n\n\n\n\n\n\nswollen_eyelids\nsplotchy_ears\ndiscolored_gums\nnosebleed\n\n\n\n\nswollen_eyelids\n0\n0\n0\n0\n\n\nsplotchy_ears\n0\n0\n0\n0\n\n\ndiscolored_gums\n0\n0\n1\n1\n\n\nnosebleed\n0\n0\n1\n1\n\n\n\n\n\nTo formalize what we just did, we will draw on one concept we have already seen and another concept we’ve sort of seen, if through a glass darkly. For the concept we have seen, recall that our models of choice and response time involved accumulating samples of evidence, where we added the new sample to a “running total”. That’s exactly how we will be updating our matrix of associative weights, and we can write it formally like this: \\[\nW(t + 1) = W(t) + \\Delta W(t)\n\\] where \\(W(t)\\) is our matrix of associative weights at the current time (\\(t\\)), \\(W(t + 1)\\) is our updated matrix after experiencing the learning event that occurs at time \\(t\\), and \\(\\Delta W(t)\\) is how we change our weights on the basis of the learning event experienced at time \\(t\\). This is just like accumulating samples of evidence, but instead we are accumulating changes in associative weights over time.\nNow for the thing we haven’t exactly seen, at least not in this form: Where do we get \\(\\Delta W(t)\\)? Since we are talking about adding something to a \\(D \\times D\\) matrix, we already know that \\(\\Delta W(t)\\) must also be a \\(D \\times D\\) matrix. We also know conceptually what \\(\\Delta W(t)\\) needs to do: It needs to represent the combinations of features that were present in the learning event at time \\(t\\). Recall from last chapter that, when dealing with binary 0/1 vectors, the dot product between two such vectors gave us a count of the number of features that were present (i.e., coded as 1) in each representation. This is because when \\(x_{ik} = 1\\) and \\(x_{jk} = 1\\), \\(x_{ik} x_{jk} = 1\\), otherwise \\(x_{ik} x_{jk} = 0\\). This is the idea behind what we are about to do.\nWe can get \\(\\Delta W(t)\\) by taking the outer product of the learning event representation \\(\\mathbf{x}(t)\\) with itself. The outer product between two vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) is sometimes written \\(\\mathbf{a} \\otimes \\mathbf{b}\\). If \\(\\mathbf{a}\\) has length \\(N\\) and \\(\\mathbf{b}\\) has length \\(M\\), then \\(\\mathbf{a} \\otimes \\mathbf{b}\\) is an \\(N \\times M\\) matrix where the entry in the \\(i\\)th row and \\(j\\)th column is the product between the \\(i\\)th element of \\(\\mathbf{a}\\) and the \\(j\\)th element of \\(\\mathbf{b}\\). This is illustrated in the example below, showing that the outer function in R gives us the outer product.\n\n\nCode\na &lt;- c(1, 0, 0, 1)\nb &lt;- c(1, 0, 1)\n\n# Outer product of a and b\nouter(a, b)\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    1\n[2,]    0    0    0\n[3,]    0    0    0\n[4,]    1    0    1\n\n\nCode\n# Outer product of a with itself\nouter(a, a)\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    1\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    1    0    0    1\n\n\nCode\n# Outer product of b with itself\nouter(b, b)\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    1\n[2,]    0    0    0\n[3,]    1    0    1\n\n\nSo if the vector \\(\\mathbf{x}(t)\\) represents the learning event experienced at time \\(t\\), then \\[\n\\Delta W(t) = \\lambda \\left[ \\mathbf{x}(t) \\otimes \\mathbf{x}(t) \\right]\n\\] where \\(\\lambda\\) is a parameter that represents the \\(learning rate\\). In the present example, we will keep things simple and assume \\(\\lambda = 1\\), but we will make use of this parameter later.\nGetting back to our symptom example, we can write the update procedure in R like this:\n\n\nCode\nlearning_rate &lt;- 1\ni &lt;- 1\n\nassc_weights &lt;- assc_weights + learning_rate * outer(event[i,], event[i,])\n\n\nYou might already anticipate where we are going: We can use a for loop to update the matrix of associative weights for each learning “event”, i.e., for each patient record we read.\n\n\nCode\n# Initialize weights to zero\nassc_weights &lt;- matrix(0, nrow = ncol(event), ncol = ncol(event))\n\n# Specify learning rate\nlearning_rate &lt;- 1\n\n# Update weights for each event\nfor (i in 1:nrow(event)) {\n    assc_weights &lt;- assc_weights + learning_rate * outer(event[i,], event[i,])\n}\n\nknitr::kable(assc_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\nswollen_eyelids\nsplotchy_ears\ndiscolored_gums\nnosebleed\n\n\n\n\nswollen_eyelids\n5\n3\n3\n3\n\n\nsplotchy_ears\n3\n5\n3\n3\n\n\ndiscolored_gums\n3\n3\n5\n5\n\n\nnosebleed\n3\n3\n5\n5\n\n\n\n\n\n\n\n11.1.3 Using what you’ve learned\nVia Hebbian learning, we have accumulated information about the co-occurrence patterns of different features (symptoms) across a series of learning events (patient records). We can use the resulting matrix of associative weights to do something that has been labeled in a few ways, such as cued recall, pattern completion, inference, and fill-in-the-blanks. The idea is that if we know that a patient has some symptoms, we can use the matrix of associative weights to make a reasonable guess about whether they have other symptoms, based on the degree to which those other symptoms co-occurred with the ones we know about. As the various names listed above imply, this is the same kind of process that goes on when we, say, correctly infer that penguins have wings by knowing that they have beaks and webbed feet (we might also incorrectly infer that penguins can fly for the same reason!).\nSay, for example, that we know a patient has discolored gums. By looking at the corresponding row in our matrix of associative weights, we can see that this symptom co-occurs with nosebleed more often than it does with either swollen eyelids or splotchy ears, as shown below.\n\n\nCode\nassc_weights[\"discolored_gums\",]\n\n\nswollen_eyelids   splotchy_ears discolored_gums       nosebleed \n              3               3               5               5 \n\n\nOf course, the entries in our associative weight matrix are just numbers, they are not behavior. We need an additional process that enables us to predict behavior on the basis of the knowledge represented by the associative weight matrix. The relevant behavior here is whether or not you would be willing to say that a patient had symptom \\(k\\)—this is a choice. We could also model the response time associated with that choice (see the Exercises), but for now we only focus on choice behavior.\nSpecifically, we will transform the numbers extracted from the associative weight matrix into probabilities using the logistic function. This function will be familiar if you’ve done logistic regression, since it transforms unrestricted real numbers into the range between zero and one. The formula for the logistic function is \\[\nf(x) = \\frac{1}{1 + \\exp \\left(-x \\right)}\n\\] and it looks like this:\n\n\nCode\ntibble(x = seq(-6, 6, length.out = 501)) %&gt;%\n    mutate(y = 1 / (1 + exp(-x))) %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_line() +\n    labs(x = \"x\", y = expression(f(x)), title = \"Logistic function\")\n\n\n\n\n\n\n\n\n\nAs shown above, the logistic function returns values greater than 0.5 whenever its argument \\(x\\) is a positive number. When \\(x\\) is negative, the logistic function returns values less than 0.5. And if \\(x = 0\\), the logistic function is exactly 0.5.\nWe can apply the logistic function to the row of the associative weight matrix above:\n\n\nCode\n1 / (1 + exp(-assc_weights[\"discolored_gums\",]))\n\n\nswollen_eyelids   splotchy_ears discolored_gums       nosebleed \n      0.9525741       0.9525741       0.9933071       0.9933071 \n\n\nOf course, we can probably ignore the entry for discolored_gums because we already know the patient has those! Looking at the other probabilities, they are pretty big, reflecting the fact that the associative weights are also pretty large. The magnitude of the weights depends on the learning rate parameter. The chunk of code below shows what happens if we reduce the learning rate.\n\n\nCode\n# Initialize weights to zero\nassc_weights &lt;- matrix(0, nrow = ncol(event), ncol = ncol(event))\n\n# Specify learning rate\nlearning_rate &lt;- 0.2\n\n# Update weights for each event\nfor (i in 1:nrow(event)) {\n    assc_weights &lt;- assc_weights + learning_rate * outer(event[i,], event[i,])\n}\n\nknitr::kable(assc_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\nswollen_eyelids\nsplotchy_ears\ndiscolored_gums\nnosebleed\n\n\n\n\nswollen_eyelids\n1.0\n0.6\n0.6\n0.6\n\n\nsplotchy_ears\n0.6\n1.0\n0.6\n0.6\n\n\ndiscolored_gums\n0.6\n0.6\n1.0\n1.0\n\n\nnosebleed\n0.6\n0.6\n1.0\n1.0\n\n\n\n\n\nCode\n1 / (1 + exp(-assc_weights[\"discolored_gums\",]))\n\n\nswollen_eyelids   splotchy_ears discolored_gums       nosebleed \n      0.6456563       0.6456563       0.7310586       0.7310586 \n\n\nWith a smaller learning rate, the probabilities are not so extreme, and there is a bigger difference between the larger probabilities and smaller probabilities. Even so, we can anticipate that more learning will cause these weights to keep increasing and the probabilities to increase along with them. How can we address this counterintuitive behavior?\n\n\n11.1.4 Discriminative learning\nThe fault, as it turns out, is not in our stars but in our representations. Recall that we coded the presence/absence of symptoms as 1 or 0. As a result, associative weights can only ever increase with learning. To avoid this, we can instead code the absence of a symptom as -1. By using negative values, we can represent the absence of a feature more explicitly, thereby allowing us to learn to discriminate between the presence or absence of different features.\nFirst, let’s see what our learning event representations look like now:\n\n\nCode\nevent &lt;- matrix(c(\n    -1, -1,  1,  1,\n     1,  1,  1,  1,\n    -1,  1, -1, -1,\n     1,  1,  1,  1,\n     1, -1,  1,  1,\n     1,  1, -1, -1,\n    -1,  1,  1,  1,\n     1, -1, -1, -1\n), nrow = 8, ncol = 4, byrow = TRUE,\ndimnames = list(\n    c(\"EM\", \"RM\", \"JJ\", \"LF\", \"AM\", \"JS\", \"ST\", \"SE\"),\n    c(\"swollen_eyelids\", \"splotchy_ears\", \"discolored_gums\", \"nosebleed\")\n))\n\nknitr::kable(event)\n\n\n\n\n\n\nswollen_eyelids\nsplotchy_ears\ndiscolored_gums\nnosebleed\n\n\n\n\nEM\n-1\n-1\n1\n1\n\n\nRM\n1\n1\n1\n1\n\n\nJJ\n-1\n1\n-1\n-1\n\n\nLF\n1\n1\n1\n1\n\n\nAM\n1\n-1\n1\n1\n\n\nJS\n1\n1\n-1\n-1\n\n\nST\n-1\n1\n1\n1\n\n\nSE\n1\n-1\n-1\n-1\n\n\n\n\n\nNow let’s see what happens when we take the outer product of the first learning event with itself:\n\n\nCode\nouter(event[1,], event[1,])\n\n\n                swollen_eyelids splotchy_ears discolored_gums nosebleed\nswollen_eyelids               1             1              -1        -1\nsplotchy_ears                 1             1              -1        -1\ndiscolored_gums              -1            -1               1         1\nnosebleed                    -1            -1               1         1\n\n\nWe can see that the fact that discolored gums and nosebleed were both present while swollen eyelids and splotchy ears were not will result in a lowering of the associative weight between those two sets of symptoms.\nNow let’s see what the final set of associative weights looks like:\n\n\nCode\n# Initialize weights to zero\nassc_weights &lt;- matrix(0, nrow = ncol(event), ncol = ncol(event))\n\n# Specify learning rate\nlearning_rate &lt;- 1\n\n# Update weights for each event\nfor (i in 1:nrow(event)) {\n    assc_weights &lt;- assc_weights + learning_rate * outer(event[i,], event[i,])\n}\n\nknitr::kable(assc_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\nswollen_eyelids\nsplotchy_ears\ndiscolored_gums\nnosebleed\n\n\n\n\nswollen_eyelids\n8\n0\n0\n0\n\n\nsplotchy_ears\n0\n8\n0\n0\n\n\ndiscolored_gums\n0\n0\n8\n8\n\n\nnosebleed\n0\n0\n8\n8\n\n\n\n\n\nNow if we know a patient has discolored gums, we are very sure that they also have nosebleed and are equivocal about whether or not they have swollen eyelids or splotchy ears, as shown below:\n\n\nCode\n1 / (1 + exp(-assc_weights[\"discolored_gums\",]))\n\n\nswollen_eyelids   splotchy_ears discolored_gums       nosebleed \n      0.5000000       0.5000000       0.9996646       0.9996646 \n\n\nOn the other hand, if we know a patient has swollen eyelids, we don’t have any strong opinions about whether or not they have any other symptoms (though we think it is more likely than not). Note that this is a consequence of the fact that discolored gums and nosebleed did tend to co-occur in our training events, whereas swollen eyelids did not systematically covary with any other symptoms.\n\n\nCode\n1 / (1 + exp(-assc_weights[\"swollen_eyelids\",]))\n\n\nswollen_eyelids   splotchy_ears discolored_gums       nosebleed \n      0.9996646       0.5000000       0.5000000       0.5000000 \n\n\n\n\n11.1.5 Using multiple cues\nSo far, we have only thought about situations in which a single symptom was given for a new patient. What if we know that a patient has 2 symptoms or 3? Or what if we know they do not have a particular symptom? To address these situations, we need to go beyond just looking at a single row at a time.\nTo appreciate what we are about to do, imagine that we knew a patient had discolored gums and that they did not have swollen eyelids. In that case, the strength of support for each feature would be the row corresponding to discolored_gums minus the row corresponding to swollen_eyelids, as shown below:\n\n\nCode\nassc_weights[\"discolored_gums\",] - assc_weights[\"swollen_eyelids\",]\n\n\nswollen_eyelids   splotchy_ears discolored_gums       nosebleed \n             -8               0               8               8 \n\n\nWhen we apply the logistic function to the vector above, we are still equivocal about splotchy ears but we are actually more sure that they have nosebleed:\n\n\nCode\n1 / (1 + exp(-(assc_weights[\"discolored_gums\",] - assc_weights[\"swollen_eyelids\",])))\n\n\nswollen_eyelids   splotchy_ears discolored_gums       nosebleed \n   0.0003353501    0.5000000000    0.9996646499    0.9996646499 \n\n\nWe can make the logic of what we just did to obtain that prediction more general. We will again make use of some linear algebra. Specifically, we will obtain the “strength of support” for each symptom by multiplying the matrix of associative weights with a vector that encodes the presence/absence/missingness of each known symptom. This vector is called a “probe” or a “cue”, and the result will be another vector that gives the total degree of support for each symptom. In math, we can write the operation like this: \\[\n\\mathbf{o} = \\mathbf{c} W\n\\] where \\(\\mathbf{c}\\) is the cue (or probe) vector and \\(\\mathbf{o}\\) is the “output” vector.\nThe example above corresponds to the following cue vector:\n\n\nCode\ncue &lt;- c(-1, 0, 1, 0)\nnames(cue) &lt;- rownames(assc_weights)\n\nprint(cue)\n\n\nswollen_eyelids   splotchy_ears discolored_gums       nosebleed \n             -1               0               1               0 \n\n\nNotice that we use -1 to code for the known absence of a feature, 1 to code for the known presence of a feature, and 0 to code for “missing knowledge” about a feature.\nWe can use the cue vector to “probe” the associative weight matrix. In R, matrix multiplication uses the %*% operator (think of it as a “fancy multiplication”). So the code below directly implements the equation listed above:\n\n\nCode\ncue %*% assc_weights\n\n\n     swollen_eyelids splotchy_ears discolored_gums nosebleed\n[1,]              -8             0               8         8\n\n\nAnd as we hoped, we get the same result from our fancy linear algebra as we did when we did it by hand earlier. The point of introducing this linear algebra now is that it will generalize more readily to situations in which we have more complex event representations (e.g., integral representations).\n\n\n11.1.6 Associating events with actions\nIn the preceding examples, the learner associated features of events with one another, but now we consider how to model learning the relations between features of events and outcomes or actions. In the context of our ongoing medical example, imagine that a new doctor is learning to diagnose whether or not someone has a particular disease based on the pattern of symptoms they display. Now, instead of associating symptoms with symptoms, the doctor needs to associate symptoms with the presence/absence of the disease. Modeling this form of learning will require making two changes to the Hebbian learning model we have been building so far.\nFirst, we need to redefine the associative weight matrix \\(W\\). Before, it was \\(D \\times D\\), where \\(D\\) is the number of dimensions in our learning event representations. Now, it will be \\(D \\times O\\), where \\(O\\) is the number of dimensions used to represent the available actions. Sometimes, as in some of the examples below, \\(O\\) will equal 1 if the learner just needs to decide whether to take an action or not (e.g., whether or not someone has a disease). But in general, \\(O\\) could have many dimensions if the learner can take many actions or if the actions are sufficiently complex that they require a distributed representation. The entry in the \\(i\\)th row and \\(j\\)th column of our new associative weight matrix will represent the strength of association between event dimension \\(i\\) and action/outcome dimension \\(j\\).\nThe other thing we need to do is define, for each learning event, the “label” or “correct answer” that is associated with it. In the model implementation below, we do this by having two matrices: an event matrix that stores the features of each learning event, with one row per event and one column per feature; and a target matrix that stores the “correct answer” for each event, with one row per event and one column per outcome dimension.\nThe example below uses the same set of symptoms as our running example. The first four patients were diagnosed with a disease while the second four were not.\n\n\nCode\nevent &lt;- matrix(c(\n     1,  1,  1,  1,\n    -1,  1, -1, -1,\n     1, -1, -1,  1,\n     1, -1, -1, -1,\n     1, -1,  1,  1,\n    -1, -1,  1, -1,\n    -1,  1, -1,  1,\n    -1, -1, -1, -1\n), nrow = 8, ncol = 4, byrow = TRUE, dimnames = list(\n    c(paste0(\"T\", 1:4), paste0(\"F\", 1:4)),\n    c(\"swollen_eyelids\", \"splotchy_ears\", \"discolored_gums\", \"nosebleed\")\n))\n\ntarget &lt;- matrix(c(\n     1,\n     1,\n     1,\n     1,\n    -1,\n    -1,\n    -1,\n    -1\n), nrow = 8, ncol = 1, byrow = TRUE, dimnames = list(\n    c(paste0(\"T\", 1:4), paste0(\"F\", 1:4)),\n    c(\"Diagnosis\")\n))\n\nassc_weights &lt;- matrix(0, nrow = ncol(event), ncol = ncol(target))\n\nlearning_rate &lt;- 1\n\nfor (i in 1:nrow(event)) {\n    assc_weights &lt;- assc_weights + learning_rate * outer(event[i,], target[i,])\n}\n\nknitr::kable(assc_weights)\n\n\n\n\n\nswollen_eyelids\n4\n\n\nsplotchy_ears\n2\n\n\ndiscolored_gums\n-2\n\n\nnosebleed\n0\n\n\n\n\n\nIn the end, the new doctor has learned that swollen eyelids are strongly diagnostic of the disease, splotchy ears are weakly diagnostic, discolored gums are weakly counter-indicative of the disease, and nosebleed is uninformative.\nNotice that all we needed to do to model this form of associative learning was to swap out event for target. Formally, we can specify the learning procedure like this: \\[\n\\Delta W(t) = \\lambda \\left[ \\mathbf{x}(t) \\otimes \\mathbf{t}(t) \\right]\n\\] where \\(\\lambda\\) and \\(\\mathbf{x}(t)\\) are as defined above and \\(\\mathbf{t}(t)\\) is the vector representation of the target (“correct answer”) for the learning event experienced at time \\(t\\).",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Associative Learning</span>"
    ]
  },
  {
    "objectID": "learning.html#error-driven-learning",
    "href": "learning.html#error-driven-learning",
    "title": "11  Associative Learning",
    "section": "11.2 Error-driven learning",
    "text": "11.2 Error-driven learning\nAs noted above, Hebbian learning—whether of associations between event features or associations between events and actions—was a form of unsupervised learning, since the learner had no explicit goal other than to learn. In supervised learning, a learner again associates features of events with different actions or outcomes. The difference between supervised and unsupervised learning is that, in supervised learning, the adjustments the learner makes to their matrix of associations depends on the error between the target action/outcome and the learner’s guess or prediction about the action/outcome. Each time a learning event occurs, the learner uses the features of that event to form a representation of the action/outcome they select. The learner then receives feedback telling them the correct action/outcome they should have chosen. The goal of the learner is to adjust their associative weights in such a way that they reduce the discrepancy between the action/outcome they pick and the one they are told is correct.\nTo return to our running example, in supervised learning, the new doctor examines the symptoms of a patient report (this is the “learning event”), makes a diagnosis (this is the chosen action/outcome), and then gets told what the correct diagnosis would have been for that patient (this is the feedback). The new doctor then has to adjust the pattern of associations between symptoms (learning event features) and diagnoses (actions/outcomes).\nWe can imagine the same kind of learning occurring in many situations: When you are learning language, you might choose which word to use to describe an object based on its visible features (e.g., calling a black and white creature with four legs a “zebra”) and then someone nearby would either confirm your choice or provide the correct name (e.g., “no, that’s a dalmatian”). When you are learning a skill like using a tool or playing an instrument, you take an action (e.g., pressing a key on a keyboard) and then get feedback about whether it was correct or not (e.g., you hear the correct note you should have played).\nFormally, error-driven learning happens in two steps: \\[\\begin{align*}\n\\mathbf{y}(t) & = f \\left[ \\mathbf{x}(t) W(t) \\right] & \\text{Make a prediction/guess} \\\\\n\\Delta W(t) & = \\lambda \\left\\lbrace \\mathbf{x}(t) \\otimes \\overbrace{\\left[\\mathbf{t}(t) - \\mathbf{y}(t) \\right]}^{\\text{Error}} \\right\\rbrace & \\text{Adjust weights in proportion to error}\n\\end{align*}\\]\nYou may notice the mysterious function \\(f\\) in the first line above. This is because sometimes our prediction is a function of the associative weights and event features, like we saw with the logistic function above. Indeed, the logistic function is a common choice for \\(f\\) because we are often interested in modeling learning where the outcome is a binary decision (e.g., approach/avoid, good/bad, diagnose or not, etc.).\n\n11.2.1 Of salivating dogs and doctors\nThe error-driven learning rule written above is a kind of Delta rule originally proposed by Widrow & Hoff (1960) but applied to animal and human learning by Rescorla & Wagner (1972). In particular, Rescorla & Wagner (1972) showed how error-driven learning could explain various phenomena in classical conditioning, hence the reference in the section title to Pavlov’s salivating dogs.\nConditioning experiments are functionally identical to the disease diagnosis example, along with many other cases in which a learner needs to form a prediction or expectation on the basis of the features of some event.\nThe code below illustrates how error-driven learning can explain extinction. We imagine that a single cue is presented for n_early times at the beginning of the experiment, when it is paired with the presence of some outcome. After that, we present the same cue again for n_late trials, but now it is no longer paired with the outcome. These patterns are built into the event and target matrices below. In addition, because the prediction is about the presence/absence of the outcome, the function \\(f\\) above is the logistic function.\nYou’ll notice too that this code stores the associative weights on each trial in an array. We can then use the array2DF function to make a pretty plot of how the associative weights change from one learning event to the next.\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\nn_early &lt;- 10\nn_late &lt;- 10\n\nevent &lt;- matrix(c(\n    rep(c(1), n_early),\n    rep(c(1), n_late)\n), nrow = n_early + n_late, ncol = 1, byrow = TRUE)\n\ntarget &lt;- matrix(c(\n    rep(c(1), n_early),\n    rep(c(0), n_late)\n), nrow = n_early + n_late, ncol = 1, byrow = TRUE)\n\nassc_weights &lt;- array(0, dim = c(nrow(event) + 1, ncol(event), ncol(target)),\n                      dimnames = list(\n                          \"t\" = 1:(nrow(event) + 1),\n                          \"cue\" = paste(\"cue\", LETTERS[1:ncol(event)]),\n                          \"outcome\" = paste(\"outcome\", LETTERS[1:ncol(target)])\n                      ))\n\nlearning_rate &lt;- 0.5\n\nfor (i in 1:nrow(event)) {\n    prediction &lt;- as.vector(event[i,] %*% assc_weights[i, , ])\n    assc_weights[i + 1, , ] &lt;- assc_weights[i, , ] + learning_rate * outer(event[i,], target[i,] - prediction)\n}\n\narray2DF(assc_weights, responseName = \"weight\") %&gt;%\n    mutate(t = as.numeric(t)) %&gt;%\n    ggplot(aes(x = t, y = weight)) +\n    geom_line() +\n    labs(x = \"Learning event\", y = \"Associative weight between cue and outcome\")\n\n\n\n\n\n\n\n\n\nAs we can see, the associative weight between the cue and outcome increases at first, eventually saturating when the presence of the outcome is correctly predicted by the presence of the cue. When the outcome stops happening, now that prediction is wrong and the associative weight diminishes in response, eventually returning to near zero.\nThe next example of the Rescorla & Wagner (1972) model in action illustrates learning two different associations, one between an early cue and an early outcome and another between a late cue and a late outcome. Since associative weights only change in order to reduce error, the early-learned weights are preserved through the later phase of learning.\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\nn_early &lt;- 10\nn_late &lt;- 10\n\nevent &lt;- matrix(c(\n    rep(c(1, 0), n_early),\n    rep(c(0, 1), n_late)\n), nrow = n_early + n_late, ncol = 2, byrow = TRUE)\n\ntarget &lt;- matrix(c(\n    rep(c(1, 0), n_early),\n    rep(c(0, 1), n_late)\n), nrow = n_early + n_late, ncol = 2, byrow = TRUE)\n\nassc_weights &lt;- array(0, dim = c(nrow(event) + 1, ncol(event), ncol(target)),\n                      dimnames = list(\n                          \"t\" = 1:(nrow(event) + 1),\n                          \"cue\" = paste(\"cue\", LETTERS[1:ncol(event)]),\n                          \"outcome\" = paste(\"outcome\", LETTERS[1:ncol(target)])\n                      ))\n\nlearning_rate &lt;- 0.5\n\nfor (i in 1:nrow(event)) {\n    prediction &lt;- as.vector(event[i,] %*% assc_weights[i, , ])\n    assc_weights[i + 1, , ] &lt;- assc_weights[i, , ] + learning_rate * outer(event[i,], target[i,] - prediction)\n}\n\narray2DF(assc_weights, responseName = \"weight\") %&gt;%\n    mutate(t = as.numeric(t)) %&gt;%\n    ggplot(aes(x = t, y = weight, color = cue, linetype = outcome)) +\n    geom_line() +\n    labs(x = \"Learning event\", y = \"Associative weight between cue and outcome\")\n\n\n\n\n\n\n\n\n\nThe following is a more complex learning situation known as a blocking paradigm (Kruschke, 2011). In the early phase, outcome A is paired with cue A and outcome B is paired with cue E. During the later phase, outcome A is paired with both cue A and cue B while outcome B is paired with cues C and D. The question is: Does the fact that cue A and outcome A had already been paired in the early phase block learning of an association between cue B and outcome A? That question is addressed by presenting transfer trials to the learner, coded below in the matrix transfer_event.\nThe first transfer trial involves presenting cues B and D to the learner. If cue B had been blocked, the learner should have a stronger expectation that outcome B would occur over outcome A.\nThe second transfer trial involves presenting cues A and C to the learner. If cue B had been blocked, then cue A should remain a stronger predictor of outcome A relative to how strongly cue C predicts outcome B.\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\nn_early &lt;- 10\nn_late &lt;- 10\n\nevent &lt;- matrix(c(\n    rep(c(1, 0, 0, 0, 0), n_early),\n    rep(c(0, 0, 0, 0, 1), n_early),\n    rep(c(1, 1, 0, 0, 0), n_late),\n    rep(c(0, 0, 1, 1, 0), n_late)\n), nrow = 2 * (n_early + n_late), ncol = 5, byrow = TRUE)\n\ntarget &lt;- matrix(c(\n    rep(c(1, 0), n_early),\n    rep(c(0, 1), n_early),\n    rep(c(1, 0), n_late),\n    rep(c(0, 1), n_late)\n), nrow = 2 * (n_early + n_late), ncol = 2, byrow = TRUE)\n\nassc_weights &lt;- array(0, dim = c(nrow(event) + 1, ncol(event), ncol(target)),\n                      dimnames = list(\n                          \"t\" = 1:(nrow(event) + 1),\n                          \"cue\" = paste(\"cue\", LETTERS[1:ncol(event)]),\n                          \"outcome\" = paste(\"outcome\", LETTERS[1:ncol(target)])\n                      ))\n\nlearning_rate &lt;- 0.2\n\nfor (i in 1:nrow(event)) {\n    prediction &lt;- as.vector(event[i,] %*% assc_weights[i, , ])\n    assc_weights[i + 1, , ] &lt;- assc_weights[i, , ] + learning_rate * outer(event[i,], target[i,] - prediction)\n}\n\narray2DF(assc_weights, responseName = \"weight\") %&gt;%\n    mutate(t = as.numeric(t)) %&gt;%\n    ggplot(aes(x = t, y = weight, color = cue, linetype = outcome)) +\n    geom_line()\n\n\n\n\n\n\n\n\n\nCode\ntransfer_event &lt;- matrix(c(\n    0, 1, 0, 1, 0,\n    1, 0, 1, 0, 0\n), nrow = 2, ncol = 5, byrow = TRUE)\n\ntransfer_prediction &lt;- matrix(0, nrow = nrow(transfer_event), ncol = ncol(target))\n\nfor (i in 1:nrow(transfer_event)) {\n    transfer_prediction[i,] &lt;- as.vector(transfer_event[i,] %*% assc_weights[dim(assc_weights)[1], , ])\n}\n\ntransfer_prediction\n\n\n           [,1]      [,2]\n[1,] 0.05336247 0.4969767\n[2,] 0.94598828 0.4969767\n\n\nWe can see that, according to the Rescorla & Wagner (1972) model, learning of the association between cue B and outcome A has indeed been “blocked”. The reason is because the previously learned association between cue A and outcome A means that there is simply no need to learn a new association between cue B and outcome A—the learner is already capable of predicting the correct outcome. The result is that, as predicted by blocking, the learner is more likely to predict outcome B in the first transfer trial and is more likely to predict outcome A in the second transfer trial.\n\n\n11.2.2 Batch learning\nApplying the Rescorla & Wagner (1972) model to our novice doctor presents a challenge, namely, that the order in which events are encountered is no longer relevant to the doctor, since they can read the reports in any order. To model this form of learning, instead of updating the matrix of associative weights after each event, we can accumulate the changes across all events and update them all at once. This is called batch learning because the learner experiences a batch of events. This is a common approach in neural network models, as we shall see below.\nBatch learning is implemented in the chunk of code below, which also allows for the number of batches n_batch to be varied. Notice that error-driven learning can also achieve discriminative learning without needing to code event features as -1 and 1.\n\n\nCode\nevent &lt;- matrix(c(\n     1,  1,  1,  1,\n    0,  1, 0, 0,\n     1, 0, 0,  1,\n     1, 0, 0, 0,\n     1, 0,  1,  1,\n    0, 0,  1, 0,\n    0,  1, 0,  1,\n    0, 0, 0, 0\n), nrow = 8, ncol = 4, byrow = TRUE, dimnames = list(\n    c(paste0(\"T\", 1:4), paste0(\"F\", 1:4)),\n    c(\"swollen_eyelids\", \"splotchy_ears\", \"discolored_gums\", \"nosebleed\")\n))\n\ntarget &lt;- matrix(c(\n     1,\n     1,\n     1,\n     1,\n    0,\n    0,\n    0,\n    0\n), nrow = 8, ncol = 1, byrow = TRUE, dimnames = list(\n    c(paste0(\"T\", 1:4), paste0(\"F\", 1:4)),\n    c(\"Diagnosis\")\n))\n\nlearning_rate &lt;- 0.1\nn_batch &lt;- 30\n\nassc_weights &lt;- array(0, dim = c(n_batch + 1, ncol(event), ncol(target)),\n                      dimnames = list(\n                          \"batch\" = 1:(n_batch + 1),\n                          \"cue\" = colnames(event),\n                          \"outcome\" = colnames(target)\n                      ))\n\nfor (b in 1:n_batch) {\n    d_assc_weights &lt;- matrix(0, nrow = dim(assc_weights)[2], ncol = dim(assc_weights)[3])\n    \n    for (i in 1:nrow(event)) {\n        prediction &lt;- logistic_act(as.vector(event[i,] %*% assc_weights[i, , ]))\n        d_assc_weights &lt;- d_assc_weights + learning_rate * outer(event[i,], target[i,] - prediction)\n    }\n    \n    assc_weights[b + 1, , ] &lt;- assc_weights[b, , ] + d_assc_weights\n}\n\narray2DF(assc_weights, responseName = \"weight\") %&gt;%\n    mutate(batch = as.numeric(batch)) %&gt;%\n    ggplot(aes(x = batch, y = weight, color = cue)) +\n    geom_line() +\n    labs(x = \"Batch number\", y = \"Associative weight\")\n\n\n\n\n\n\n\n\n\n\n\n11.2.3 Attention and learning\nSince the Rescorla & Wagner (1972) model has been doing well so far, let’s present another learning situation to it. This situation is called highlighting (Kruschke, 2009) and, like blocking, involves an early and late set of learning events. In the early events, cues A and B are both paired with outcome A. In the late events, cues A and C are both paired with outcome B. The critical thing is that cue A is paired with both outcomes with equal frequency. If a learner were only sensitive to those frequencies, then cue A would be equally associated with both outcomes. Moreover, the learner would be ambivalent if presented with both cues B and C, since each had been paired with their respective outcomes an equal number of times. As you might guess, those two situations constitute the transfer_events in the code below.\nHowever, animals, whether human or otherwise, do not show the pattern of results described above. In fact, learners tend to associate cue A with the early outcome more than the late outcome. And when confronted with both cues B and C, learners are more likely to expect the later outcome (outcome B) over the early one. This phenomenon is referred to as highlighting since the novelty of cue C, when it appears, seems to “highlight” it as the “true predictor” of outcome B. For the same reason, learners tend not to “unlearn” the early-learned association between cue A and outcome A.\nLet’s see whether Rescorla & Wagner (1972) correctly predict this outcome…\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\nn_early &lt;- 10\nn_late &lt;- 10\n\nevent &lt;- matrix(c(\n    rep(c(1, 1, 0), n_early),\n    rep(c(1, 0, 1), n_late)\n), nrow = n_early + n_late, ncol = 3, byrow = TRUE)\n\ntarget &lt;- matrix(c(\n    rep(c(1, 0), n_early),\n    rep(c(0, 1), n_late)\n), nrow = n_early + n_late, ncol = 2, byrow = TRUE)\n\nassc_weights &lt;- array(0, dim = c(nrow(event) + 1, ncol(event), ncol(target)),\n                      dimnames = list(\n                          \"t\" = 1:(nrow(event) + 1),\n                          \"cue\" = paste(\"cue\", LETTERS[1:ncol(event)]),\n                          \"outcome\" = paste(\"outcome\", LETTERS[1:ncol(target)])\n                      ))\n\nlearning_rate &lt;- 0.2\n\nfor (i in 1:nrow(event)) {\n    prediction &lt;- as.vector(event[i,] %*% assc_weights[i, , ])\n    assc_weights[i + 1, , ] &lt;- assc_weights[i, , ] + learning_rate * outer(event[i,], target[i,] - prediction)\n}\n\narray2DF(assc_weights, responseName = \"weight\") %&gt;%\n    mutate(t = as.numeric(t)) %&gt;%\n    ggplot(aes(x = t, y = weight, color = cue, linetype = outcome)) +\n    geom_line()\n\n\n\n\n\n\n\n\n\nCode\ntransfer_event &lt;- matrix(c(\n    1, 0, 0,\n    0, 1, 1\n), nrow = 2, ncol = 3, byrow = TRUE)\n\ntransfer_prediction &lt;- matrix(0, nrow = nrow(transfer_event), ncol = ncol(target))\n\nfor (i in 1:nrow(transfer_event)) {\n    transfer_prediction[i,] &lt;- as.vector(transfer_event[i,] %*% assc_weights[dim(assc_weights)[1], , ])\n}\n\ntransfer_prediction\n\n\n          [,1]      [,2]\n[1,] 0.2499909 0.4969767\n[2,] 0.2499909 0.4969767\n\n\nUnfortunately for Rescorla & Wagner (1972), their error-driven learning model cannot predict highlighting. To address this, Mackintosh (1975) suggested that the learning rates in the model may depend on the relative predictive strength of the available cues on each trial. This is a form of selective attention, in that it predicts that attention will be drawn to cues that do not yet have any strong associations. In other words, when the second learning phase begins, attention is drawn to cue C because it has no strong associations whereas attention is drawn away from cue A, since it is already strongly associated with outcome A.\nFormally, incorporating attention into error-driven learning involves introducing an associative weight on each cue which determines the learning rate on each event. In the code below, the associative weight for each cue is the sum of the corresponding row in the associative weight matrix. The attention weight on each cue is adjusted in proportion to the difference between its associative weight and the sum of the associative weights for the other cues present in the current event. Note that, in the model below, the learning_rate parameter refers to the rate at which attention weights are adjusted from one event to the next. These attention weights are, in turn, the degree to which associative weights are adjusted from one event to the next.\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\nn_early &lt;- 10\nn_late &lt;- 10\n\nevent &lt;- matrix(c(\n    rep(c(1, 1, 0), n_early),\n    rep(c(1, 0, 1), n_late)\n), nrow = n_early + n_late, ncol = 3, byrow = TRUE)\n\ntarget &lt;- matrix(c(\n    rep(c(1, 0), n_early),\n    rep(c(0, 1), n_late)\n), nrow = n_early + n_late, ncol = 2, byrow = TRUE)\n\nassc_weights &lt;- array(0, dim = c(nrow(event) + 1, ncol(event), ncol(target)),\n                      dimnames = list(\n                          \"t\" = 1:(nrow(event) + 1),\n                          \"cue\" = paste(\"cue\", LETTERS[1:ncol(event)]),\n                          \"outcome\" = paste(\"outcome\", LETTERS[1:ncol(target)])\n                      ))\n\nattn_weights &lt;- matrix(0.2, nrow = nrow(event) + 1, ncol = ncol(event),\n                       dimnames = list(\n                           \"t\" = 1:(nrow(event) + 1),\n                           \"cue\" = paste(\"cue\", LETTERS[1:ncol(event)])\n                       ))\n\nlearning_rate &lt;- 0.2\n\nfor (i in 1:nrow(event)) {\n    prediction &lt;- as.vector(event[i,] %*% assc_weights[i, , ])\n    \n    self_prediction_strength &lt;- rep(0, ncol(event))\n    other_prediction_strength &lt;- rep(0, ncol(event))\n    \n    for (j in 1:ncol(event)) {\n        self_prediction_strength[j] &lt;- sum(assc_weights[i, j, ])\n        other_prediction_strength[j] &lt;- sum(event[i, -j] * assc_weights[i, -j, ])\n    }\n    \n    attn_weights[i + 1, ] &lt;- pmax(0, attn_weights[i, ] + learning_rate * event[i,] * (other_prediction_strength - self_prediction_strength))\n    assc_weights[i + 1, , ] &lt;- assc_weights[i, , ] + outer(attn_weights[i, ] * event[i,], target[i,] - prediction)\n}\n\narray2DF(assc_weights, responseName = \"weight\") %&gt;%\n    mutate(t = as.numeric(t)) %&gt;%\n    ggplot(aes(x = t, y = weight, color = cue, linetype = outcome)) +\n    geom_line()\n\n\n\n\n\n\n\n\n\nCode\narray2DF(attn_weights, responseName = \"attention\") %&gt;%\n    mutate(t = as.numeric(t)) %&gt;%\n    ggplot(aes(x = t, y = attention, color = cue)) +\n    geom_line()\n\n\n\n\n\n\n\n\n\nCode\ntransfer_event &lt;- matrix(c(\n    1, 0, 0,\n    0, 1, 1\n), nrow = 2, ncol = 3, byrow = TRUE)\n\ntransfer_prediction &lt;- matrix(0, nrow = nrow(transfer_event), ncol = ncol(target))\n\nfor (i in 1:nrow(transfer_event)) {\n    transfer_prediction[i,] &lt;- as.vector(transfer_event[i,] %*% assc_weights[dim(assc_weights)[1], , ])\n}\n\ntransfer_prediction\n\n\n          [,1]      [,2]\n[1,] 0.3673661 0.2607982\n[2,] 0.1296393 0.7391441\n\n\nAs we can see, incorporating a role for attention produces the highlighting effect. When cue C appears, the attention weight on cue A rapidly diminishes, such that its association with outcome A is preserved through the second half of learning. On the other hand, the extra attention devoted to the novel cue C means that its association with outcome B is stronger than the association between cue B and outcome A.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Associative Learning</span>"
    ]
  },
  {
    "objectID": "learning.html#exercises",
    "href": "learning.html#exercises",
    "title": "11  Associative Learning",
    "section": "11.3 Exercises",
    "text": "11.3 Exercises\n\nWithin the context of a model like the EBRW, storing a new exemplar as a trace in memory could be considered a form of learning, in the sense that the performance of the model changes as a function of having experienced that new exemplar. Compare and contrast this form of learning with the forms of learning we encountered in this chapter.\nDescribe how you would use the output of any of the learning models we’ve seen in this chapter to predict not only choice behavior, but also choice and response time.\nTry running the two-cue-two-outcome Rescorla-Wagner model from earlier in the chapter, but instead of coding the events as c(1, 0) for the early cues and c(0, 1) for the later cues, try using c(-1, 1) for the later cues. Describe what happens to the associative weight between cue A and outcome B. Describe a situation that might produce similar learning trajectories.\nIn the chapter, we saw that one way to allocate attention to cues during learning is based on the idea that attention is attracted to cues which are not yet strongly associated to anything else. What other principles could govern how attention is allocated to different cues during learning?\n\n\n\n\n\nHebb, D. O. (1949). The organization of behavior: A neuropsychological theory. Wiley.\n\n\nKruschke, J. K. (2009). Highlighting: A canonical experiment. In Psychology of learning and motivation (Vol. 51, pp. 153–185). Elsevier.\n\n\nKruschke, J. K. (2011). Models of attentional learning. In E. M. Pothos & A. J. Wills (Eds.), Formal approaches in categorization (pp. 120–152). Cambridge University Press.\n\n\nMackintosh, N. J. (1975). A theory of attention: Variations in the associability of stimuli with reinforcement. Psychological Review, 82(4), 276–298.\n\n\nMedin, D. L., Altom, M. W., Edelson, S. M., & Freko, D. (1982). Correlated symptoms and simulated medical classification. Journal of Experimental Psychology: Learning, Memory, and Cognition, 8(1), 37–50. https://www.proquest.com/scholarly-journals/correlated-symptoms-simulated-medical/docview/614362118/se-2\n\n\nRescorla, R. A., & Wagner, A. R. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. In A. H. Black & W. F. Prokasy (Eds.), Classical conditioning II: Current research and theory (pp. 64–99). Appleton–Century–Crofts.\n\n\nWidrow, B., & Hoff, M. E. (1960). Adaptive switching circuits. IRE WESCON Convention Record, 96–104.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Associative Learning</span>"
    ]
  },
  {
    "objectID": "backprop.html",
    "href": "backprop.html",
    "title": "12  Learning useful representations",
    "section": "",
    "text": "12.1 Backpropagation of error\nIn the previous chapter, we looked at models that learned by adjusting a matrix of associative weights between two sets of nodes. In those models, knowledge was represented by the entries in that matrix, reflecting the degree to which event features would “activate” other nodes. We saw that one powerful form of learning was error-driven learning where weight adjustments were made explicitly for the purpose of reducing the error between the learner’s predictions/expectations and what actually transpired.\nThe models in this chapter also learn by attempting to minimize error and they also do so by adjusting entries in associative weight matrices. However, these models add a so-called “hidden layer” that intervenes between the event features that act as “input” and the predictions/expectations that act as “output”. This category of model was the core of the connectionist or parallel distributed processing (PDP) framework.\nThese models learn via an extended form of error-driven learning called backpropagation of error (Rumelhart et al., 1986). This learning procedure enables a model to learn how to form an internal representation of the events it encounters, in the form of a pattern of activation across the nodes of its “hidden layer”. Such representations are a form of integral distributed vector representation. The purpose of these representations is to re-encode the features of an event into a form that enables the model to make more correct predictions/actions.\nThe kinds of models we will see in this chapter can thus be viewed as psychological theories of how distributed vector representations are acquired via experience. As we have already seen these representations are themselves essential for understanding performance (choice and RT) in different cognitive tasks. So these models “peel back” one more layer of the cognitive onion to help us understand where those representations may have come from.\nUnfortunately, as we shall see (and as was explicitly stated by the developers of these models), learning by backpropagation of error is not very biologically plausible. So while these models are often called “artificial neural networks”, the resemblance between them and real neurons should be viewed in only an abstract sense.\nAnother drawback with these models is that learning requires extensive experience before the associative weights in the network stabilize and stop getting adjusted. As a result, this form of learning should not be thought of as something that happens on short timescales. Indeed, it has been proposed by several authors that biological brains have two complementary learning systems (McClelland et al., 1995). There is a “fast” learning system—typically thought to depend on the hippocampus and other medial temporal lobe structures—that encodes distinct events as separate traces, like we saw with the EBRW. Meanwhile, there is also a “slow” learning system—typically thought to depend on cortical areas—that does the kind of gradual weight adjustment we will see in the models in this chapter. The fast learning system encodes separate traces for distinct events using distributed representations learned by the slow learning system.\nThe models we will examine in this chapter all have the basic structure depicted in the graph below:\nG\n\n\n\ni1\n\ni1\n\n\n\nh1\n\nh1\n\n\n\ni1-&gt;h1\n\n\n\n\n\nh2\n\nh2\n\n\n\ni1-&gt;h2\n\n\n\n\n\nh3\n\nh3\n\n\n\ni1-&gt;h3\n\n\n\n\n\no1\n\no1\n\n\n\nh1-&gt;o1\n\n\n\n\n\no2\n\no2\n\n\n\nh1-&gt;o2\n\n\n\n\n\nh2-&gt;o1\n\n\n\n\n\nh2-&gt;o2\n\n\n\n\n\nh3-&gt;o1\n\n\n\n\n\nh3-&gt;o2\n\n\n\n\n\ni2\n\ni2\n\n\n\ni2-&gt;h1\n\n\n\n\n\ni2-&gt;h2\n\n\n\n\n\ni2-&gt;h3\n\n\n\n\n\ni3\n\ni3\n\n\n\ni3-&gt;h1\n\n\n\n\n\ni3-&gt;h2\n\n\n\n\n\ni3-&gt;h3\n\n\n\n\n\ni4\n\ni4\n\n\n\ni4-&gt;h1\n\n\n\n\n\ni4-&gt;h2\n\n\n\n\n\ni4-&gt;h3\nThe circles represent the elements of different vector representations; each representation is referred to as a layer. The “i” nodes are elements of a vector that represents the “input”—this is a representation of a learning event, just like in the last chapter. The “o” nodes are elements of a vector that represents the action/prediction/expectation that the learner takes, again like we saw in the last chapter. The new thing are the “h” nodes, which represent the network’s internal (hidden) representation of the learning event.\nThe links between the nodes depict associative weights between them. In the previous chapter, we only had associative weights between “input” and “output”. In the models in this chapter, we consider two sets of associations represented by two matrices of associative weights. If there are \\(D\\) input dimensions, \\(H\\) hidden dimensions, and \\(O\\) output dimensions, we can write the matrix of weights from input to hidden as \\(W_{ih}\\) and the matrix of weights from hidden to output as \\(W_{ho}\\).",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Learning useful representations</span>"
    ]
  },
  {
    "objectID": "backprop.html#backpropagation-of-error",
    "href": "backprop.html#backpropagation-of-error",
    "title": "12  Learning useful representations",
    "section": "",
    "text": "12.1.1 Forward propagation\nJust like in the last chapter, we treat the pattern of activation on the output vector as a representation of what the learner expects or plans to do. The learner forms this representation by “propagating” the activity from the input layer to the hidden layer and then from the hidden layer to the output layer. Like last time, we can write that propagation process using some linear algebra: \\[\\begin{align}\n\\mathbf{h} & = f_h \\left( \\mathbf{x} W_{ih} \\right) & \\text{Input to hidden} \\\\\n\\mathbf{o} & = f_o \\left( \\mathbf{h} W_{ho} \\right) & \\text{Hidden to output}\n\\end{align}\\] where \\(\\mathbf{x}\\) is the vector representing the learning event, \\(\\mathbf{h}\\) is the vector representing the pattern of activation at the hidden layer, and \\(\\mathbf{o}\\) is the vector representing the pattern of activation at the output layer. Notice that we allow for the activity at the hidden and output layers to be functions of their corresponding “inputs”, indicated by \\(f_h \\left( \\cdot \\right)\\) and \\(f_o \\left( \\cdot \\right)\\). In this chapter, we will only look at models where both \\(f_h\\) and \\(f_o\\) are the logistic function we used last chapter, i.e., \\[\nf_h(a) = f_o(a) = \\frac{1}{1 + \\exp \\left(-a \\right)}\n\\]\n\n\n12.1.2 Backward propagation\nThe procedure above describes how the model forms an expectation \\(\\mathbf{o}\\) on the basis of the event features \\(\\mathbf{x}\\). As we saw last time, the vector \\(\\mathbf{o}\\) can be treated as a vector of probabilities with which each output feature is expected to be present or not (since we are using the logistic function).\nThe model learns by first computing the error between its expectation and a “target” representation and then propagating that error signal backwards along the associative weights. That’s why this learning procedure is called “backward propagation”, or “backprop” for short (not to be confused with a lumbar support pillow).\nTo be more mathy about it, backprop essentially adds one step to the error-driven learning procedure we saw last time. The additional step is because we have to adjust two sets of associative weights, \\(W_{ho}\\) and \\(W_{ih}\\). The equations below spell out the process: \\[\\begin{align*}\n\\mathbf{e_o} & = \\mathbf{t} - \\mathbf{o} & \\text{Error between target and output} \\\\\n\\mathbf{d_o} & = \\mathbf{e_o} \\mathbf{o} \\left(1 - \\mathbf{o} \\right) & \\text{Derivative of error with respect to output activation} \\\\\n\\Delta W_{ho} & = \\lambda_{ho} \\left( \\mathbf{h} \\otimes \\mathbf{d_o} \\right) & \\text{How much to adjust hidden to output weights} \\\\\n\\mathbf{d_h} & = W_{ho} \\mathbf{e_o} \\mathbf{h} \\left(1 - \\mathbf{h} \\right) & \\text{Derivative of error with respect to hidden activation} \\\\\n\\Delta W_{ih} & = \\lambda_{ih} \\left( \\mathbf{x} \\otimes \\mathbf{d_h} \\right) & \\text{How much to adjust input to hidden weights}\n\\end{align*}\\] You may notice above some stuff about “derivatives”, but this is not so mysterious as at may first appear. We were able to avoid mentioning derivatives in the last chapter because we only needed to worry about how to adjust a single set of weights. But really what we were doing was computing how much the error would change if we adjusted each associative weight. That’s a derivative! Now that we have two sets of weights to learn, we need to consider how much each weight contributes to the total error at the output layer. Backprop just expands this concept of the derivative so that it can assign “blame” for the error to each weight.\nYou may notice the terms \\(\\mathbf{o} \\left(1 - \\mathbf{o} \\right)\\) and \\(\\mathbf{h} \\left(1 - \\mathbf{h} \\right)\\) in the derivative formulae above. That’s because we also need to consider the derivative of the link function for each layer. The derivative of the logistic function \\(f \\left( a \\right)\\), which we are using for both the hidden and output layers, is just \\(f \\left(a \\right) \\left[1 - f \\left(a \\right) \\right]\\), hence the appearance of those terms in the formulae above.\nFinally, although we could use different learning rates for input-to-hidden and hidden-to-output weights, denoted \\(\\lambda_{ih}\\) and \\(\\lambda_{ho}\\) respectively, in the models below we will just use a single learning rate \\(\\lambda\\) that applies to all weights.\n\n\n12.1.3 Baseline activations\nWe will introduce one more aspect to these models that we did not employ earlier, which is the use of a “baseline” activity level for both the hidden and output layers. We can think of this baseline as like an intercept term in linear regression. And just like in linear regression, the intercept can be thought of as an “input” that has a constant value. These baseline levels will be learned just like the weights are, i.e., they will be adjusted so as to minimize error. Specifically, the learning rule is: \\[\\begin{align*}\n\\Delta \\mathbf{b_h} & = \\lambda \\mathbf{d_h} \\\\\n\\Delta \\mathbf{b_o} & = \\lambda \\mathbf{d_o}\n\\end{align*}\\] where \\(\\mathbf{b_h}\\) and \\(\\mathbf{b_o}\\) are the baselines for the hidden and output units, respectively, and \\(\\mathbf{d_h}\\) and \\(\\mathbf{d_o}\\) are the vectors of error derivatives defined above.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Learning useful representations</span>"
    ]
  },
  {
    "objectID": "backprop.html#learning-concept-representations-from-properties",
    "href": "backprop.html#learning-concept-representations-from-properties",
    "title": "12  Learning useful representations",
    "section": "12.2 Learning concept representations from properties",
    "text": "12.2 Learning concept representations from properties\nNow let’s see how one of these models works in a setting we have already seen. The example below shows how a simple backprop network can learn to represent the different concepts we saw in some of our preceding examples. Specifically, we will train the network to learn the features of different concepts. Each learning event will be represented with a localist vector representation of a concept. The output that the network will be trained to produce is a separable vector representation of the features of that concept. Thus, the network will learn to report the features of a concept, given the concept as a “cue”. We can think of this as like learning to provide a dictionary definition of a term.\nWhat we are particularly interested in is the dynamics by which the network learns representations for the eight concepts. As we shall see, the representation that the network learns in its hidden layer will distinguish between broad categories early on and later come to make more fine-grained distinctions. This gradual differentiation over the course of learning is a natural consequence of error-driven learning: Since the biggest errors are due to the biggest confusions, those will be learned first.\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\n# Localist representations of eight concepts\nevent &lt;- matrix(c(\n     1, 0, 0, 0, 0, 0, 0, 0,\n     0, 1, 0, 0, 0, 0, 0, 0,\n     0, 0, 1, 0, 0, 0, 0, 0,\n     0, 0, 0, 1, 0, 0, 0, 0,\n     0, 0, 0, 0, 1, 0, 0, 0,\n     0, 0, 0, 0, 0, 1, 0, 0,\n     0, 0, 0, 0, 0, 0, 1, 0,\n     0, 0, 0, 0, 0, 0, 0, 1\n), nrow = 8, ncol = 8, byrow = TRUE, dimnames = list(\n    c(\"pine\", \"oak\", \"rose\", \"daisy\", \"robin\", \"canary\", \"sunfish\", \"salmon\"),\n    c(\"pine\", \"oak\", \"rose\", \"daisy\", \"robin\", \"canary\", \"sunfish\", \"salmon\")\n))\n\n# Separable representations of concept features\ntarget &lt;- matrix(c(\n     1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n     0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n     0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n     0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n     0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n     0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n     0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n     0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1\n), nrow = 8, ncol = 22, byrow = TRUE, dimnames = list(\n    c(\"pine\", \"oak\", \"rose\", \"daisy\", \"robin\", \"canary\", \"sunfish\", \"salmon\"),\n    c(\"is_green\", \"has_leaves\", \"is_red\", \"is_yellow\", \"can_sing\", \"has_bark\", \"is_big\", \"has_branches\", \"has_leaves\", \"has_petals\", \"is_pretty\", \"has_feathers\", \"can_fly\", \"has_wings\", \"has_scales\", \"can_swim\", \"has_gills\", \"has_roots\", \"can_move\", \"has_skin\", \"can_grow\", \"is_living\")\n))\n\nlearning_rate &lt;- 0.05\nn_hidden &lt;- 6\nn_epochs &lt;- 5000\nlearn_by_epoch &lt;- TRUE\n\nn_hidden_snapshots &lt;- 9\nepoch_to_snap &lt;- round(exp(seq(0, log(n_epochs), length.out = n_hidden_snapshots)))\nsnapshot_hidden_activation &lt;- array(0, dim = c(n_hidden_snapshots, nrow(event), n_hidden), dimnames = list(\"epoch\" = epoch_to_snap, \"item\" = rownames(event), \"hidden_unit\" = 1:n_hidden))\n\ni_to_h_assc_weights &lt;- matrix(rnorm(n = ncol(event) * n_hidden), nrow = ncol(event), ncol = n_hidden) * 0.01\nh_to_o_assc_weights &lt;- matrix(rnorm(n = n_hidden * ncol(target)), nrow = n_hidden, ncol = ncol(target)) * 0.01\n\nhidden_baseline &lt;- rnorm(n = n_hidden) * 0.01\noutput_baseline &lt;- rnorm(n = ncol(target)) * 0.01\n\nfor (epoch in 1:n_epochs) {\n    training_order &lt;- sample(nrow(event))\n\n    d_h_to_o &lt;- 0 * h_to_o_assc_weights\n    d_i_to_h &lt;- 0 * i_to_h_assc_weights\n    \n    d_h_baseline &lt;- 0 * hidden_baseline\n    d_o_baseline &lt;- 0 * output_baseline\n\n    for (i in training_order) {\n        # Forward propagation (prediction)\n\n        # Note the use of `as.vector` to ensure result is a vector (not a matrix with a single row)\n        hidden_activation &lt;- logistic_act(as.vector(event[i,] %*% i_to_h_assc_weights) + hidden_baseline)\n        output_activation &lt;- logistic_act(as.vector(hidden_activation %*% h_to_o_assc_weights) + output_baseline)\n\n        # Compute error at output layer\n        output_error &lt;- target[i,] - output_activation\n\n        # Back propagation (learning)\n\n        # Derivative of error with respect to output unit activation\n        d_output_error &lt;- output_error * output_activation * (1 - output_activation)\n\n        # Derivative of error with respect to hidden unit activation\n        d_hidden_error &lt;- as.vector(h_to_o_assc_weights %*% d_output_error) * hidden_activation * (1 - hidden_activation)\n\n        # How much to adjust the associative weights between the hidden and output layers\n        d_h_to_o &lt;- d_h_to_o + learning_rate * outer(hidden_activation, d_output_error)\n\n        # How much to adjust the associative weights between the input and hidden layers\n        d_i_to_h &lt;- d_i_to_h + learning_rate * outer(event[i,], d_hidden_error)\n        \n        # How much to adjust the baseline of the output units\n        d_o_baseline &lt;- learning_rate * d_output_error\n        \n        # How much to adjust the baseline of the hidden units\n        d_h_baseline &lt;- learning_rate * d_hidden_error\n\n        if (!learn_by_epoch) {\n            # Adjust weights\n            i_to_h_assc_weights &lt;- i_to_h_assc_weights + d_i_to_h\n            h_to_o_assc_weights &lt;- h_to_o_assc_weights + d_h_to_o\n            \n            hidden_baseline &lt;- hidden_baseline + d_h_baseline\n            output_baseline &lt;- output_baseline + d_o_baseline\n\n            # Reset adjustment to zero\n            d_h_to_o &lt;- 0 * h_to_o_assc_weights\n            d_i_to_h &lt;- 0 * i_to_h_assc_weights\n            \n            d_h_baseline &lt;- 0 * hidden_baseline\n            d_o_baseline &lt;- 0 * output_baseline\n        }\n    }\n\n    if (learn_by_epoch) {\n        # Adjust weights\n        i_to_h_assc_weights &lt;- i_to_h_assc_weights + d_i_to_h\n        h_to_o_assc_weights &lt;- h_to_o_assc_weights + d_h_to_o\n        \n        hidden_baseline &lt;- hidden_baseline + d_h_baseline\n        output_baseline &lt;- output_baseline + d_o_baseline\n    }\n    \n    if (epoch %in% as.numeric(dimnames(snapshot_hidden_activation)[[1]])) {\n        for (i in 1:nrow(event)) {\n            snapshot_hidden_activation[as.character(epoch), i, ] &lt;- logistic_act(as.vector(event[i,] %*% i_to_h_assc_weights) + hidden_baseline)\n        }\n    }\n}\n\narray2DF(snapshot_hidden_activation) %&gt;%\n    mutate(item = factor(item, levels = rownames(event))) %&gt;%\n    mutate(hidden_unit = as.numeric(hidden_unit)) %&gt;%\n    mutate(epoch = factor(epoch, levels = as.character(sort(unique(as.numeric(epoch)))))) %&gt;%\n    ggplot(aes(x = hidden_unit, y = Value, color = item)) +\n    geom_line() +\n    scale_color_brewer(palette = \"Paired\") +\n    facet_wrap(\"epoch\", labeller = label_both)\n\n\n\n\n\n\n\n\n\nCode\nitem_mds &lt;- array(0, dim = c(n_hidden_snapshots, nrow(event), 2), dimnames = list(\"epoch\" = epoch_to_snap, \"item\" = rownames(event), \"dim\" = paste0(\"dim\", 1:2)))\n\nfor (epoch in dimnames(snapshot_hidden_activation)[[1]]) {\n    item_mds[epoch, , ] &lt;- cmdscale(d = dist(snapshot_hidden_activation[epoch,,]), k = 2)\n}\n\narray2DF(item_mds) %&gt;%\n    pivot_wider(id_cols = c(epoch, item), names_from = dim, values_from = Value) %&gt;%\n    mutate(item = factor(item, levels = rownames(event))) %&gt;%\n    mutate(epoch = factor(epoch, levels = as.character(sort(unique(as.numeric(epoch)))))) %&gt;%\n    ggplot(aes(x = dim1, y = dim2, color = item)) +\n    geom_path() +\n    coord_equal() +\n    scale_color_brewer(palette = \"Paired\") +\n    labs(title = \"Evolution of internal representations with learning\", x = \"Dimension 1\", y = \"Dimension 2\", caption = \"Visualization by multidimensional scaling\")\n\n\n\n\n\n\n\n\n\nThe model was trained in “batches”, like we saw last time, only those batches are referred to as “epochs”, following some rather grandiose terminology from the early days of such models. The code above takes “snapshots” of the hidden unit activations at various epochs during learning so we can examine gradual differentiation of the eight concepts. To get these snapshots, we cue the network with the vector representation of a concept and forward-propagate that activation into the hidden layer. The vector of activation values among the hidden units, \\(\\mathbf{h}\\), is treated as a vector representation of that concept.\nA common visualization technique in this domain is to use multidimensional scaling to plot the vector representations derived from the hidden layer in two dimensions. As you can see, the model learns first to distinguish animals and plants, then the subcategories of animals and plants, and finally individual animals within each subcategory. Again, this is a natural consequence of error-driven learning, but also mirrors the developmental trajectories by which many human learners acquire concepts (Rogers & McClelland, 2004).\n\n\nCode\ntransfer_event &lt;- event\n\ntransfer_hidden_activation &lt;- matrix(0, nrow = nrow(transfer_event), ncol = n_hidden, dimnames = list(rownames(transfer_event), 1:n_hidden))\np_resp &lt;- matrix(0, nrow = nrow(transfer_event), ncol = ncol(target), dimnames = list(rownames(transfer_event), colnames(target)))\n\nfor (i in 1:nrow(transfer_event)) {\n    transfer_hidden_activation[i,] &lt;- logistic_act(as.vector(transfer_event[i,] %*% i_to_h_assc_weights) + hidden_baseline)\n    p_resp[i,] &lt;- logistic_act(as.vector(transfer_hidden_activation[i,] %*% h_to_o_assc_weights) + output_baseline)\n}\n\nknitr::kable(p_resp)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nis_green\nhas_leaves\nis_red\nis_yellow\ncan_sing\nhas_bark\nis_big\nhas_branches\nhas_leaves\nhas_petals\nis_pretty\nhas_feathers\ncan_fly\nhas_wings\nhas_scales\ncan_swim\nhas_gills\nhas_roots\ncan_move\nhas_skin\ncan_grow\nis_living\n\n\n\n\npine\n0.6197004\n0.2152856\n0.1634120\n0.0023826\n0.0000787\n0.9250800\n0.9248442\n0.9249413\n0.0654654\n0.0653810\n0.0655194\n0.0003422\n0.0003460\n0.0003463\n0.0290274\n0.0291252\n0.0290503\n0.9841509\n0.0158996\n0.0159508\n0.9934424\n0.9934167\n\n\noak\n0.3639541\n0.7542412\n0.0035378\n0.0604771\n0.0016651\n0.9710283\n0.9711196\n0.9710512\n0.0327099\n0.0327668\n0.0326952\n0.0006676\n0.0006763\n0.0006762\n0.0327475\n0.0326825\n0.0327317\n0.9800315\n0.0200837\n0.0200627\n0.9852742\n0.9852162\n\n\nrose\n0.1171546\n0.0051179\n0.8592013\n0.0786542\n0.0023262\n0.0339779\n0.0337560\n0.0338263\n0.9526771\n0.9526321\n0.9527004\n0.0370967\n0.0370953\n0.0370894\n0.0009683\n0.0009861\n0.0009683\n0.9703105\n0.0296178\n0.0296769\n0.9930061\n0.9930950\n\n\ndaisy\n0.0253838\n0.0450993\n0.0555032\n0.9082180\n0.0622958\n0.0514525\n0.0517702\n0.0516991\n0.9389191\n0.9389509\n0.9388634\n0.0445643\n0.0445280\n0.0445385\n0.0015944\n0.0016179\n0.0015916\n0.9610207\n0.0388918\n0.0388536\n0.9854807\n0.9857238\n\n\nrobin\n0.0592497\n0.0018862\n0.9673368\n0.0214734\n0.0703603\n0.0076952\n0.0074745\n0.0074523\n0.0352395\n0.0352719\n0.0352792\n0.9347017\n0.9348090\n0.9347979\n0.0373993\n0.0373436\n0.0374353\n0.0433034\n0.9567807\n0.9567997\n0.9878454\n0.9877362\n\n\ncanary\n0.0026075\n0.0145775\n0.0436665\n0.9792545\n0.8833943\n0.0039277\n0.0039028\n0.0038735\n0.0438923\n0.0439235\n0.0438537\n0.9448623\n0.9447779\n0.9447972\n0.0529522\n0.0529290\n0.0529405\n0.0255592\n0.9744926\n0.9743351\n0.9806805\n0.9806508\n\n\nsunfish\n0.0099680\n0.0281456\n0.0288670\n0.9513678\n0.0739919\n0.0286904\n0.0290737\n0.0290171\n0.0039767\n0.0039643\n0.0039774\n0.0403594\n0.0403262\n0.0402801\n0.9442030\n0.9443196\n0.9441888\n0.0235738\n0.9763927\n0.9763234\n0.9860802\n0.9860688\n\n\nsalmon\n0.1096960\n0.0026860\n0.9250481\n0.0347728\n0.0020557\n0.0254518\n0.0253029\n0.0253500\n0.0029490\n0.0029360\n0.0029553\n0.0519291\n0.0519060\n0.0519225\n0.9519660\n0.9519600\n0.9519801\n0.0195692\n0.9803492\n0.9804679\n0.9909340\n0.9908830\n\n\n\n\n\nFinally, the code above verifies that the model has, indeed, learned to correctly predict the features of each concept.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Learning useful representations</span>"
    ]
  },
  {
    "objectID": "backprop.html#autoencoders-for-learning-co-occurrence-patterns",
    "href": "backprop.html#autoencoders-for-learning-co-occurrence-patterns",
    "title": "12  Learning useful representations",
    "section": "12.3 Autoencoders for learning co-occurrence patterns",
    "text": "12.3 Autoencoders for learning co-occurrence patterns\nJust like in the last chapter, backprop networks can learn to reproduce patterns. Such networks are called autoencoders. The only difference between the following example and the one above is that the target output is the same vector as the input. The value of backprop in this instance is that the network can learn a reduced representation that takes advantage of redundancies or patterns within the input vectors. This is conceptually similar to how principal components analysis works!\nThe example below uses the disease symptoms from last chapter. Notice that the last two symptoms, discolored gums and nosebleed, are correlated—they are either both present or both absent. Thus, even though the hidden layer only has 3 dimensions instead of 4, the model can correctly reproduce the pattern of symptoms for each patient.\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\nevent &lt;- matrix(c(\n    0, 0, 1, 1,\n    1, 1, 1, 1,\n    0, 1, 0, 0,\n    1, 1, 1, 1,\n    1, 0, 1, 1,\n    1, 1, 0, 0,\n    0, 1, 1, 1,\n    1, 0, 0, 0\n), nrow = 8, ncol = 4, byrow = TRUE,\ndimnames = list(\n    c(\"EM\", \"RM\", \"JJ\", \"LF\", \"AM\", \"JS\", \"ST\", \"SE\"),\n    c(\"swollen_eyelids\", \"splotchy_ears\", \"discolored_gums\", \"nosebleed\")\n))\n\ntarget &lt;- event\n\nlearning_rate &lt;- 0.05\nn_hidden &lt;- 3\nn_epochs &lt;- 5000\nlearn_by_epoch &lt;- TRUE\n\nn_hidden_snapshots &lt;- 9\nepoch_to_snap &lt;- round(exp(seq(0, log(n_epochs), length.out = n_hidden_snapshots)))\nsnapshot_hidden_activation &lt;- array(0, dim = c(n_hidden_snapshots, nrow(event), n_hidden), dimnames = list(\"epoch\" = epoch_to_snap, \"item\" = rownames(event), \"hidden_unit\" = 1:n_hidden))\n\ni_to_h_assc_weights &lt;- matrix(rnorm(n = ncol(event) * n_hidden), nrow = ncol(event), ncol = n_hidden) * 0.01\nh_to_o_assc_weights &lt;- matrix(rnorm(n = n_hidden * ncol(target)), nrow = n_hidden, ncol = ncol(target)) * 0.01\n\nhidden_baseline &lt;- rnorm(n = n_hidden) * 0.01\noutput_baseline &lt;- rnorm(n = ncol(target)) * 0.01\n\nfor (epoch in 1:n_epochs) {\n    training_order &lt;- sample(nrow(event))\n\n    d_h_to_o &lt;- 0 * h_to_o_assc_weights\n    d_i_to_h &lt;- 0 * i_to_h_assc_weights\n    \n    d_h_baseline &lt;- 0 * hidden_baseline\n    d_o_baseline &lt;- 0 * output_baseline\n\n    for (i in training_order) {\n        # Forward propagation (prediction)\n\n        # Note the use of `as.vector` to ensure result is a vector (not a matrix with a single row)\n        hidden_activation &lt;- logistic_act(as.vector(event[i,] %*% i_to_h_assc_weights) + hidden_baseline)\n        output_activation &lt;- logistic_act(as.vector(hidden_activation %*% h_to_o_assc_weights) + output_baseline)\n\n        # Compute error at output layer\n        output_error &lt;- target[i,] - output_activation\n\n        # Back propagation (learning)\n\n        # Derivative of error with respect to output unit activation\n        d_output_error &lt;- output_error * output_activation * (1 - output_activation)\n\n        # Derivative of error with respect to hidden unit activation\n        d_hidden_error &lt;- as.vector(h_to_o_assc_weights %*% d_output_error) * hidden_activation * (1 - hidden_activation)\n\n        # How much to adjust the associative weights between the hidden and output layers\n        d_h_to_o &lt;- d_h_to_o + learning_rate * outer(hidden_activation, d_output_error)\n\n        # How much to adjust the associative weights between the input and hidden layers\n        d_i_to_h &lt;- d_i_to_h + learning_rate * outer(event[i,], d_hidden_error)\n        \n        # How much to adjust the baseline of the output units\n        d_o_baseline &lt;- learning_rate * d_output_error\n        \n        # How much to adjust the baseline of the hidden units\n        d_h_baseline &lt;- learning_rate * d_hidden_error\n\n        if (!learn_by_epoch) {\n            # Adjust weights\n            i_to_h_assc_weights &lt;- i_to_h_assc_weights + d_i_to_h\n            h_to_o_assc_weights &lt;- h_to_o_assc_weights + d_h_to_o\n            \n            hidden_baseline &lt;- hidden_baseline + d_h_baseline\n            output_baseline &lt;- output_baseline + d_o_baseline\n\n            # Reset adjustment to zero\n            d_h_to_o &lt;- 0 * h_to_o_assc_weights\n            d_i_to_h &lt;- 0 * i_to_h_assc_weights\n            \n            d_h_baseline &lt;- 0 * hidden_baseline\n            d_o_baseline &lt;- 0 * output_baseline\n        }\n    }\n\n    if (learn_by_epoch) {\n        # Adjust weights\n        i_to_h_assc_weights &lt;- i_to_h_assc_weights + d_i_to_h\n        h_to_o_assc_weights &lt;- h_to_o_assc_weights + d_h_to_o\n        \n        hidden_baseline &lt;- hidden_baseline + d_h_baseline\n        output_baseline &lt;- output_baseline + d_o_baseline\n    }\n    \n    if (epoch %in% as.numeric(dimnames(snapshot_hidden_activation)[[1]])) {\n        for (i in 1:nrow(event)) {\n            snapshot_hidden_activation[as.character(epoch), i, ] &lt;- logistic_act(as.vector(event[i,] %*% i_to_h_assc_weights) + hidden_baseline)\n        }\n    }\n}\n\narray2DF(snapshot_hidden_activation) %&gt;%\n    mutate(item = factor(item, levels = rownames(event))) %&gt;%\n    mutate(hidden_unit = as.numeric(hidden_unit)) %&gt;%\n    mutate(epoch = factor(epoch, levels = as.character(sort(unique(as.numeric(epoch)))))) %&gt;%\n    ggplot(aes(x = hidden_unit, y = Value, color = item)) +\n    geom_line() +\n    scale_color_okabeito() +\n    facet_wrap(\"epoch\", labeller = label_both)\n\n\n\n\n\n\n\n\n\nCode\nitem_mds &lt;- array(0, dim = c(n_hidden_snapshots, nrow(event), 2), dimnames = list(\"epoch\" = epoch_to_snap, \"item\" = rownames(event), \"dim\" = paste0(\"dim\", 1:2)))\n\nfor (epoch in dimnames(snapshot_hidden_activation)[[1]]) {\n    item_mds[epoch, , ] &lt;- cmdscale(d = dist(snapshot_hidden_activation[epoch,,]), k = 2)\n}\n\narray2DF(item_mds) %&gt;%\n    pivot_wider(id_cols = c(epoch, item), names_from = dim, values_from = Value) %&gt;%\n    mutate(item = factor(item, levels = rownames(event))) %&gt;%\n    mutate(epoch = factor(epoch, levels = as.character(sort(unique(as.numeric(epoch)))))) %&gt;%\n    ggplot(aes(x = dim1, y = dim2, color = item)) +\n    geom_path() +\n    coord_equal() +\n    scale_color_okabeito() +\n    labs(title = \"Evolution of internal representations with learning\", x = \"Dimension 1\", y = \"Dimension 2\", caption = \"Visualization by multidimensional scaling\")\n\n\n\n\n\n\n\n\n\nCode\ntransfer_event &lt;- diag(ncol(event))\ncolnames(transfer_event) &lt;- rownames(transfer_event) &lt;- colnames(event)\n\ntransfer_hidden_activation &lt;- matrix(0, nrow = nrow(transfer_event), ncol = n_hidden, dimnames = list(rownames(transfer_event), 1:n_hidden))\np_resp &lt;- matrix(0, nrow = nrow(transfer_event), ncol = ncol(target), dimnames = list(rownames(transfer_event), colnames(target)))\n\nfor (i in 1:nrow(transfer_event)) {\n    transfer_hidden_activation[i,] &lt;- logistic_act(as.vector(transfer_event[i,] %*% i_to_h_assc_weights) + hidden_baseline)\n    p_resp[i,] &lt;- logistic_act(as.vector(transfer_hidden_activation[i,] %*% h_to_o_assc_weights) + output_baseline)\n}\n\nknitr::kable(p_resp)\n\n\n\n\n\n\n\n\n\n\n\n\n\nswollen_eyelids\nsplotchy_ears\ndiscolored_gums\nnosebleed\n\n\n\n\nswollen_eyelids\n0.8942735\n0.3141048\n0.1306991\n0.1301509\n\n\nsplotchy_ears\n0.0644822\n0.9956340\n0.0766419\n0.0766610\n\n\ndiscolored_gums\n0.3069308\n0.0945172\n0.9869707\n0.9869458\n\n\nnosebleed\n0.3068504\n0.0959893\n0.9869667\n0.9869420\n\n\n\n\n\nCode\nknitr::kable(transfer_hidden_activation)\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\nswollen_eyelids\n0.0599611\n0.0086411\n0.0845510\n\n\nsplotchy_ears\n0.0468187\n0.9518634\n0.0275678\n\n\ndiscolored_gums\n0.9654033\n0.4996647\n0.8173764\n\n\nnosebleed\n0.9660011\n0.5010516\n0.8161011\n\n\n\n\n\nBy cuing the hidden layer with a single symptom at a time, we can obtain what is, in essence, a “factor loading matrix”. Specifically, by examining how each hidden unit is activated by each feature, we see how strongly that feature is associated with each hidden unit. Each hidden unit can then be thought of as analogous to a “latent factor” that the network has learned, which enables it to correctly reproduce the input.\n\n12.3.1 Learning and distinguishing category prototypes\nThe ability of autoencoders to infer latent factors even in the presence of variation in the input suggests how they might serve as models for how participants learn category prototypes. Consider, for example, the category of “dog”. No two dogs are exactly alike (though all dogs go to heaven). Nonetheless, despite the considerable variability between individual dogs, there are shared characteristics that most dogs possess that enable us to form a representation of a prototypical “dog”. Those same regularities enable us to “fill in the blanks”, just like we did with Hebbian learning earlier. For example, if we only see two oThis feature of autoencoders makes them robust to noise in the input.\nThis latter property is illustrated in the example below, which is a simplified version of a classic set of studies by Posner & Keele (1968). In these studies, participants viewed images comprised of random dot patterns, like those shown below.\n\n\nCode\nprototype &lt;- scale(matrix(rnorm(n = 7 * 2), nrow = 7, ncol = 2))\ncolnames(prototype) &lt;- c(\"x\", \"y\")\n\nn_examples &lt;- 4\nlow_sd &lt;- 0.1\nhigh_sd &lt;- 0.2\n\nexamples &lt;- c()\n\nfor (i in 1:n_examples) {\n    examples &lt;- rbind(\n        examples,\n        cbind(type = \"Low distortion\", item = paste(\"Example\", i), as_tibble(prototype + matrix(rnorm(n = 7 * 2, sd = low_sd), nrow = 7, ncol = 2))),\n        cbind(type = \"High distortion\", item = paste(\"Example\", i), as_tibble(prototype + matrix(rnorm(n = 7 * 2, sd = high_sd), nrow = 7, ncol = 2)))\n    )\n}\n\nscale_lim &lt;- range(c(prototype, examples$x, examples$y))\n\nprototype_plot &lt;- cbind(type = \"Prototype\", as_tibble(prototype)) %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    coord_equal(xlim = scale_lim, ylim = scale_lim) +\n    theme_void() +\n    theme(panel.border = element_rect(fill = NA, color = \"black\", linetype = \"solid\", linewidth = 1)) +\n    facet_wrap(\"type\") +\n    labs(x = NULL, y = NULL)\n\nexample_plot &lt;- examples %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    coord_equal(xlim = scale_lim, ylim = scale_lim) +\n    theme_void() +\n    theme(panel.border = element_rect(fill = NA, color = \"black\", linetype = \"solid\", linewidth = 1)) +\n    facet_grid(type ~ item, switch = \"y\", as.table = FALSE) +\n    labs(x = NULL, y = NULL)\n\nprint(prototype_plot + example_plot + plot_layout(ncol = 1, heights = c(1, 2)))\n\n\n\n\n\n\n\n\n\nEach dot pattern was a distortion of a “prototype” dot pattern. As shown above, there could be “low distortions” which differ only a little from the prototype as well as “high distortions” which differ more strongly from the prototype. Collectively, the various distortions from a prototype share a “family resemblance” to one another. In that sense, they mimic the “dog” example from earlier, where even though all dogs look different, they can be understood as “variations on a theme”. The “theme” is a prototype, and we will now see that an autoencoder can learn a representation of the regularities that define such a prototype.\nThe example below uses a simplified version of the Posner & Keele (1968) paradigm. Each prototype is a sequence of binary symbols, which we might think of as indicating the presence or absence of different features. In the example below, there are two prototypes, perhaps representing two different diseases or two different concepts. The two prototypes differ on the first six features but match on the final two, perhaps like two different diseases with shared symptoms or two concepts with shared features.\nOn each epoch of training, the network is presented with a number of exemplars of each prototype. Each exemplar consists of a set of features, and there is a variable p_change that indicates the probability that any given feature of an exemplar will be “flipped” relative to its value in the prototype. For example, if the prototype is c(1, 1, 1, 0, 0, 0, 0, 1), then an exemplar is initially created that is an exact copy of that prototype. If p_change is greater than zero, then there is a chance that any one of the features of the exemplar could be flipped. For example, if the third feature is flipped, the resulting exemplar would be c(1, 1, 0, 0, 0, 0, 0, 1).\nTo begin, let’s see what happens if we only give the network 2 hidden units to work with. This is essentially forcing the model to “make do” with an internal representation of with only two dimensions. If the network is able to pick up on the patterns of family resemblance, then the activation of the two hidden units should reflect which of the two prototypes most likely generated any given exemplar. Below, p_change is set to 0.1. Let’s see how well the model does!\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\nprototypes &lt;- matrix(c(\n    1, 1, 1, 0, 0, 0, 0, 1,\n    0, 0, 0, 1, 1, 1, 0, 1\n), nrow = 2, ncol = 8, byrow = TRUE,\ndimnames = list(\n    c(\"P1\", \"P2\")\n))\n\n# On each epoch, the model sees a new randomly generated set of exemplars generated from each prototype\nn_epochs &lt;- 5000\n# This is the number of exemplars of each prototype per epoch\nn_per_prototype &lt;- 100\n# This is what proportion of features, on average, will differ between the exemplars and their prototype\np_change &lt;- 0.1\n\nlearning_rate &lt;- 0.1\nn_hidden &lt;- 2\n\ni_to_h_assc_weights &lt;- matrix(rnorm(n = ncol(prototypes) * n_hidden), nrow = ncol(prototypes), ncol = n_hidden) * 0.01\nh_to_o_assc_weights &lt;- matrix(rnorm(n = n_hidden * ncol(prototypes)), nrow = n_hidden, ncol = ncol(prototypes)) * 0.01\n\nhidden_baseline &lt;- rnorm(n = n_hidden) * 0.01\noutput_baseline &lt;- rnorm(n = ncol(prototypes)) * 0.01\n\nfor (epoch in 1:n_epochs) {\n    event &lt;- prototypes[sample(rep(1:nrow(prototypes), each = n_per_prototype)), , drop = FALSE]\n    to_change &lt;- matrix(rbinom(n = nrow(event) * ncol(event), size = 1, p = p_change), nrow = nrow(event), ncol = ncol(event))\n    event[to_change] &lt;- 1 - event[to_change]\n    \n    target &lt;- event\n    \n    d_h_to_o &lt;- 0 * h_to_o_assc_weights\n    d_i_to_h &lt;- 0 * i_to_h_assc_weights\n    \n    d_h_baseline &lt;- 0 * hidden_baseline\n    d_o_baseline &lt;- 0 * output_baseline\n    \n    prev_hidden_activation &lt;- rep(0, n_hidden)\n    \n    for (i in 1:nrow(event)) {\n        # Forward propagation (prediction)\n        hidden_activation &lt;- logistic_act(as.vector(event[i,] %*% i_to_h_assc_weights) + hidden_baseline)\n        output_activation &lt;- logistic_act(as.vector(hidden_activation %*% h_to_o_assc_weights) + output_baseline)\n    \n        # Compute error at output layer\n        output_error &lt;- target[i,] - output_activation\n    \n        # Back propagation (learning)\n    \n        # Derivative of error with respect to output unit activation\n        d_output_error &lt;- output_error * output_activation * (1 - output_activation)\n    \n        # Derivative of error with respect to hidden unit activation\n        d_hidden_error &lt;- as.vector(h_to_o_assc_weights %*% d_output_error) * hidden_activation * (1 - hidden_activation)\n    \n        # How much to adjust the associative weights between the hidden and output layers\n        d_h_to_o &lt;- d_h_to_o + learning_rate * outer(hidden_activation, d_output_error)\n    \n        # How much to adjust the associative weights between the input and hidden layers\n        d_i_to_h &lt;- d_i_to_h + learning_rate * outer(event[i,], d_hidden_error)\n        \n        # How much to adjust the baseline of the output units\n        d_o_baseline &lt;- learning_rate * d_output_error\n        \n        # How much to adjust the baseline of the hidden units\n        d_h_baseline &lt;- learning_rate * d_hidden_error\n    }\n    \n    # Adjust weights\n    i_to_h_assc_weights &lt;- i_to_h_assc_weights + d_i_to_h\n    h_to_o_assc_weights &lt;- h_to_o_assc_weights + d_h_to_o\n    \n    hidden_baseline &lt;- hidden_baseline + d_h_baseline\n    output_baseline &lt;- output_baseline + d_o_baseline\n}\n\ntransfer_event &lt;- diag(ncol(prototypes))\nrownames(transfer_event) &lt;- colnames(transfer_event) &lt;- paste(\"Feature\", 1:nrow(transfer_event))\n\ntransfer_hidden_activation &lt;- matrix(0, nrow = nrow(transfer_event), ncol = n_hidden, dimnames = list(rownames(transfer_event), 1:n_hidden))\np_resp &lt;- matrix(0, nrow = nrow(transfer_event), ncol = ncol(target), dimnames = list(rownames(transfer_event), colnames(target)))\n\nfor (i in 1:nrow(transfer_event)) {\n    transfer_hidden_activation[i,] &lt;- logistic_act(as.vector(transfer_event[i,] %*% i_to_h_assc_weights) + hidden_baseline)\n    p_resp[i,] &lt;- logistic_act(as.vector(transfer_hidden_activation[i,] %*% h_to_o_assc_weights) + output_baseline)\n}\n\n\nAt the end of the chunk above, we probe what the model has learned by presenting it with eight “transfer” events each consisting of one of the eight features being set to 1 while the others are set to 0:\n\n\nCode\nknitr::kable(transfer_event)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature 1\nFeature 2\nFeature 3\nFeature 4\nFeature 5\nFeature 6\nFeature 7\nFeature 8\n\n\n\n\nFeature 1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nFeature 2\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nFeature 3\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nFeature 4\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nFeature 5\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nFeature 6\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nFeature 7\n0\n0\n0\n0\n0\n0\n1\n0\n\n\nFeature 8\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n\nThe table below indicates the activation at the model’s “output” layer for each of the transfer events above, representing its expectations for the other feature values:\n\n\nCode\nknitr::kable(p_resp)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature 1\n0.9997033\n0.9893001\n0.9893061\n0.0106886\n0.0107070\n0.0106903\n0.0018336\n0.9981642\n\n\nFeature 2\n0.9987015\n0.9682807\n0.9683123\n0.0316595\n0.0317569\n0.0316688\n0.0029465\n0.9970532\n\n\nFeature 3\n0.9987136\n0.9685302\n0.9685615\n0.0314105\n0.0315071\n0.0314198\n0.0029452\n0.9970544\n\n\nFeature 4\n0.0774881\n0.0077560\n0.0077707\n0.9922162\n0.9922614\n0.9922206\n0.0021182\n0.9978799\n\n\nFeature 5\n0.0765345\n0.0077274\n0.0077421\n0.9922447\n0.9922901\n0.9922491\n0.0021584\n0.9978398\n\n\nFeature 6\n0.0790053\n0.0079269\n0.0079419\n0.9920447\n0.9920909\n0.9920491\n0.0021339\n0.9978642\n\n\nFeature 7\n0.8709508\n0.3250548\n0.3254244\n0.6742455\n0.6753852\n0.6743548\n0.0034501\n0.9965506\n\n\nFeature 8\n0.9168317\n0.3092547\n0.3094502\n0.6903754\n0.6909781\n0.6904331\n0.0010764\n0.9989210\n\n\n\n\n\nWe can see that the network has largely learned to expect feature values consistent with the prototype that is most closely associated with the activated feature. Of course, for the final two features which have the same value in both prototypes, the model’s expectations are more equivocal.\nIt is intriguing to note that, for the transfer pattern in which Feature 7 is “turned on”, the model still expects this feature to have a very low value! This feature had the value “0” in both prototypes, so the model would have only experienced a value of 1 for Feature 7 if it happened to be “flipped” by chance in one of the training examples. So in a sense, by expecting Feature 7 to have a value of 0 even when the model is “told” that it has a value of 1, the model is “discounting” the evidence of its “senses”!\nBy examining the patterns of hidden unit activation for each transfer pattern, we can see the source of the model’s expectations:\n\n\nCode\nknitr::kable(transfer_hidden_activation)\n\n\n\n\n\n\n1\n2\n\n\n\n\nFeature 1\n0.9908998\n0.1121541\n\n\nFeature 2\n0.8531575\n0.1670194\n\n\nFeature 3\n0.8538664\n0.1663734\n\n\nFeature 4\n0.2083492\n0.8811281\n\n\nFeature 5\n0.2061844\n0.8799589\n\n\nFeature 6\n0.2094186\n0.8787193\n\n\nFeature 7\n0.4976117\n0.4998254\n\n\nFeature 8\n0.6069245\n0.5975704\n\n\n\n\n\nAs we might have hoped, the model has learned to use its two hidden units to indicate which of the two prototypes is most consistent with the features in each transfer event. Once again, the final two (ambiguous) features also result in close-to-equal activation of each hidden unit.\nThat said, it might be more fair to judge the model if we don’t “give it the answer” by only equipping it with two hidden units. Let’s instead give it eight hidden units—the same number as there are features—and see if it can still discover the regularities consistent with each prototype.\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\nprototypes &lt;- matrix(c(\n    1, 1, 1, 0, 0, 0, 0, 1,\n    0, 0, 0, 1, 1, 1, 0, 1\n), nrow = 2, ncol = 8, byrow = TRUE,\ndimnames = list(\n    c(\"P1\", \"P2\")\n))\n\n# On each epoch, the model sees a new randomly generated set of exemplars generated from each prototype\nn_epochs &lt;- 5000\n# This is the number of exemplars of each prototype per epoch\nn_per_prototype &lt;- 100\n# This is what proportion of features, on average, will differ between the exemplars and their prototype\np_change &lt;- 0.1\n\nlearning_rate &lt;- 0.1\nn_hidden &lt;- 8\n\ni_to_h_assc_weights &lt;- matrix(rnorm(n = ncol(prototypes) * n_hidden), nrow = ncol(prototypes), ncol = n_hidden) * 0.01\nh_to_o_assc_weights &lt;- matrix(rnorm(n = n_hidden * ncol(prototypes)), nrow = n_hidden, ncol = ncol(prototypes)) * 0.01\n\nhidden_baseline &lt;- rnorm(n = n_hidden) * 0.01\noutput_baseline &lt;- rnorm(n = ncol(prototypes)) * 0.01\n\nfor (epoch in 1:n_epochs) {\n    event &lt;- prototypes[sample(rep(1:nrow(prototypes), each = n_per_prototype)), , drop = FALSE]\n    to_change &lt;- matrix(rbinom(n = nrow(event) * ncol(event), size = 1, p = p_change), nrow = nrow(event), ncol = ncol(event))\n    event[to_change] &lt;- 1 - event[to_change]\n    \n    target &lt;- event\n    \n    d_h_to_o &lt;- 0 * h_to_o_assc_weights\n    d_i_to_h &lt;- 0 * i_to_h_assc_weights\n    \n    d_h_baseline &lt;- 0 * hidden_baseline\n    d_o_baseline &lt;- 0 * output_baseline\n    \n    prev_hidden_activation &lt;- rep(0, n_hidden)\n    \n    for (i in 1:nrow(event)) {\n        # Forward propagation (prediction)\n        hidden_activation &lt;- logistic_act(as.vector(event[i,] %*% i_to_h_assc_weights) + hidden_baseline)\n        output_activation &lt;- logistic_act(as.vector(hidden_activation %*% h_to_o_assc_weights) + output_baseline)\n    \n        # Compute error at output layer\n        output_error &lt;- target[i,] - output_activation\n    \n        # Back propagation (learning)\n    \n        # Derivative of error with respect to output unit activation\n        d_output_error &lt;- output_error * output_activation * (1 - output_activation)\n    \n        # Derivative of error with respect to hidden unit activation\n        d_hidden_error &lt;- as.vector(h_to_o_assc_weights %*% d_output_error) * hidden_activation * (1 - hidden_activation)\n    \n        # How much to adjust the associative weights between the hidden and output layers\n        d_h_to_o &lt;- d_h_to_o + learning_rate * outer(hidden_activation, d_output_error)\n    \n        # How much to adjust the associative weights between the input and hidden layers\n        d_i_to_h &lt;- d_i_to_h + learning_rate * outer(event[i,], d_hidden_error)\n        \n        # How much to adjust the baseline of the output units\n        d_o_baseline &lt;- learning_rate * d_output_error\n        \n        # How much to adjust the baseline of the hidden units\n        d_h_baseline &lt;- learning_rate * d_hidden_error\n    }\n    \n    # Adjust weights\n    i_to_h_assc_weights &lt;- i_to_h_assc_weights + d_i_to_h\n    h_to_o_assc_weights &lt;- h_to_o_assc_weights + d_h_to_o\n    \n    hidden_baseline &lt;- hidden_baseline + d_h_baseline\n    output_baseline &lt;- output_baseline + d_o_baseline\n}\n\ntransfer_event &lt;- diag(ncol(prototypes))\nrownames(transfer_event) &lt;- colnames(transfer_event) &lt;- paste(\"Feature\", 1:nrow(transfer_event))\n\ntransfer_hidden_activation &lt;- matrix(0, nrow = nrow(transfer_event), ncol = n_hidden, dimnames = list(rownames(transfer_event), 1:n_hidden))\np_resp &lt;- matrix(0, nrow = nrow(transfer_event), ncol = ncol(target), dimnames = list(rownames(transfer_event), colnames(target)))\n\nfor (i in 1:nrow(transfer_event)) {\n    transfer_hidden_activation[i,] &lt;- logistic_act(as.vector(transfer_event[i,] %*% i_to_h_assc_weights) + hidden_baseline)\n    p_resp[i,] &lt;- logistic_act(as.vector(transfer_hidden_activation[i,] %*% h_to_o_assc_weights) + output_baseline)\n}\n\n\n\n\nCode\nknitr::kable(p_resp)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature 1\n0.9999629\n0.9873418\n0.9873510\n0.0126542\n0.0126575\n0.0126505\n0.0008719\n0.9991304\n\n\nFeature 2\n0.9993704\n0.9234755\n0.9237602\n0.0763888\n0.0764785\n0.0763762\n0.0018163\n0.9981896\n\n\nFeature 3\n0.9993676\n0.9230696\n0.9233528\n0.0768033\n0.0768983\n0.0768017\n0.0018001\n0.9982057\n\n\nFeature 4\n0.1704360\n0.0068217\n0.0068725\n0.9931383\n0.9931712\n0.9931460\n0.0010124\n0.9989944\n\n\nFeature 5\n0.1709796\n0.0068199\n0.0068703\n0.9931409\n0.9931733\n0.9931486\n0.0010069\n0.9989999\n\n\nFeature 6\n0.1688919\n0.0067943\n0.0068453\n0.9931664\n0.9931986\n0.9931735\n0.0010205\n0.9989865\n\n\nFeature 7\n0.9243916\n0.1953597\n0.1963633\n0.8039330\n0.8044775\n0.8040272\n0.0017631\n0.9982449\n\n\nFeature 8\n0.9688339\n0.2665740\n0.2675084\n0.7328589\n0.7333190\n0.7329245\n0.0009362\n0.9990692\n\n\n\n\n\nEven without encouraging the model to find the “right” answer, it still looks like it is able to form reasonable expectations for the feature values that are consistent with the prototypes that originally generated the training exemplars.\nHowever, since there are now 8 hidden units, its a bit hard to see how the pattern of activation among the hidden units represents each potential prototype:\n\n\nCode\nknitr::kable(transfer_hidden_activation)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nFeature 1\n0.1528907\n0.4344921\n0.1661314\n0.1710326\n0.9538300\n0.9649348\n0.9494558\n0.1816092\n\n\nFeature 2\n0.2329792\n0.4075160\n0.2372756\n0.2541759\n0.7525858\n0.7815335\n0.7455288\n0.2541011\n\n\nFeature 3\n0.2375500\n0.4140858\n0.2341705\n0.2548858\n0.7512986\n0.7846717\n0.7425981\n0.2524576\n\n\nFeature 4\n0.7721480\n0.6340845\n0.7706312\n0.7539505\n0.3059169\n0.2862130\n0.3127077\n0.7517998\n\n\nFeature 5\n0.7744988\n0.6328773\n0.7695565\n0.7554082\n0.3045691\n0.2868571\n0.3155504\n0.7514157\n\n\nFeature 6\n0.7715945\n0.6332336\n0.7680214\n0.7554003\n0.3055191\n0.2870289\n0.3088729\n0.7534171\n\n\nFeature 7\n0.4976473\n0.5002462\n0.5005177\n0.5032810\n0.4940626\n0.4956329\n0.4955740\n0.4910196\n\n\nFeature 8\n0.5082826\n0.5433736\n0.5123282\n0.5144572\n0.5748479\n0.5941206\n0.5741431\n0.5155124\n\n\n\n\n\nSo let’s make a plot instead:\n\n\nCode\narray2DF(transfer_hidden_activation) %&gt;%\n    mutate(transfer_event = factor(Var1)) %&gt;%\n    mutate(hidden_unit_index = as.numeric(Var2)) %&gt;%\n    ggplot(aes(x = hidden_unit_index, y = Value, color = transfer_event)) +\n    geom_line() +\n    scale_color_okabeito() +\n    labs(x = \"Hidden unit index\", y = \"Activation\", color = \"Active feature\")\n\n\n\n\n\n\n\n\n\nThe visualization makes it easier to see that the pattern of hidden unit activation is essentially “mirrored” between features 1–3 and features 4–6, and flat for the ambiguous features 7 and 8. Once again, the network has learned to form a distributed representation of its input that captures the regularities of the events that it has experienced.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Learning useful representations</span>"
    ]
  },
  {
    "objectID": "backprop.html#simple-recurrent-networks-learning-patterns-in-time",
    "href": "backprop.html#simple-recurrent-networks-learning-patterns-in-time",
    "title": "12  Learning useful representations",
    "section": "12.4 Simple recurrent networks: Learning patterns in time",
    "text": "12.4 Simple recurrent networks: Learning patterns in time\nOne of the most interesting (in my opinion) applications of backprop networks comes in the form of simple recurrent networks (SRN). These were described by Elman (1990), who showed that they could learn structure in time. These models are applied to sequences, where the current event in the sequence is treated as “input” and the next event is the target at the output layer. Thus, these networks are explicitly trained to predict the future.\nIn principle, the same model structure we’ve seen so far could work here too. But Elman (1990) proposed an interesting change: The input on each step of the sequence isn’t just the current event—it is also the vector of hidden unit activation from the previous step of the sequence. Thus, the networks own prior state is treated as input for the purpose of making a prediction. This means that the hidden unit representations learned by the network must pull “double duty”—they need to learn, in essence, how to serve as useful memory representations.\n\n12.4.1 Learning to have memory\nElman (1990) gave a simple illustration of an SRN learning to have memory. In the following example, each event has just a single dimension—it can be 0 or 1. Sequences are comprised of three-element subsequences. Within each subsequence, the final element is the “exclusive-or” (XOR) of the previous two elements. Specifically, the final element is 1 if the previous two elements are different and it is 0 if the previous two elements are the same. These three-element subsequences are randomly concatenated to form a long exposure sequence.\nThis is a challenging pattern to learn! In many cases, it will be impossible to predict the next element. Imagine that the sequence 0, 1, 1 occurs. When the first element occurs, it is not possible to predict what the second one will be. However, once the second element occurs, it is possible to predict the third element, since it follows the XOR rule. But once the third element occurs, it is again impossible to predict the next element, since the next element is the start of the next sequence. Only for the second element of a sequence is it possible to make a reasonable prediction about the next element. And that’s only if you (a) can remember far enough back; and (b) have learned the XOR rule.\nThis learning situation can be thought of as a microcosm for learning the structure of events more generally. For example, if a pitcher throws a ball, we cannot predict whether the batter will hit it. But once we know whether the batter has hit the ball or not, we can form some expectation about what will happen next. But then the next pitch is unpredictable once again.\nThe code below implements a version of Elman’s XOR sequence task. The network only has 2 hidden units. It learns in batches, where each batch is a long sequence built by concatenating many 3-element subsequences, each of which follow the XOR rule.\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\n# Each row is a subsequence from which the whole training sequence will be built.\n# The number refer to the rows in the \"representations\" matrix below.\nsubsequences &lt;- matrix(c(\n    1, 1, 2,\n    1, 2, 1,\n    2, 1, 1,\n    2, 2, 2\n), nrow = 4, ncol = 3, byrow = TRUE)\n\nrepresentations &lt;- matrix(c(\n    1,\n    0\n), nrow = 2, ncol = 1, byrow = TRUE)\n\n# On each epoch, the model learns on a new randomly generated training sequence\nn_epochs &lt;- 5000\n# This is the number of times each subsequence will occur in each training sequence\nn_per_subsequence &lt;- 100\n\nlearning_rate &lt;- 0.1\nn_hidden &lt;- 2\n\ni_to_h_assc_weights &lt;- matrix(rnorm(n = (ncol(representations) + n_hidden) * n_hidden), nrow = ncol(representations) + n_hidden, ncol = n_hidden) * 0.01\nh_to_o_assc_weights &lt;- matrix(rnorm(n = n_hidden * ncol(representations)), nrow = n_hidden, ncol = ncol(representations)) * 0.01\n\nhidden_baseline &lt;- rnorm(n = n_hidden) * 0.01\noutput_baseline &lt;- rnorm(n = ncol(representations)) * 0.01\n\nfor (epoch in 1:n_epochs) {\n    training_sequence &lt;- c(t(subsequences[sample(rep(1:nrow(subsequences), each = n_per_subsequence)),]))\n    \n    event &lt;- representations[training_sequence[1:(length(training_sequence) - 1)], , drop = FALSE]\n    \n    target &lt;- representations[training_sequence[2:length(training_sequence)], , drop = FALSE]\n    \n    d_h_to_o &lt;- 0 * h_to_o_assc_weights\n    d_i_to_h &lt;- 0 * i_to_h_assc_weights\n    \n    d_h_baseline &lt;- 0 * hidden_baseline\n    d_o_baseline &lt;- 0 * output_baseline\n    \n    prev_hidden_activation &lt;- rep(0, n_hidden)\n    \n    for (i in 1:nrow(event)) {\n        # Forward propagation (prediction)\n        this_input &lt;- c(event[i,], prev_hidden_activation)\n        hidden_activation &lt;- logistic_act(as.vector(this_input %*% i_to_h_assc_weights) + hidden_baseline)\n        output_activation &lt;- logistic_act(as.vector(hidden_activation %*% h_to_o_assc_weights) + output_baseline)\n    \n        # Compute error at output layer\n        output_error &lt;- target[i,] - output_activation\n    \n        # Back propagation (learning)\n    \n        # Derivative of error with respect to output unit activation\n        d_output_error &lt;- output_error * output_activation * (1 - output_activation)\n    \n        # Derivative of error with respect to hidden unit activation\n        d_hidden_error &lt;- as.vector(h_to_o_assc_weights %*% d_output_error) * hidden_activation * (1 - hidden_activation)\n    \n        # How much to adjust the associative weights between the hidden and output layers\n        d_h_to_o &lt;- d_h_to_o + learning_rate * outer(hidden_activation, d_output_error)\n    \n        # How much to adjust the associative weights between the input and hidden layers\n        d_i_to_h &lt;- d_i_to_h + learning_rate * outer(this_input, d_hidden_error)\n        \n        # How much to adjust the baseline of the output units\n        d_o_baseline &lt;- learning_rate * d_output_error\n        \n        # How much to adjust the baseline of the hidden units\n        d_h_baseline &lt;- learning_rate * d_hidden_error\n    \n        # Copy hidden unit activation\n        prev_hidden_activation &lt;- hidden_activation\n    }\n    \n    # Adjust weights\n    i_to_h_assc_weights &lt;- i_to_h_assc_weights + d_i_to_h\n    h_to_o_assc_weights &lt;- h_to_o_assc_weights + d_h_to_o\n    \n    hidden_baseline &lt;- hidden_baseline + d_h_baseline\n    output_baseline &lt;- output_baseline + d_o_baseline\n}\n\ntransfer_sequence &lt;- c(t(subsequences[sample(rep(1:nrow(subsequences), each = n_per_subsequence)),]))\n\nevent &lt;- representations[transfer_sequence[1:(length(transfer_sequence) - 1)], , drop = FALSE]\n\ntarget &lt;- representations[transfer_sequence[2:length(transfer_sequence)], , drop = FALSE]\n\ntransfer_error &lt;- rep(0, nrow(event))\n\nprev_hidden_activation &lt;- rep(0, n_hidden)\n\nfor (i in 1:nrow(event)) {\n    this_input &lt;- c(event[i,], prev_hidden_activation)\n    hidden_activation &lt;- logistic_act(as.vector(this_input %*% i_to_h_assc_weights) + hidden_baseline)\n    output_activation &lt;- logistic_act(as.vector(hidden_activation %*% h_to_o_assc_weights) + output_baseline)\n    transfer_error[i] &lt;- sqrt(mean((target[i,] - output_activation)^2))\n    prev_hidden_activation &lt;- hidden_activation\n}\n\ntibble(step = 1:nrow(event), error = transfer_error) %&gt;%\n    mutate(sub_step = ((step - 1) %% ncol(subsequences)) + 1) %&gt;%\n    ggplot(aes(x = sub_step, y = error)) +\n    stat_summary(fun = mean, geom = \"line\", group = 1) +\n    stat_summary(fun.data = mean_cl_normal) +\n    labs(x = \"Position within subsequence\", y = \"Root mean squared prediction error\")\n\n\n\n\n\n\n\n\n\nAs we can see, once trained, the network shows exactly the “dip” in prediction error that we would expect if the network (a) could use its hidden layer to retain memory sufficient for the task; and (b) had learned the XOR pattern.\n\n\n12.4.2 Learning concept representations by prediction\nThe following illustrates how the same SRN model structure can learn to distinguish between more complex concepts based on how they occur within “sentences” of a “language”. Specifically, the network will be presented with a sequence consisting of concatenated subsequences, each of which is three elements long. These elements comprise “sentences” following a “subject-verb-object” frame. Only some items can serve as each category, some subjects can engage in different verbs, and some objects can only receive certain verbs.\n\n\nCode\nlogistic_act &lt;- function(x) {\n    return(1 / (1 + exp(-x)))\n}\n\n# Each row is a subsequence from which the whole training sequence will\n# be built.\nsubsequences &lt;- matrix(c(\n    1, 5, 2,\n    1, 5, 3,\n    1, 5, 4,\n    1, 6, 2,\n    1, 6, 3,\n    1, 7, 3,\n    1, 8, 4,\n    2, 5, 1,\n    2, 5, 3,\n    2, 5, 4,\n    2, 6, 1,\n    2, 6, 3,\n    2, 7, 3,\n    2, 8, 3, # (only the dog can throw the door)\n    2, 8, 4 \n), nrow = 15, ncol = 3, byrow = TRUE)\n\nrepresentations &lt;- diag(8)\nrownames(representations) &lt;- colnames(representations) &lt;- c(\"cat\", \"dog\", \"door\", \"rock\", \"see\", \"hear\", \"use\", \"throw\")\n\n# On each epoch, the model learns on a new randomly generated training sequence\nn_epochs &lt;- 10000\n# This is the number of times each subsequence will occur in each training sequence\nn_per_subsequence &lt;- 10\n\nlearning_rate &lt;- 0.1\nn_hidden &lt;- 7\n\nn_hidden_snapshots &lt;- 9\nepoch_to_snap &lt;- round(exp(seq(0, log(n_epochs), length.out = n_hidden_snapshots)))\nsnapshot_hidden_activation &lt;- array(0, dim = c(n_hidden_snapshots, nrow(representations), n_hidden), dimnames = list(\"epoch\" = epoch_to_snap, \"item\" = rownames(representations), \"hidden_unit\" = 1:n_hidden))\n\ni_to_h_assc_weights &lt;- matrix(rnorm(n = (ncol(representations) + n_hidden) * n_hidden), nrow = ncol(representations) + n_hidden, ncol = n_hidden) * 0.01\nh_to_o_assc_weights &lt;- matrix(rnorm(n = n_hidden * ncol(representations)), nrow = n_hidden, ncol = ncol(representations)) * 0.01\n\nhidden_baseline &lt;- rnorm(n = n_hidden) * 0.01\noutput_baseline &lt;- rnorm(n = ncol(representations)) * 0.01\n\nfor (epoch in 1:n_epochs) {\n    training_sequence &lt;- c(t(subsequences[sample(rep(1:nrow(subsequences), each = n_per_subsequence)),]))\n    \n    event &lt;- representations[training_sequence[1:(length(training_sequence) - 1)], , drop = FALSE]\n    \n    target &lt;- representations[training_sequence[2:length(training_sequence)], , drop = FALSE]\n    \n    d_h_to_o &lt;- 0 * h_to_o_assc_weights\n    d_i_to_h &lt;- 0 * i_to_h_assc_weights\n    \n    d_h_baseline &lt;- 0 * hidden_baseline\n    d_o_baseline &lt;- 0 * output_baseline\n    \n    prev_hidden_activation &lt;- rep(0, n_hidden)\n    \n    for (i in 1:nrow(event)) {\n        # Forward propagation (prediction)\n        this_input &lt;- c(event[i,], prev_hidden_activation)\n        hidden_activation &lt;- logistic_act(as.vector(this_input %*% i_to_h_assc_weights) + hidden_baseline)\n        output_activation &lt;- logistic_act(as.vector(hidden_activation %*% h_to_o_assc_weights) + output_baseline)\n    \n        # Compute error at output layer\n        output_error &lt;- target[i,] - output_activation\n    \n        # Back propagation (learning)\n    \n        # Derivative of error with respect to output unit activation\n        d_output_error &lt;- output_error * output_activation * (1 - output_activation)\n    \n        # Derivative of error with respect to hidden unit activation\n        d_hidden_error &lt;- as.vector(h_to_o_assc_weights %*% d_output_error) * hidden_activation * (1 - hidden_activation)\n    \n        # How much to adjust the associative weights between the hidden and output layers\n        d_h_to_o &lt;- d_h_to_o + learning_rate * outer(hidden_activation, d_output_error)\n    \n        # How much to adjust the associative weights between the input and hidden layers\n        d_i_to_h &lt;- d_i_to_h + learning_rate * outer(this_input, d_hidden_error)\n        \n        # How much to adjust the baseline of the output units\n        d_o_baseline &lt;- learning_rate * d_output_error\n        \n        # How much to adjust the baseline of the hidden units\n        d_h_baseline &lt;- learning_rate * d_hidden_error\n    \n        # Copy hidden unit activation\n        prev_hidden_activation &lt;- hidden_activation\n    }\n    \n    # Adjust weights\n    i_to_h_assc_weights &lt;- i_to_h_assc_weights + d_i_to_h\n    h_to_o_assc_weights &lt;- h_to_o_assc_weights + d_h_to_o\n    \n    hidden_baseline &lt;- hidden_baseline + d_h_baseline\n    output_baseline &lt;- output_baseline + d_o_baseline\n    \n    if (epoch %in% as.numeric(dimnames(snapshot_hidden_activation)[[1]])) {\n        prev_hidden_activation &lt;- rep(0, n_hidden)\n        \n        for (i in 1:nrow(representations)) {\n            snapshot_hidden_activation[as.character(epoch), i, ] &lt;- logistic_act(as.vector(c(representations[i,], prev_hidden_activation) %*% i_to_h_assc_weights) + hidden_baseline)\n        }\n    }\n}\n\narray2DF(snapshot_hidden_activation) %&gt;%\n    mutate(item = factor(item, levels = rownames(representations))) %&gt;%\n    mutate(hidden_unit = as.numeric(hidden_unit)) %&gt;%\n    mutate(epoch = factor(epoch, levels = as.character(sort(unique(as.numeric(epoch)))))) %&gt;%\n    ggplot(aes(x = hidden_unit, y = Value, color = item)) +\n    geom_line() +\n    scale_color_brewer(palette = \"Paired\") +\n    facet_wrap(\"epoch\", labeller = label_both)\n\n\n\n\n\n\n\n\n\nCode\nitem_mds &lt;- array(0, dim = c(n_hidden_snapshots, nrow(representations), 2), dimnames = list(\"epoch\" = epoch_to_snap, \"item\" = rownames(representations), \"dim\" = paste0(\"dim\", 1:2)))\n\nfor (epoch in dimnames(snapshot_hidden_activation)[[1]]) {\n    item_mds[epoch, , ] &lt;- cmdscale(d = dist(snapshot_hidden_activation[epoch,,]), k = 2)\n}\n\narray2DF(item_mds) %&gt;%\n    pivot_wider(id_cols = c(epoch, item), names_from = dim, values_from = Value) %&gt;%\n    mutate(item = factor(item, levels = rownames(representations))) %&gt;%\n    mutate(epoch = factor(epoch, levels = as.character(sort(unique(as.numeric(epoch)))))) %&gt;%\n    ggplot(aes(x = dim1, y = dim2, color = item)) +\n    geom_path() +\n    coord_equal() +\n    scale_color_brewer(palette = \"Paired\") +\n    labs(title = \"Evolution of internal representations with learning\", x = \"Dimension 1\", y = \"Dimension 2\", caption = \"Visualization by multidimensional scaling\")\n\n\n\n\n\n\n\n\n\nAs we can see, the model learns to distinguish between nouns and verbs fairly quickly. Same goes for subjects (cat, dog) and objects (door, rock). Eventually, the individual items get differentiated, like we saw with the concepts at the beginning of the chapter. The key difference is that the SRN has learned these concept representations by virtue of how they enable it to predict the next items in the sequence, not in terms of clearly labeled features.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Learning useful representations</span>"
    ]
  },
  {
    "objectID": "backprop.html#exercises",
    "href": "backprop.html#exercises",
    "title": "12  Learning useful representations",
    "section": "12.5 Exercises",
    "text": "12.5 Exercises\n\nAdapt the concept-feature learning model so that it operates “in reverse”. In other words, swap the target and event matrices. The resulting model will thus learn to assign a concept name (oak, pine, etc.) to a set of features. Leaving everything else the same, run the model and comment on the nature of the representations it learns within its hidden units and compare them to the representations learned by the model in the chapter.\nTry running the prototype-learning autoencoder model with different values of the p_change variable, leaving n_hidden set to 8. See if you can find a value of p_change at which the model no longer shows any evidence of learning the two prototypes. Comment on what sorts of representations the model has learned, if indeed it has learned any structure at all!\nDevise a learning situation that is relevant to your research and adapt one of the models from this chapter to apply it to that scenario. You will need to think about how you want to represent the features of each learning event and what the “target” is for the learner (e.g., are they learning to categorize events or choose an action, are they an “autoencoder”, are they learning to predict like an SRN?). You will also need to explore different values for the number of hidden units in the model, the learning rate, as well as the number of epochs and the number of learning events per epoch. In the end, describe the learning situation you devised and the settings required for your model to succeed in your learning task. Describe the nature of the hidden-layer representations the model seems to have learned as well as the trajectory (over epochs) by which the model appears to have learned those trajectories (e.g., did some distinctions get learned earlier than others?).\n\n\n\n\n\nElman, J. L. (1990). Finding structure in time. Cognitive Science, 14(2), 179–211. https://doi.org/10.1207/s15516709cog1402_1\n\n\nMcClelland, J. L., McNaughton, B. L., & O’Reilly, R. C. (1995). Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological Review, 102(3), 419–457.\n\n\nPosner, M. I., & Keele, S. W. (1968). On the genesis of abstract ideas. Journal of Experimental Psychology, 77, 353–363. https://doi.org/10.1037/h0025953\n\n\nRogers, T. T., & McClelland, J. L. (2004). Semantic cognition: A parallel distributed processing approach. MIT Press.\n\n\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations by error propagation. In D. E. Rumelhart & J. L. McClelland (Eds.), Parallel distributed processing: Vol. I. The MIT Press.",
    "crumbs": [
      "Coupling behavior to structured representations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Learning useful representations</span>"
    ]
  },
  {
    "objectID": "agent_based_models.html",
    "href": "agent_based_models.html",
    "title": "13  Agent-Based Models",
    "section": "",
    "text": "13.1 Cultural evolution\nThe models we have built so far have been models of how individuals perform a task or learn a representation. The environments with which these models have been engaged—the trials in an experiment or the materials in a domain to be learned—were provided to the model. This mimics the typical laboratory scenario where an experimenter defines the goals and givens for a participant and the job of the participant is to figure out how to achieve the goal with what they are given.\nHowever, in many real-world settings, the environment that people engage in is itself comprised of other people! (Okay, experimenters are people too, but the interactions between experimenters and participants are, by design, tightly constrained.) Each person has some characteristics, shaped by their genetic heritage, development, culture, prior life experiences, etc., that ultimately inform what actions they will take at any given moment. Those actions—and their consequences—then become part of the environment for everyone else around them, providing them information that may get encoded in memory and used to guide their decisions. Those other people will then take their own actions, which will also be shaped by their own motivations, preferences, beliefs, knowledge, etc. In turn, those actions and their consequences contribute to the environment with which everyone interacts. As we have seen, building computational cognitive models even of single individuals in a tightly-constrained environment is far from trivial. Modeling an entire group of people mutually interacting with one another is thus a daunting task!\nOf course, intrepid scientists find a way. The field of agent-based modeling helps us to understand how the interactions between cognitively sophisticated individuals—called “agents”—produces phenomena at the level of entire groups, communities, and populations. Thus, agent-based modeling acts as a bridge between computational cognitive models of individuals and theories of groups.\nIt will probably not be surprising to you that the inherent complexity of modeling groups of people means that the models for individual agents will tend to be much simpler than the individual-focused cognitive models we have built so far. In addition, agent-based models are primarily used for simulation purposes, to demonstrate the predictions of a theory, rather than for parameter estimation. This second property is due to a couple of factors: First, it is rarely possible to compute the likelihood (or other measure of goodness-of-fit) for agent-based models. Second, it is not always practical or sensible to define goodness-of-fit for many of the phenomena that agent-based models are designed to explain. When modeling individuals, we can rely on our ability to recruit many participants into a study and for each individual to engage in many trials of a task, thus allowing us identify systematic features of behavior that are the explanatory targets of our model. When modeling groups, unless they are very small, we cannot typically “replicate” the conditions under which the group is acting, making it difficult to quantify the degree to which groups systematically exhibit some quantifiable characteristic. Nonetheless, agent-based models are a powerful theoretical tool for understanding how qualitative group phenomena arise from interactions between individuals (Smith & Conrey, 2007).\nThe material in this chapter owes a lot to this fantastic resource on agent-based modeling in R. I encourage anyone interested to check it out!\nThe models below are focused on what can broadly be called cultural evolution, in that the models focus on the decisions by individuals to adopt (or not) a particular characteristic.\nIn biological evolution, we would consider how traits encoded by genes do or do not get passed on from one generation to the next, depending on whether or not those traits were beneficial to the organism’s reproductive success in its environment. By building models that allow for different selection mechanisms, different mating assortments, and different forms of mutation, we can examine those choices influence how the traits of a population will change over many generations.\nCultural evolution, on the other hand, does not require births and deaths to occur. Instead of a population changing as a function of organisms either surviving to reproduce or not, we will be modeling how populations change as a function of the choices made by the individual members of those populations. The kinds of choices we will consider could be concrete things like whether to support a political candidate, buy a product, use a service, etc. They could also represent more abstract choices, like the choice to like a particular style of music or a type of diet or a strategy for performing some task. To that last point, the choices made by the agents in our models need not be discrete—they may instead be choices to adjust a parameter with a particular meaning within the scope of the model.\nThe outcome of each choice amounts to a decision by the agent about whether to adopt a certain value for a trait. The evidence that agents use for making these choices comes from their interactions with other members of the population. In all of the models below, we imagine that each agent interacts with one or more other agents in discrete time-steps. At the end of each time step, each agent decides what value to take for its trait.\nWe will first explore models with discrete-valued traits and then models with continuous-valued traits. Conceptually, there is not much difference between these two types of traits, but there are both conceptual and technical differences in terms of how we model different mechanisms of social cognition with regard to these traits.",
    "crumbs": [
      "Cognitive models at larger and smaller scales",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Agent-Based Models</span>"
    ]
  },
  {
    "objectID": "agent_based_models.html#copying-your-neighbors",
    "href": "agent_based_models.html#copying-your-neighbors",
    "title": "13  Agent-Based Models",
    "section": "13.2 Copying your neighbors",
    "text": "13.2 Copying your neighbors\nWe will begin by constructing a very simple model of a population of agents interacting with one another. Each agent has a single trait that can take one of two values, A or B. In each time step, an agent picks at random another member of the population and adopts whatever value of the trait they have. We can think of this population as sort of like “myopic lemmings”. The point of this is not necessarily to be a realistic model of any interesting social process. But constructing this model will introduce us to the basic components of an agent-based model.\n\n13.2.1 Representing a population\nTo begin, we will need a data structure to store the trait values for each agent on each time step. Also, since this will be a simulation model, we will want to run the simulation several times to get a sense of the kinds of populations that are produced by each of the mechanisms we will explore. Thus, our data structure should have three dimensions: one to index each simulation, one to index each time-step, and one to index each agent in the population. Thus, a 3D array is called for, as illustrated below (where I also give the array and traits some handy names).\n\n\nCode\nn_pop &lt;- 1000\nmax_t &lt;- 100\nn_traits &lt;- 2\n\ntrait_names &lt;- LETTERS[1:n_traits]\n\npopulation &lt;- array(NA,\n                    dim = c(n_sims, max_t, n_pop),\n                    dimnames = list(\"sim_index\" = 1:n_sims,\n                                    \"t\" = 1:max_t,\n                                    \"member\" = 1:n_pop)\n                    )\n\n\n\n\n13.2.2 Initializing the population\nAt the moment, the population array is full of NAs. We need to initialize the population by assigning trait values to each agent on the first time step. These initial values will be random. We will want the flexibility to specify the probability that any agent is initialized with each possible value of the trait. For example, we could use the variable specification p0 &lt;- c(0.5, 0.5) to specify that, in a model with 2 n_traits, each member of the initial population has an equal chance of adopting either value. This is illustrated in a more general form below which refers to the n_traits variable.\n\n\nCode\np0 &lt;- rep(1 / n_traits, n_traits)\n\n# sim_index is a variable that indicates which simulation we are currently running\npopulation[sim_index, 1, ] &lt;- sample(trait_names, size = n_pop, replace = TRUE, prob = p0)\n\n\n\n\n13.2.3 Specifying interaction and updating processes\nFor each agent-based model we build, we need to specify (a) how the agents interact; and (b) how those interactions result in adopting a potentially updated trait value on the next time-step. For our first model, these two processes are very simple, as illustrated in the code below. Note that the code is wrapped in a for loop so that these processes recur on each time-step.\n\n\nCode\nfor (t in 2:max_t) {\n    # For each member of the population, sample a \"demonstrator\" at random\n    demonstrator &lt;- sample(n_pop, size = n_pop, replace = TRUE)\n    \n    # Each population member at time `t` adopts the trait from their `demonstrator`\n    # on the previous time-step `t - 1`\n    population[sim_index, t, ] &lt;- population[sim_index, t - 1, demonstrator]\n}\n\n\n\n\n13.2.4 The final function\nFinally, let’s put all the steps above into a function that we can call with different parameter values, as shown below. We will continue to add to this function and make alternate versions as we go. Note the use of the array2DF function at the end which converts our population array into a form that will be easier for us to visualize afterwards.\n\n\nCode\ndiscrete_trait_evolution &lt;- function(n_pop = 1000, max_t = 100, n_sims = 10, n_traits = 2, p0 = rep(1 / n_traits, n_traits), trait_names = LETTERS[1:n_traits]) {\n    population &lt;- array(NA,\n                        dim = c(n_sims, max_t, n_pop),\n                        dimnames = list(\"sim_index\" = 1:n_sims,\n                                        \"t\" = 1:max_t,\n                                        \"member\" = 1:n_pop)\n                        )\n    \n    for (sim_index in 1:n_sims) {\n        population[sim_index, 1, ] &lt;- sample(trait_names, size = n_pop, replace = TRUE, prob = p0)\n        \n        for (t in 2:max_t) {\n            demonstrator &lt;- sample(n_pop, size = n_pop, replace = TRUE)\n            \n            population[sim_index, t, ] &lt;- population[sim_index, t - 1, demonstrator]\n        }\n    }\n    \n    return(\n        as_tibble(array2DF(population, responseName = \"trait\")) %&gt;%\n            mutate(sim_index = as.numeric(sim_index), t = as.numeric(t), member = as.numeric(member))\n    )\n}\n\n\n\n\n13.2.5 Examples\nThe chunk of code below gives an example of running our discrete_trait_evolution function. After running the function, we create a summary data frame that, for each simulation run and each time-step, gives the proportion of the population with trait \"A\". This summary is then used to plot how this proportion evolves over time as a result of the simple interaction/updating processes we specified in our model.\n\n\nCode\nsim &lt;- discrete_trait_evolution(max_t = 1000)\n\nsim_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(p_A = mean(trait == \"A\"))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nsim_summary %&gt;%\n    ggplot(aes(x = t, y = p_A, color = factor(sim_index))) +\n    geom_line(aes(group = sim_index), linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1.5, color = \"black\") +\n    scale_color_discreterainbow() +\n    ylim(0, 1) +\n    labs(x = \"Time\", y = \"Proportion with trait A\", color = \"Sim.\")\n\n\n\n\n\n\n\n\n\nEach colored line represents a different simulation and the thick black line is the mean over all 10 simulations. We can see that the proportion of the population with trait A evolves according to basically a random walk until, at some point, every agent in the population has the same trait, either A (proportion 1) or B (proportion 0). At that point, no more cultural evolution can occur because each agent will only ever copy another agent with the same trait.\nJust like when we used a random walk to model deliberation during decision making, if we initialize the population to be biased toward one trait or another, the population is more likely to evolve to a point where the initially more common trait becomes dominant. This is shown in the simulations below.\n\n\nCode\nsim &lt;- discrete_trait_evolution(max_t = 1000, p0 = c(0.2, 0.8))\n\nsim_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(p_A = mean(trait == \"A\"))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nsim_summary %&gt;%\n    ggplot(aes(x = t, y = p_A, color = factor(sim_index))) +\n    geom_line(aes(group = sim_index), linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1.5, color = \"black\") +\n    scale_color_discreterainbow() +\n    ylim(0, 1) +\n    labs(x = \"Time\", y = \"Proportion with trait A\", color = \"Sim.\")",
    "crumbs": [
      "Cognitive models at larger and smaller scales",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Agent-Based Models</span>"
    ]
  },
  {
    "objectID": "agent_based_models.html#more-complex-interactions",
    "href": "agent_based_models.html#more-complex-interactions",
    "title": "13  Agent-Based Models",
    "section": "13.3 More complex interactions",
    "text": "13.3 More complex interactions\nWith the basic model framework established, we can flesh out the model with some more sophisticated mechanisms for interaction and updating. This will enable us to model more interesting kinds of processes that can lead to some more interesting kinds of phenomena.\n\n13.3.1 Varying probability of adopting a trait\nThe first thing we can try is to imagine that different trait values are more likely to be adopted than others. This might occur, for example, if the decision about whether to adopt the trait were based on its utility, ease, or some other reward. For example, if there may be two candidate strategies for performing a task—a slow and laborious method adopted by those with trait B and a faster and easier method adopted by those with trait A. If the agents can be assumed to be aware of this distinction, then an agent would be more likely to adopt a trait from a demonstrator with trait A than with trait B.\nThe code below amends our earlier function to include a new argument, p_adopt. By default, this argument is set to rep(1, n_traits) which reproduces the model we just built where an agent always adopts whatever trait their demonstrator has. The key changes are labeled with comments below.\n\n\nCode\ndiscrete_trait_evolution &lt;- function(n_pop = 1000, max_t = 100, n_sims = 10, n_traits = 2, p0 = rep(1 / n_traits, n_traits), trait_names = LETTERS[1:n_traits], p_adopt = rep(1, n_traits)) {\n    population &lt;- array(NA,\n                        dim = c(n_sims, max_t, n_pop),\n                        dimnames = list(\"sim_index\" = 1:n_sims,\n                                        \"t\" = 1:max_t,\n                                        \"member\" = 1:n_pop)\n                        )\n    \n    # Make sure `p_adopt` is a named vector so we can use those names later as indices\n    names(p_adopt) &lt;- trait_names\n    \n    for (sim_index in 1:n_sims) {\n        population[sim_index, 1, ] &lt;- sample(trait_names, size = n_pop, replace = TRUE, prob = p0)\n        \n        for (t in 2:max_t) {\n            demonstrator &lt;- sample(n_pop, size = n_pop, replace = TRUE)\n            demonstrator_trait &lt;- population[sim_index, t - 1, demonstrator]\n            \n            # For each member of the population, we use the `runif` trick to\n            # decide whether they should adopt the demonstrators trait or keep\n            # their old one.\n            population[sim_index, t, ] &lt;- if_else(\n                # This condition is TRUE with probability `p_adopt[demonstrator_trait]`\n                runif(n = n_pop) &lt; p_adopt[demonstrator_trait],\n                # If true, adopt the demonstrator's trait\n                population[sim_index, t - 1, demonstrator],\n                # Else, keep the old trait\n                population[sim_index, t - 1, ]\n            )\n        }\n    }\n    \n    return(\n        as_tibble(array2DF(population, responseName = \"trait\")) %&gt;%\n            mutate(sim_index = as.numeric(sim_index), t = as.numeric(t), member = as.numeric(member))\n    )\n}\n\n\nLet’s see what happens if people have a higher chance of adopting trait A than trait B. Specifically, we will set p_adopt = c(0.7, 0.5) so the probability of adopting trait A is 0.7 and is 0.5 for trait B. Note that these numbers don’t have to add up to one!\n\n\nCode\nsim &lt;- discrete_trait_evolution(max_t = 50, p_adopt = c(0.7, 0.5))\n\nsim_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(p_A = mean(trait == \"A\"))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nsim_summary %&gt;%\n    ggplot(aes(x = t, y = p_A, color = factor(sim_index))) +\n    geom_line(aes(group = sim_index), linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1.5, color = \"black\") +\n    scale_color_discreterainbow() +\n    ylim(0, 1) +\n    labs(x = \"Time\", y = \"Proportion with trait A\", color = \"Sim.\")\n\n\n\n\n\n\n\n\n\nAs we can see above, the population quickly evolves so that the “better” trait (A) dominates. This is true even if trait A is rare in the initial population, as shown below.\n\n\nCode\nsim &lt;- discrete_trait_evolution(max_t = 50, p_adopt = c(0.7, 0.5), p0 = c(0.1, 0.9))\n\nsim_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(p_A = mean(trait == \"A\"))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nsim_summary %&gt;%\n    ggplot(aes(x = t, y = p_A, color = factor(sim_index))) +\n    geom_line(aes(group = sim_index), linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1.5, color = \"black\") +\n    scale_color_discreterainbow() +\n    ylim(0, 1) +\n    labs(x = \"Time\", y = \"Proportion with trait A\", color = \"Sim.\")\n\n\n\n\n\n\n\n\n\n\n\n13.3.2 Multiple demonstrators and conformity\nThere’s no reason why each agent has to only pick one demonstrator with which to interact! Our next amendment to the model is thus to allow for more than one demonstrator per time-step. We will add an argument n_demonstrators to specify how many demonstrators each agent picks on each time-step.\nHaving multiple demonstrators opens the possibility that they do not all share the same trait—so how should we adjust our updating process to accommodate this? First, each agent will need to add up the number of their demonstrators that exhibit each trait. Then, the agent will adopt a trait in proportion to its frequency among those demonstrators.\nWe will take this opportunity to introduce a new parameter to the model, using \\(k\\) to represent it in math and conformity to represent it in code. This parameter will represent the tendency for each agent in the population to prefer to adopt traits that are more common among their demonstrators. Specifically, if \\(n_v\\) is the number of demonstrators with trait \\(v\\), the probability of adopting that trait will be \\[\np_v = \\frac{n_v^k}{\\sum_{u = 1}^{N_T} n_u^k}\n\\] where \\(N_T\\) is the total number of possible trait values.\nTo see the effect of the \\(k\\) parameter, the graph below illustrates the probability of adopting trait \\(v\\) in a situation where there are two possible traits and there are 10 demonstrators.\n\n\nCode\nexpand_grid(n_v = seq(0, 10), k = seq(0, 5)) %&gt;%\n    mutate(p_v = n_v^k / (n_v^k + (10 - n_v)^k)) %&gt;%\n    ggplot(aes(x = n_v, y = p_v, color = k, group = k)) +\n    geom_line() +\n    scale_color_viridis_c() +\n    labs(x = expression(n[v]), y = expression(p[v]), color = \"k\")\n\n\n\n\n\n\n\n\n\nIf \\(k = 0\\), then the agent totally ignores the frequency of the traits among the demonstrators. If \\(k = 1\\), the probability of adopting the trait is simply the prevalence of the trait among the demonstrators. As \\(k\\) increases above 1, the probability of adopting the more common trait increases when there is a mixture of traits among the demonstrators (e.g., when only 6 out of 10 exhibit trait \\(v\\)).\nThis updating process is implemented in the code below—note the comments explaining what each line does. Some of these are a bit ugly, but serve the purpose of making the model run much faster!\n\n\nCode\ndiscrete_trait_evolution &lt;- function(n_pop = 1000, max_t = 100, n_sims = 10, n_traits = 2, p0 = rep(1 / n_traits, n_traits), trait_names = LETTERS[1:n_traits], p_adopt = rep(1, n_traits), n_demonstrators = 1, conformity = 1) {\n    population &lt;- array(NA,\n                        dim = c(n_sims, max_t, n_pop),\n                        dimnames = list(\"sim_index\" = 1:n_sims,\n                                        \"t\" = 1:max_t,\n                                        \"member\" = 1:n_pop)\n                        )\n    \n    names(p_adopt) &lt;- trait_names\n    \n    p_adopt_mat &lt;- matrix(p_adopt, nrow = n_pop, ncol = n_traits, byrow = TRUE)\n    \n    for (sim_index in 1:n_sims) {\n        population[sim_index, 1, ] &lt;- sample(trait_names, size = n_pop, replace = TRUE, prob = p0)\n        \n        for (t in 2:max_t) {\n            demonstrators &lt;- sample(n_pop, size = n_pop * n_demonstrators, replace = TRUE)\n            \n            demonstrator_trait &lt;- matrix(\n                population[sim_index, t - 1, demonstrators],\n                nrow = n_pop, ncol = n_demonstrators\n            )\n            \n            # Count the number of times each trait appears among the demonstrators for each agent, weighted by the probability of adoption (p_adopt_mat)\n            demo_counts &lt;- t(apply(demonstrator_trait, MARGIN = 1, FUN = function(traits) table(factor(traits, levels = trait_names)))) * p_adopt_mat\n            \n            # Apply the conformity transformation to find the probability of choosing\n            p_demo &lt;- demo_counts^conformity / rowSums(demo_counts^conformity)\n            \n            # For each agent, pick a new trait in proportion to p_demo\n            population[sim_index, t, ] &lt;- apply(p_demo, MARGIN = 1, FUN = function(p) sample(trait_names, size = 1, prob = p))\n        }\n    }\n    \n    return(\n        as_tibble(array2DF(population, responseName = \"trait\")) %&gt;%\n            mutate(sim_index = as.numeric(sim_index), t = as.numeric(t), member = as.numeric(member))\n    )\n}\n\n\nWhen the conformity parameter is set to 1, then we just replicate the first model we built. In that case, the probability of adopting a trait is directly proportional to its prevalence. This unbiased meandering is shown in the simulations below.\n\n\nCode\nsim &lt;- discrete_trait_evolution(max_t = 50, n_demonstrators = 3, conformity = 1)\n\nsim_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(p_A = mean(trait == \"A\"))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nsim_summary %&gt;%\n    ggplot(aes(x = t, y = p_A, color = factor(sim_index))) +\n    geom_line(aes(group = sim_index), linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1.5, color = \"black\") +\n    scale_color_discreterainbow() +\n    ylim(0, 1) +\n    labs(x = \"Time\", y = \"Proportion with trait A\", color = \"Sim.\")\n\n\n\n\n\n\n\n\n\nOn the other hand, when conformity is greater than one, the population tends to evolve to have one dominant trait, but it is effectively random which trait wins out each time, as shown below.\n\n\nCode\nsim &lt;- discrete_trait_evolution(max_t = 50, n_demonstrators = 3, conformity = 2)\n\nsim_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(p_A = mean(trait == \"A\"))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nsim_summary %&gt;%\n    ggplot(aes(x = t, y = p_A, color = factor(sim_index))) +\n    geom_line(aes(group = sim_index), linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1.5, color = \"black\") +\n    scale_color_discreterainbow() +\n    ylim(0, 1) +\n    labs(x = \"Time\", y = \"Proportion with trait A\", color = \"Sim.\")\n\n\n\n\n\n\n\n\n\nWhichever trait manages to become more common will then tend to be adopted more, resulting in a “rich-get-richer” feedback cycle. These dynamics are even more apparent if we bias the initial population so that one trait tends to be more common than the other, as illustrated below.\n\n\nCode\nsim &lt;- discrete_trait_evolution(max_t = 50, n_demonstrators = 3, conformity = 2, p0 = c(0.4, 0.6))\n\nsim_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(p_A = mean(trait == \"A\"))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nsim_summary %&gt;%\n    ggplot(aes(x = t, y = p_A, color = factor(sim_index))) +\n    geom_line(aes(group = sim_index), linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1.5, color = \"black\") +\n    scale_color_discreterainbow() +\n    ylim(0, 1) +\n    labs(x = \"Time\", y = \"Proportion with trait A\", color = \"Sim.\")\n\n\n\n\n\n\n\n\n\n\n\n13.3.3 Demonstrators of varying status\nIn the models we’ve built so far, we’ve assumed that every member of the population has an equal chance of being picked as one of the demonstrators picked by an agent. Often, though, it would be better to think that demonstrators are selected on the basis of some characteristic that makes them more “prominent” than others. For example, some agents might be more sociable, interacting with more agents. Or some agents may have more resources to send messages to other agents (e.g., if someone is a popular influencer). Or some agents might be more successful or attractive than others, causing other agents to pay more attention to them. The point is that some agents may be more “prominent” than others, though the reason why will depend on the particular scenario we are modeling.\nFor the purposes of our model, we will now add a property to each agent called status. This will be a number between zero and one that represents the relative likelihood that an agent gets picked as a demonstrator on each time-step. There are many other ways we could operationalize this construct, of course. For example, we could treat status as discrete (e.g., high vs. low status) or we could treat status as unbounded rather than capped at one. The approach here is just one option!\n\n13.3.3.1 Representing status\nTo model varying status, we will use a second array to keep track of the status of each agent on each time-step of each simulation run. This array has exactly the same structure as the population array we’ve been using so far to keep track of each agent’s trait value, as shown below:\n\n\nCode\nstatus &lt;- array(NA,\n                dim = c(n_sims, max_t, n_pop),\n                dimnames = list(\"sim_index\" = 1:n_sims,\n                                \"t\" = 1:max_t,\n                                \"member\" = 1:n_pop)\n                )\n\n\nIn our initial set of simulations, we will assume that each agent’s status is fixed over time, so technically the above is “overkill”. However, later on we will look at models that allow status to vary over time, so adopting this “over-powered” approach saves us some work down the line.\n\n\n13.3.3.2 Initializing status\nThere are many ways we could assign initial status values to each agent, but we take the following approach. We will introduce a parameter mean_init_status that varies between 0 and 1. We will use this parameter to define the parameters of a Beta distribution from which we will sample the initial status values, as shown in the code below which will appear at the beginning of each simulation run. We also do a correction to ensure that there is at least one agent with the maximum status value of 1, so that we don’t accidentally have simulations where all agents have low status values.\n\n\nCode\n# First, sample initial status values from Beta distribution\nstatus[sim_index, 1, ] &lt;- rbeta(n = n_pop, shape1 = mean_init_status, shape2 = 1 - mean_init_status)\n# Then divide by the maximum sampled value so that there is at least one agent with a status of 1\nstatus[sim_index, 1, ] &lt;- status[sim_index, 1, ] / max(status[sim_index, 1, ])\n\n\nThe plot below shows how the mean_init_status parameter affects the distribution of status across the population. Note that there are always modes at 0 and 1, so we will expect so find “clusters” of high and low status agents as well as a smattering of agents with moderate status between 0 and 1. When mean_init_status is less than 0.5, more agents will fall on the low end than the high end, modeling a situation where there are comparatively few high status agents. The opposite occurs when mean_init_status is greater than 0.5.\n\n\nCode\nexpand_grid(status = seq(0.01, 0.99, length.out = 101), mean_init_status = seq(0.1, 0.9, length.out = 5)) %&gt;%\n    mutate(d = dbeta(status, mean_init_status, 1 - mean_init_status)) %&gt;%\n    ggplot(aes(x = status, y = d, color = mean_init_status, group = mean_init_status)) +\n    geom_line() +\n    scale_color_viridis_c() +\n    labs(x = \"Status\", y = \"Density\", color = \"Mean initial\\nstatus\", title = \"Distribution of initial status across population\")\n\n\n\n\n\n\n\n\n\nFinally, if we set mean_init_status to 1, the resulting distribution is a “spike” at 1. In that circumstance, all agents will get a status of 1, effectively eliminating any effect of status.\n\n\n13.3.3.3 Examples\nThe code below is our updated simulation function. The role of an agent’s status is indicated by the comment, where status[sim_index, t - 1, ] is used to define the probability with which an agent is sampled as a demonstrator. Note that the sample function automatically normalizes the prob argument to sum to one, so we don’t need to do that ourselves.\n\n\nCode\ndiscrete_trait_evolution &lt;- function(n_pop = 1000, max_t = 100, n_sims = 10, n_traits = 2, p0 = rep(1 / n_traits, n_traits), trait_names = LETTERS[1:n_traits], p_adopt = rep(1, n_traits), n_demonstrators = 1, conformity = 1, mean_init_status = 1) {\n    population &lt;- array(NA,\n                        dim = c(n_sims, max_t, n_pop),\n                        dimnames = list(\"sim_index\" = 1:n_sims,\n                                        \"t\" = 1:max_t,\n                                        \"member\" = 1:n_pop)\n                        )\n    \n    status &lt;- array(NA,\n                        dim = c(n_sims, max_t, n_pop),\n                        dimnames = list(\"sim_index\" = 1:n_sims,\n                                        \"t\" = 1:max_t,\n                                        \"member\" = 1:n_pop)\n                        )\n    \n    names(p_adopt) &lt;- trait_names\n    \n    p_adopt_mat &lt;- matrix(p_adopt, nrow = n_pop, ncol = n_traits, byrow = TRUE)\n    \n    for (sim_index in 1:n_sims) {\n        population[sim_index, 1, ] &lt;- sample(trait_names, size = n_pop, replace = TRUE, prob = p0)\n        status[sim_index, 1, ] &lt;- rbeta(n = n_pop, shape1 = mean_init_status, shape2 = 1 - mean_init_status)\n        \n        status[sim_index, 1, ] &lt;- status[sim_index, 1, ] / max(status[sim_index, 1, ])\n        \n        for (t in 2:max_t) {\n            # Probability of sampling is proportional to status\n            demonstrators &lt;- sample(n_pop, size = n_pop * n_demonstrators, replace = TRUE, prob = status[sim_index, t - 1, ])\n            \n            demonstrator_trait &lt;- matrix(\n                population[sim_index, t - 1, demonstrators],\n                nrow = n_pop, ncol = n_demonstrators\n            )\n            \n            demo_counts &lt;- t(apply(demonstrator_trait, MARGIN = 1, FUN = function(traits) table(factor(traits, levels = trait_names)))) * p_adopt_mat\n            \n            p_demo &lt;- demo_counts^conformity / rowSums(demo_counts^conformity)\n            \n            population[sim_index, t, ] &lt;- apply(p_demo, MARGIN = 1, FUN = function(p) sample(trait_names, size = 1, prob = p))\n            \n            status[sim_index, t, ] &lt;- status[sim_index, t - 1, ]\n        }\n    }\n    \n    return(\n        full_join(\n            as_tibble(array2DF(population, responseName = \"trait\")),\n            as_tibble(array2DF(status, responseName = \"status\")),\n            by = join_by(sim_index, t, member)\n        ) %&gt;%\n            mutate(sim_index = as.numeric(sim_index), t = as.numeric(t), member = as.numeric(member))\n    )\n}\n\n\nNow let’s see what happens to a population when people tend to adopt the traits of high-status agents more than low-status agents. In the following simulation, we assume only a single demonstrator per agent and equal base rates for each of two traits in the initial population. To produce a large disparity in status, we set mean_init_status to 0.05, so that most agents have low status and only a few have high status.\n\n\nCode\nsim &lt;- discrete_trait_evolution(n_pop = 1000, max_t = 200, mean_init_status = 0.05)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(p_A = mean(trait == \"A\"))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\np_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = p_A, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    ylim(0, 1) +\n    labs(x = \"Time\", y = \"Proportion with trait A\", color = \"Sim.\")\n\ninit_plot &lt;- sim %&gt;%\n    filter(t == 1) %&gt;%\n    group_by(sim_index, trait) %&gt;%\n    summarize(mean_status = mean(status)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = mean_status, color = factor(sim_index), shape = trait)) +\n    geom_point(fill = \"white\") +\n    scale_color_discreterainbow(guide = \"none\") +\n    scale_shape_manual(values = c(\"A\" = 19, \"B\" = 21)) +\n    labs(x = \"Simulation\", y = \"Mean status\", color = \"Sim.\", shape = \"Trait\", subtitle = \"Mean status as a function of initial trait\")\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nprint(p_plot + init_plot + plot_layout(nrow = 1, widths = c(1.5, 1), guides = \"collect\"))\n\n\n\n\n\n\n\n\n\nAs you can see, we’ve made two plots to visualize our results for reasons that will become clear shortly. The left plot shows that, as in the first set of simulations we ran, the population will tend to evolve until one trait becomes dominant. The right plot shows the mean status among agents grouped by their initial trait values. The right plot helps explain which of the two traits eventually becomes dominant. If one trait is more closely associated with high-status individuals at the beginning of the simulation, then that trait is the one that is most likely to dominate. This is because agents will tend to emulate high-status agents.",
    "crumbs": [
      "Cognitive models at larger and smaller scales",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Agent-Based Models</span>"
    ]
  },
  {
    "objectID": "agent_based_models.html#continuous-valued-traits",
    "href": "agent_based_models.html#continuous-valued-traits",
    "title": "13  Agent-Based Models",
    "section": "13.4 Continuous-valued traits",
    "text": "13.4 Continuous-valued traits\nAs noted earlier, we can also build agent-based models where the agents’ traits take continuous values instead of discrete values. Continuous traits might represent the degree of belief in a theory or cultural tenet, degree of preference for a political candidate or work of art, or even a parameter in a cognitive model. Modeling the evolution of continuous traits involves making some changes to our simulation code, although we will see that many of the same concepts apply. The main difference between continuous and discrete traits is that, with continuous traits, we can model phenomena that depend on similarity between agents, as measured by how different their traits are.\n\n13.4.1 Initializing the population\nWith discrete traits, we assigned initial values to agents using the sample function. In the following model, we will instead assume that initial trait values are sampled from a normal distribution. For simplicity we will assume that the mean of this normal distribution is zero and introduce an (optional) parameter init_trait_sd with a default value of 1 that represents the standard deviation of the initial trait distribution. Of course, we could choose different distributions like a uniform or a Beta distribution or a Gamma distribution depending on what the trait was intended to represent. And we could add additional parameters if we wanted, too. Again, what we are doing here is just one of many viable approaches. Nonetheless, here’s how we will initialize the trait values on the first time-step of the simulation indexed by sim_index:\n\n\nCode\npopulation[sim_index, 1, ] &lt;- rnorm(n = n_pop, mean = 0, sd = init_trait_sd)\n\n\n\n\n13.4.2 Multiple demonstrators and conformity\nOur model will still allow us to simulate agents who sample multiple demonstrators, rather than just one. To model conformity in this situation, we can’t just look at the proportion of demonstrators with each trait. Instead, we will model conformity by assigning a weight to each demonstrator that is proportional to how far their trait value is from the mean of the demonstrators.\nIn the code snippet below, demonstrator_trait is a matrix with one row per agent and one column per demonstrator. Therefore, rowMeans(demonstrator_trait) gives a vector of the average trait value for each agent’s sampled demonstrators. We can then find each demonstrators squared deviation from their mean by writing (demonstrator_trait - matrix(rowMeans(demonstrator_trait), nrow = n_pop, ncol = n_demonstrators, byrow = FALSE))^2. Finally, since we want to give more weight to demonstrators close to the mean, we will use a similar exponential transformation to the one we used with the EBRW.\nMathematically, if \\(\\bar{x}_{\\mathcal{D}}\\) is the mean trait value among the set of demonstrators \\(\\mathcal{D}\\), then the weight for demonstrator \\(j\\) is: \\[\nw_j = \\exp \\left[ -k \\frac{\\left(x_j - \\bar{x}_{\\mathcal{D}} \\right)^2}{2} \\right]\n\\] where the parameter \\(k\\) is a nonnegative number representing a preference for conformity and the division by 2 is to maintain convention with how the Gaussian distribution is defined. The graph below illustrates how the weight changes as a function of distance from the mean demonstrator trait and conformity parameter \\(k\\).\n\n\nCode\nexpand_grid(d = seq(0, 5, length.out = 101), k = seq(0, 5)) %&gt;%\n    mutate(w = exp(-k * d^2 / 2)) %&gt;%\n    ggplot(aes(x = d, y = w, color = k, group = k)) +\n    geom_line() +\n    scale_color_viridis_c() +\n    labs(x = expression(x[j] - bar(x)[D]), y = expression(w[j]), color = \"k\")\n\n\n\n\n\n\n\n\n\nWhen \\(k = 0\\), all demonstrators get equal weight. The larger \\(k\\) gets, the less weight is given to demonstrators that are farther from the mean. Note that this means that, if none of the demonstrators are close to the mean, they will all be given comparatively little weight. A preference for conformity means that an agent will only give weight to demonstrators who tend to cluster around a common value.\nIn code, we write the weighting function like so (where conformity is the code equivalent of \\(k\\)):\n\n\nCode\ndemo_weight &lt;- exp(-conformity * (demonstrator_trait - matrix(rowMeans(demonstrator_trait), nrow = n_pop, ncol = n_demonstrators, byrow = FALSE))^2 / 2)\n\n\n\n\n13.4.3 Updating traits\nThe final change to our model is in how agents update their traits. Instead of simply copying a value from a demonstrator, an agent will be nudged toward the value of the demonstrators. Specifically, the new trait value will be a weighted average of their previous value and those of the demonstrator(s). Mathematically, we can write this as \\[\nx_{i}(t) = \\frac{w_{\\text{Self}} x_{i}(t - 1) + \\sum_{j \\in \\mathcal{D}} w_j x_j(t - 1)}{w_{\\text{Self}} + \\sum_{j \\in D} w_j}\n\\] where \\(x_i(t - 1)\\) is agent \\(i\\)’s trait value at time-step \\(t - 1\\), \\(w_{text{Self}}\\) is the weight given to the agent’s own previous trait value, \\(\\mathcal{D}\\) is the set of demonstrators for agent \\(i\\), and each \\(w_j\\) is the weight given to that demonstrator according to the “conformity” rule described above.\nIn code, we write this updating process like so, where self_weight is code for \\(w_{\\text{Self}}\\) above.\n\n\nCode\npopulation[sim_index, t, ] &lt;- (self_weight * population[sim_index, t - 1, ] + rowSums(demo_weight * demonstrator_trait)) / (self_weight + rowSums(demo_weight))\n\n\n\n\n13.4.4 The final function\nThe code below illustrates our final simulation function for dealing with continuous-valued traits. You’ll notice that much of it is the same as our discrete_trait_evolution function except for the changes noted earlier in this section.\n\n\nCode\ncontinuous_trait_evolution &lt;- function(n_pop = 1000, max_t = 100, n_sims = 10, init_trait_sd = 1, self_weight = 1, n_demonstrators = 1, conformity = 0, mean_init_status = 1) {\n    population &lt;- array(NA,\n                        dim = c(n_sims, max_t, n_pop),\n                        dimnames = list(\"sim_index\" = 1:n_sims,\n                                        \"t\" = 1:max_t,\n                                        \"member\" = 1:n_pop)\n                        )\n\n    status &lt;- array(NA,\n                    dim = c(n_sims, max_t, n_pop),\n                    dimnames = list(\"sim_index\" = 1:n_sims,\n                                    \"t\" = 1:max_t,\n                                    \"member\" = 1:n_pop)\n                    )\n\n    for (sim_index in 1:n_sims) {\n        population[sim_index, 1, ] &lt;- rnorm(n = n_pop, mean = 0, sd = init_trait_sd)\n        status[sim_index, 1, ] &lt;- rbeta(n = n_pop, shape1 = mean_init_status, shape2 = 1 - mean_init_status)\n\n        status[sim_index, 1, ] &lt;- status[sim_index, 1, ] / max(status[sim_index, 1, ])\n\n        for (t in 2:max_t) {\n            demonstrators &lt;- sample(n_pop, size = n_pop * n_demonstrators, replace = TRUE, prob = status[sim_index, t - 1, ])\n\n            demonstrator_trait &lt;- matrix(\n                population[sim_index, t - 1, demonstrators],\n                nrow = n_pop, ncol = n_demonstrators\n            )\n            \n            demo_weight &lt;- exp(-conformity * (demonstrator_trait - matrix(rowMeans(demonstrator_trait), nrow = n_pop, ncol = n_demonstrators, byrow = FALSE))^2 / 2)\n            \n            population[sim_index, t, ] &lt;- (self_weight * population[sim_index, t - 1, ] + rowSums(demo_weight * demonstrator_trait)) / (self_weight + rowSums(demo_weight))\n\n            status[sim_index, t, ] &lt;- status[sim_index, t - 1, ]\n        }\n    }\n\n    return(\n        full_join(\n            as_tibble(array2DF(population, responseName = \"trait\")),\n            as_tibble(array2DF(status, responseName = \"status\")),\n            by = join_by(sim_index, t, member)\n        ) %&gt;%\n            mutate(sim_index = as.numeric(sim_index), t = as.numeric(t), member = as.numeric(member))\n    )\n}\n\n\n\n\n13.4.5 Examples\nWe can largely replicate the discrete trait-copying model we used earlier if we set the self_weight parameter to 0, so that agents just adopt the trait value of whatever demonstrator they happen to select.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 1000, self_weight = 0)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait), sd_trait = sd(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nsd_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = sd_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    expand_limits(y = 0) +\n    labs(x = \"Time\", y = \"Trait value standard deviation\", color = \"Sim.\")\n\nmean_plot + sd_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nThe graphs above show both the mean (left) and standard deviation (right) of the trait values in the population over time. Although the mean trait values meander around, there is a tendency for the standard deviation to diminish over time. This is because the copying process causes certain trait values to drop out of the population over time, until eventually all agents adopt the same trait. This mimics the same convergence behavior that we saw with the discrete trait models above.\nOne might naively assume that if we set the self_weight to 1, so that agents effectively adopt a “compromise” between their trait values and those of their demonstrators, that this would slow convergence. In fact, just the opposite! As shown below, populations quickly converge on a “consensus” value that is close to the population average of zero.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 1000, self_weight = 1)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait), sd_trait = sd(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nsd_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = sd_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    expand_limits(y = 0) +\n    labs(x = \"Time\", y = \"Trait value standard deviation\", color = \"Sim.\")\n\nmean_plot + sd_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nFinally, a preference for conformity among demonstrators also leads to a rapid convergence of the population onto a single consensus value, as shown below.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 50, n_demonstrators = 3, conformity = 2)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait), sd_trait = sd(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nsd_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = sd_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    expand_limits(y = 0) +\n    labs(x = \"Time\", y = \"Trait value standard deviation\", color = \"Sim.\")\n\nmean_plot + sd_plot + plot_layout(nrow = 1, guides = \"collect\")",
    "crumbs": [
      "Cognitive models at larger and smaller scales",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Agent-Based Models</span>"
    ]
  },
  {
    "objectID": "agent_based_models.html#popularity-of-the-fittest",
    "href": "agent_based_models.html#popularity-of-the-fittest",
    "title": "13  Agent-Based Models",
    "section": "13.5 Popularity of the fittest",
    "text": "13.5 Popularity of the fittest\nEarlier, we modeled situations in which agents were more likely to pick high-status as opposed to low-status agents as demonstrators. So far, we have treated status as something that each agent simply possessed and which did not change over time. Modeling continuous-valued traits gives us a natural way to allow status to vary as a function of the relative fitness that an agent is able to achieve. “Fitness” could mean various things—it might reflect an objective increase in the agent’s success, like being happier, or achieving more, or being able to do something better, or having more stuff. It might also reflect a latent preference among members of the population for some property, like an aesthetic preference that is somehow inherent in each agent’s beliefs. Regardless of the cause, if an agent’s status is related to its trait value, then it would make sense for agents to prefer to choose high-status agents as demonstrators. That way, an agent will be more likely to improve their own status.\nAfter seeing an example of how status-based selection works with continuous traits, we will build in one way to implement the idea that there is an “ideal” trait value that agents seek to adopt. We will then build into the model a form of “innovation” that enables the population to converge toward this ideal more consistently.\n\n13.5.1 Convergence to high-status trait values\nLike we saw with discrete-valued traits, continuous-valued traits in a population tend to converge on trait values that are more common among high-status individuals. This is illustrated in the example below, where we set mean_init_status to 0.05, so that most individuals have low status and only a few have high status.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 50, mean_init_status = 0.05)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait), sd_trait = sd(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_trait_range &lt;- range(trait_summary$mean_trait)\n\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    coord_cartesian(ylim = mean_trait_range) +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nsd_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = sd_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    expand_limits(y = 0) +\n    labs(x = \"Time\", y = \"Trait value standard deviation\", color = \"Sim.\")\n\ninit_trait_plot &lt;- sim %&gt;%\n    filter(t == 1) %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(weighted_mean_trait = sum(trait * status) / sum(status)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = weighted_mean_trait, color = factor(sim_index))) +\n    geom_point() +\n    scale_color_discreterainbow(guide = \"none\") +\n    coord_cartesian(ylim = mean_trait_range) +\n    labs(x = \"Simulation index\", y = \"Status-weighted mean initial trait value\")\n\nmean_plot + init_trait_plot + sd_plot + plot_layout(nrow = 1, widths = c(1.5, 1, 1), guides = \"collect\")\n\n\n\n\n\n\n\n\n\nAs shown in the left and right graphs, the trait values in the population tend to “collapse” to a single value fairly quickly. The middle graph shows the average trait value in the initial population, weighted by the status of each agent (which again does not change over time). Similar to what we saw with discrete-valued traits, it looks like the value that the population converges on is biased in the direction of the values that were more common among high-status individuals. This behavior is a bit easier to see if we make the initial trait values more variable, so that there is a larger difference between them. In the simulation below, we do this by setting init_trait_sd to a value larger than the default value of 1.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 50, mean_init_status = 0.05, init_trait_sd = 5)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait), sd_trait = sd(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_trait_range &lt;- range(trait_summary$mean_trait)\n\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    coord_cartesian(ylim = mean_trait_range) +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nsd_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = sd_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    expand_limits(y = 0) +\n    labs(x = \"Time\", y = \"Trait value standard deviation\", color = \"Sim.\")\n\ninit_trait_plot &lt;- sim %&gt;%\n    filter(t == 1) %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(weighted_mean_trait = sum(trait * status) / sum(status)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = weighted_mean_trait, color = factor(sim_index))) +\n    geom_point() +\n    scale_color_discreterainbow(guide = \"none\") +\n    coord_cartesian(ylim = mean_trait_range) +\n    labs(x = \"Simulation index\", y = \"Status-weighted mean initial trait value\")\n\nmean_plot + init_trait_plot + sd_plot + plot_layout(nrow = 1, widths = c(1.5, 1, 1), guides = \"collect\")\n\n\n\n\n\n\n\n\n\nThe main thing to take away from these examples is that, when demonstrators are selected in proportion to their status, agents are more likely to adopt the traits of high-status individuals. This leads to the population converging on a trait value that is biased toward whatever values happened to be held by the high-status individuals initially. This is just like what we saw with discrete-valued traits.\n\n\n13.5.2 Defining an “ideal” trait value\nNow we introduce the idea that status is not merely random, but is instead a function of how close someone’s trait is to an “ideal” value. As noted above, a value might be “ideal” because it has some objectively good consequences (like doing something more easily or more successfully) or because there is some latent preference among agents for a particular value.\nTo model this idea, we will set the status of each individual to be inversely proportional to the squared difference between their trait value and the ideal trait value. In math, if \\(x_i(t)\\) is the trait value of agent \\(i\\) at time \\(t\\) and \\(\\tilde{x}\\) is the ideal trait value, then the status of agent \\(i\\) at time \\(t\\) is \\[\ns_i(t) = \\exp \\left[-\\phi \\frac{\\left(x_i(t) - \\tilde{x} \\right)^2}{2} \\right]\n\\] where \\(\\phi\\) is a parameter representing how quickly status drops off as a function of distance from the ideal trait value. Basically, this is the same functional form we used to define conformity earlier!\nThis is implemented in the expanded function below, where the ideal trait value \\(\\tilde{x}\\) is called ideal_trait_value and \\(\\phi\\) is called ideal_strength. Note that, by default, the ideal trait value is NA—if this is the case, the function will revert to simulating status as a static random property like it was earlier.\n\n\nCode\ncontinuous_trait_evolution &lt;- function(n_pop = 1000, max_t = 100, n_sims = 10, init_trait_sd = 1, self_weight = 1, n_demonstrators = 1, conformity = 0, mean_init_status = 1, ideal_trait_value = NA, ideal_strength = 1) {\n    population &lt;- array(NA,\n                        dim = c(n_sims, max_t, n_pop),\n                        dimnames = list(\"sim_index\" = 1:n_sims,\n                                        \"t\" = 1:max_t,\n                                        \"member\" = 1:n_pop)\n                        )\n\n    status &lt;- array(NA,\n                    dim = c(n_sims, max_t, n_pop),\n                    dimnames = list(\"sim_index\" = 1:n_sims,\n                                    \"t\" = 1:max_t,\n                                    \"member\" = 1:n_pop)\n                    )\n\n    for (sim_index in 1:n_sims) {\n        population[sim_index, 1, ] &lt;- rnorm(n = n_pop, mean = 0, sd = init_trait_sd)\n        \n        if (is.na(ideal_trait_value)) {\n            status[sim_index, 1, ] &lt;- rbeta(n = n_pop, shape1 = mean_init_status, shape2 = 1 - mean_init_status)\n    \n            status[sim_index, 1, ] &lt;- status[sim_index, 1, ] / max(status[sim_index, 1, ])\n        } else {\n            status[sim_index, 1, ] &lt;- exp(-ideal_strength * (population[sim_index, 1, ] - ideal_trait_value)^2 / 2)\n        }\n\n        for (t in 2:max_t) {\n            demonstrators &lt;- sample(n_pop, size = n_pop * n_demonstrators, replace = TRUE, prob = status[sim_index, t - 1, ])\n\n            demonstrator_trait &lt;- matrix(\n                population[sim_index, t - 1, demonstrators],\n                nrow = n_pop, ncol = n_demonstrators\n            )\n            \n            demo_weight &lt;- exp(-conformity * (demonstrator_trait - matrix(rowMeans(demonstrator_trait), nrow = n_pop, ncol = n_demonstrators, byrow = FALSE))^2 / 2)\n            \n            population[sim_index, t, ] &lt;- (self_weight * population[sim_index, t - 1, ] + rowSums(demo_weight * demonstrator_trait)) / (self_weight + rowSums(demo_weight))\n            \n            if (is.na(ideal_trait_value)) {\n                status[sim_index, t, ] &lt;- status[sim_index, t - 1, ]\n            } else {\n                status[sim_index, t, ] &lt;- exp(-ideal_strength * (population[sim_index, t, ] - ideal_trait_value)^2 / 2)\n            }\n        }\n    }\n\n    return(\n        full_join(\n            as_tibble(array2DF(population, responseName = \"trait\")),\n            as_tibble(array2DF(status, responseName = \"status\")),\n            by = join_by(sim_index, t, member)\n        ) %&gt;%\n            mutate(sim_index = as.numeric(sim_index), t = as.numeric(t), member = as.numeric(member))\n    )\n}\n\n\nTo see how this works, let’s run a set of simulations setting the ideal_trait_value to 2. Recall that the initial trait values are sampled from a normal distribution with a mean of 0 and (by default) a standard deviation of 1. Therefore, this situation models what might happen if most agents are initially pretty far from the ideal trait, but a few happen to be nearby.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 50, ideal_trait_value = 2)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait), sd_trait = sd(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nsd_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = sd_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    expand_limits(y = 0) +\n    labs(x = \"Time\", y = \"Trait value standard deviation\", color = \"Sim.\")\n\nmean_plot + sd_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nAs we can see, the population tends to converge on a single value pretty quickly, but far short of the ideal value! Can increasing the strength of the ideal help? Let’s see. In the simulation below, we set ideal_strength to 10, which is considerably higher than its default value of 1.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 50, ideal_trait_value = 2, ideal_strength = 10)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait), sd_trait = sd(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nsd_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = sd_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    expand_limits(y = 0) +\n    labs(x = \"Time\", y = \"Trait value standard deviation\", color = \"Sim.\")\n\nmean_plot + sd_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nEven when there is a very strong pull to select demonstrators close to the ideal, the population still falls short. The problem is that agents can never adopt a trait that is outside the range of values currently in the population. As the population grows more homogeneous, even though they get closer to the ideal, the lack of diversity in trait values means that the population will never achieve the ideal.\n\n\n13.5.3 Innovation\nOne solution to the problem above is to allow for some additional random variability in trait values from one time-step to the next. In evolutionary biology, we might call this random variability “mutation” but since we are talking about ideas not genes, we will call it innovation. As shown in the code below, we will simply add a value sampled from a normal distribution with mean 0 and a standard deviation set by the innovation parameter to each agent’s trait value on each time-step. Everything else about the function is unchanged from before.\n\n\nCode\ncontinuous_trait_evolution &lt;- function(n_pop = 1000, max_t = 100, n_sims = 10, init_trait_sd = 1, self_weight = 1, n_demonstrators = 1, conformity = 0, mean_init_status = 1, ideal_trait_value = NA, ideal_strength = 1, innovation = 0) {\n    population &lt;- array(NA,\n                        dim = c(n_sims, max_t, n_pop),\n                        dimnames = list(\"sim_index\" = 1:n_sims,\n                                        \"t\" = 1:max_t,\n                                        \"member\" = 1:n_pop)\n                        )\n\n    status &lt;- array(NA,\n                    dim = c(n_sims, max_t, n_pop),\n                    dimnames = list(\"sim_index\" = 1:n_sims,\n                                    \"t\" = 1:max_t,\n                                    \"member\" = 1:n_pop)\n                    )\n\n    for (sim_index in 1:n_sims) {\n        population[sim_index, 1, ] &lt;- rnorm(n = n_pop, mean = 0, sd = init_trait_sd)\n        \n        if (is.na(ideal_trait_value)) {\n            status[sim_index, 1, ] &lt;- rbeta(n = n_pop, shape1 = mean_init_status, shape2 = 1 - mean_init_status)\n    \n            status[sim_index, 1, ] &lt;- status[sim_index, 1, ] / max(status[sim_index, 1, ])\n        } else {\n            status[sim_index, 1, ] &lt;- exp(-ideal_strength * (population[sim_index, 1, ] - ideal_trait_value)^2 / 2)\n        }\n\n        for (t in 2:max_t) {\n            demonstrators &lt;- sample(n_pop, size = n_pop * n_demonstrators, replace = TRUE, prob = status[sim_index, t - 1, ])\n\n            demonstrator_trait &lt;- matrix(\n                population[sim_index, t - 1, demonstrators],\n                nrow = n_pop, ncol = n_demonstrators\n            )\n            \n            demo_weight &lt;- exp(-conformity * (demonstrator_trait - matrix(rowMeans(demonstrator_trait), nrow = n_pop, ncol = n_demonstrators, byrow = FALSE))^2 / 2)\n            \n            population[sim_index, t, ] &lt;- (self_weight * population[sim_index, t - 1, ] + rowSums(demo_weight * demonstrator_trait)) / (self_weight + rowSums(demo_weight)) + rnorm(n = n_pop, mean = 0, sd = innovation)\n            \n            if (is.na(ideal_trait_value)) {\n                status[sim_index, t, ] &lt;- status[sim_index, t - 1, ]\n            } else {\n                status[sim_index, t, ] &lt;- exp(-ideal_strength * (population[sim_index, t, ] - ideal_trait_value)^2 / 2)\n            }\n        }\n    }\n\n    return(\n        full_join(\n            as_tibble(array2DF(population, responseName = \"trait\")),\n            as_tibble(array2DF(status, responseName = \"status\")),\n            by = join_by(sim_index, t, member)\n        ) %&gt;%\n            mutate(sim_index = as.numeric(sim_index), t = as.numeric(t), member = as.numeric(member))\n    )\n}\n\n\nAs shown in the example below, when we allow for a decent amount of random variation by setting innovation to 0.5, the population average is able to reach the ideal trait value of 2, even without increasing the strength of the ideal value.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 50, ideal_trait_value = 2, innovation = 0.5)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait), sd_trait = sd(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nsd_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = sd_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    expand_limits(y = 0) +\n    labs(x = \"Time\", y = \"Trait value standard deviation\", color = \"Sim.\")\n\nmean_plot + sd_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nThe right graph illustrates that even though the mean trait value in the population is the ideal, there is still considerable variability among the agents in the population. Of course, this is to be expected, given that the model now ensures random variation in trait values will always be present. But it’s worth asking whether the population can still achieve the ideal with a smaller amount of random variability. The simulation below sets innovation to 0.1 to see.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 500, ideal_trait_value = 2, innovation = 0.1)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait), sd_trait = sd(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nsd_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = sd_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    expand_limits(y = 0) +\n    labs(x = \"Time\", y = \"Trait value standard deviation\", color = \"Sim.\")\n\nmean_plot + sd_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nAs we can see, it takes longer for the population average to converge on the ideal (note the scale of the \\(x\\) axes in the above graphs), but they all make it eventually. Even a small amount of diversity can help a population get out of a rut.",
    "crumbs": [
      "Cognitive models at larger and smaller scales",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Agent-Based Models</span>"
    ]
  },
  {
    "objectID": "agent_based_models.html#emergence-of-belief-polarization",
    "href": "agent_based_models.html#emergence-of-belief-polarization",
    "title": "13  Agent-Based Models",
    "section": "13.6 Emergence of belief polarization",
    "text": "13.6 Emergence of belief polarization\nWe have now seen a number of examples of how agent-based models can help us understand the emergence of a number of interesting social phenomena, like the tendency to evolve toward a single dominant trait, how this is influenced by a preference for conformity, and how innovation can help a population to reach an “ideal” trait value. In this section, we extend our model one last time to address the phenomenon of polarization. Using our continuous-trait model, polarization occurs whenever the population evolves to a point where there are distinct groups of individuals who cluster around different trait values.\nPolarization notably occurs in politics, but it also occurs in other domains like the arts (e.g., representational vs. abstract visual art or “uptown” vs. “downtown” styles of musical innovation, etc.) or the sciences (e.g., single- vs. dual-process theories, nature vs. nurture, etc.). Of particular interest is when polarization occurs despite some objective evaluation of the quality of a particular belief. This kind of polarization occurs in fields like climate science, where theories, models, and observations that support the presence of mechanisms that produce climate change are nonetheless disregarded. It also occurs in fields like medicine, where despite extensive disconfirmatory research, people who claim to be scientists (or to lead scientific agencies) nonetheless claim that vaccines cause autism.\nThese kinds of polarization were studied using agent-based models by O’Connor & Weatherall (2018). Although the model we develop here is different in its technical assumptions, we will implement the same basic interaction and updating processes that they did. In particular, we will see how giving increased weight to demonstrators with similar traits can result in polarization that is shockingly difficult to avoid. However, to presage the moral of the story, we will see that so long as agents give at least some credence to demonstrators who are dissimilar from them, a society can avoid destructive polarization.\n\n13.6.1 Attraction and repulsion\nThe key idea explored in the models by O’Connor & Weatherall (2018) is this: When an agent picks one or more demonstrators, they give more weight to demonstrators who have similar trait values to them than to demonstrators with dissimilar trait values. Conceptually, it is easy to imagine situations where this occurs: If someone shares your beliefs, you might be more likely to trust that their opinions are sound and to therefore adjust your beliefs to better match those of your interlocutor. On the other hand, if someone espouses a belief that is very different from yours, you might be inclined to disregard them. You may even be repelled by their beliefs and adopt a trait that moves away from theirs! The point is that people may be more inclined to trust people who already hold similar beliefs.\nWe can build this into our model by changing how agents assign weight to demonstrators. We will measure the similarity between an agent \\(i\\) and its demonstrators \\(j\\) using the same exponentially-transformed squared distance measure we’ve been using so far. The rate at which similarity falls off with the squared difference in trait values is governed by a parameter \\(\\rho\\), called similarity_range in our function below. There’s also a parameter for the weight given to a demonstrator with zero similarity, \\(\\zeta\\), called dissimilar_influence in the function below. The weight given to demonstrator \\(j\\) as a function of its similarity to agent \\(i\\) is \\[\n\\psi_j = \\zeta + \\left(1 - \\zeta \\right) \\exp \\left[ -\\frac{\\left( x_i(t) - x_j(t) \\right)^2}{2 \\rho^2} \\right]\n\\]\nThis weighting factor is implemented in the commented lines in the revised model code below.\n\n\nCode\ncontinuous_trait_evolution &lt;- function(n_pop = 1000, max_t = 100, n_sims = 10, init_trait_sd = 1, self_weight = 1, n_demonstrators = 1, conformity = 0, mean_init_status = 1, ideal_trait_value = NA, ideal_strength = 1, innovation = 0, dissimilar_influence = 1, similarity_range = 1) {\n    population &lt;- array(NA,\n                        dim = c(n_sims, max_t, n_pop),\n                        dimnames = list(\"sim_index\" = 1:n_sims,\n                                        \"t\" = 1:max_t,\n                                        \"member\" = 1:n_pop)\n                        )\n\n    status &lt;- array(NA,\n                    dim = c(n_sims, max_t, n_pop),\n                    dimnames = list(\"sim_index\" = 1:n_sims,\n                                    \"t\" = 1:max_t,\n                                    \"member\" = 1:n_pop)\n                    )\n\n    for (sim_index in 1:n_sims) {\n        population[sim_index, 1, ] &lt;- rnorm(n = n_pop, mean = 0, sd = init_trait_sd)\n        \n        if (is.na(ideal_trait_value)) {\n            status[sim_index, 1, ] &lt;- rbeta(n = n_pop, shape1 = mean_init_status, shape2 = 1 - mean_init_status)\n    \n            status[sim_index, 1, ] &lt;- status[sim_index, 1, ] / max(status[sim_index, 1, ])\n        } else {\n            status[sim_index, 1, ] &lt;- exp(-ideal_strength * (population[sim_index, 1, ] - ideal_trait_value)^2 / 2)\n        }\n\n        for (t in 2:max_t) {\n            demonstrators &lt;- sample(n_pop, size = n_pop * n_demonstrators, replace = TRUE, prob = status[sim_index, t - 1, ])\n\n            demonstrator_trait &lt;- matrix(\n                population[sim_index, t - 1, demonstrators],\n                nrow = n_pop, ncol = n_demonstrators\n            )\n            \n            agent_demo_sq_diff &lt;- (matrix(population[sim_index, t - 1, ], nrow = n_pop, ncol = n_demonstrators, byrow = FALSE) - demonstrator_trait)^2\n            \n            # Weight is the product of the similarity-based weight and the conformity-based weight\n            demo_weight &lt;- (dissimilar_influence + (1 - dissimilar_influence) * exp(-agent_demo_sq_diff / (2 * similarity_range^2))) * exp(-conformity * (demonstrator_trait - matrix(rowMeans(demonstrator_trait), nrow = n_pop, ncol = n_demonstrators, byrow = FALSE))^2 / 2)\n            \n            population[sim_index, t, ] &lt;- (self_weight * population[sim_index, t - 1, ] + rowSums(demo_weight * demonstrator_trait)) / (self_weight + rowSums(abs(demo_weight))) + rnorm(n = n_pop, mean = 0, sd = innovation)\n            \n            if (is.na(ideal_trait_value)) {\n                status[sim_index, t, ] &lt;- status[sim_index, t - 1, ]\n            } else {\n                status[sim_index, t, ] &lt;- exp(-ideal_strength * (population[sim_index, t, ] - ideal_trait_value)^2 / 2)\n            }\n        }\n    }\n\n    return(\n        full_join(\n            as_tibble(array2DF(population, responseName = \"trait\")),\n            as_tibble(array2DF(status, responseName = \"status\")),\n            by = join_by(sim_index, t, member)\n        ) %&gt;%\n            mutate(sim_index = as.numeric(sim_index), t = as.numeric(t), member = as.numeric(member))\n    )\n}\n\n\nBefore we get to running some simulations, it may be useful to visualize how the similarity weight varies as a function of distance and the two parameters just described; see below. When \\(\\zeta = 1\\), similarity doesn’t contribute at all to the weight of a demonstrator. When \\(\\zeta = 0\\), dissimilar demonstrators are given no weight at all. Finally, we \\(\\zeta &lt; 0\\), dissimilar demonstrators are given negative weight, such that the agent updates their trait value away from the demonstrator. This situation will be used below to model distrust of a demonstrator.\n\n\nCode\nexpand_grid(d = seq(0, 5, length.out = 101), similarity_range = c(0.5, 1, 1.5, 2, 2.5), dissimilar_influence = seq(-1, 1, length.out = 3)) %&gt;%\n    mutate(weight = dissimilar_influence + (1 - dissimilar_influence) * exp(-d^2 / (2 * similarity_range^2))) %&gt;%\n    ggplot(aes(x = d, y = weight, color = similarity_range, linetype = factor(dissimilar_influence), group = interaction(similarity_range, dissimilar_influence))) +\n    geom_line() +\n    scale_color_viridis_c() +\n    labs(x = expression(x[i](t) - x[j](t)), y = expression(psi[j]), linetype = expression(zeta), color = expression(rho))\n\n\n\n\n\n\n\n\n\n\n\n13.6.2 Clusters from preference for similarity\nThe simulation below shows that, when people are only influenced by those who are similar to them, clusters of traits emerge. One cluster is centered around the mean of the initial population (0), but there are also clusters that form on the extremes.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 200, dissimilar_influence = 0, similarity_range = 0.5)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nAs shown below, the narrower the similarity_range is, the more clusters tend to form.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 200, dissimilar_influence = 0, similarity_range = 0.2)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nOn the other hand, when similarity_range is large enough, the entire population comes to adopt the same consensus trait value.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 100, dissimilar_influence = 0, similarity_range = 1)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n13.6.3 Distrust leads to extreme polarization\nNow let’s see what happens when agents do not merely ignore dissimilar demonstrators (dissimilar_influence = 0), but instead distrust dissimilar demonstrators. As noted above, distrust in our model amounts to giving a demonstrator negative weight as similarity diminishes.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 100, dissimilar_influence = -1, similarity_range = 0.5)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nAs shown above, with distrust, there are no longer any clusters in the middle—the population evolves into two sets of diametrically opposed traits. This results from the combined influence of agents being pulled toward agents with similar traits and away from agents with dissimilar traits.\nThat said, if the similarity range is sufficiently large, as in the simulations below, the entire population can again converge on a consensus value.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 100, dissimilar_influence = -1, similarity_range = 1)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n13.6.4 When can a distrustful society arrive at a mutually agreed solution?\nThe preceding simulations assumed that there was no inherent fitness or value associated with a trait. What happens if there is an ideal trait value? Will this counteract the tendency to polarize? First, let’s simulate a situation where the ideal trait value is 0, which is the mean of the initial trait values for each population.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 100, dissimilar_influence = -1, similarity_range = 0.5, ideal_trait_value = 0, ideal_strength = 1)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nHuh! Even in this simple scenario, polarization arises. Perhaps if we increase the pull of the ideal value?\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 100, dissimilar_influence = -1, similarity_range = 0.5, ideal_trait_value = 0, ideal_strength = 5)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nAs we can see above, a stronger pull toward the ideal gives the population a chance to avoid falling into the polarization trap. Still, that was only for a population where the ideal value was already close to the average. What happens if the ideal value is not close to the initial mean value of the population? As shown below, even with a strong pull toward an ideal value of 2, polarization occurs. In essence, there is one group who gets close to optimal and another that is diametrically opposed.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 100, dissimilar_influence = -1, similarity_range = 0.5, ideal_trait_value = 2, ideal_strength = 5)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nIn fact, looking at the mean trait values, it’s clear that most population members fall on the opposite side of the “ideal” value! Why is this? Recall that being close to the ideal increases your status, thus making it more likely that you’ll be chosen as a demonstrator. An agent who has a trait value very different from the ideal will therefore be repelled by most of the demonstrators they select!\nEven without distrust—when you just ignore people who are dissimilar—it is possible for some segment of the population to achieve the optimal, but there are often large segments of the population who fail to do so, at least at first.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 100, dissimilar_influence = 0, similarity_range = 0.5, ideal_trait_value = 2, ideal_strength = 5)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nOf course, it is worth remembering what we saw above—that even if you are equally influenced by everyone, the population can often converge on a sub-optimal trait value.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 100, dissimilar_influence = 1, similarity_range = 0.5, ideal_trait_value = 2, ideal_strength = 5)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nInnovation is not necessarily sufficient for an ignorant population to converge on an optimal solution. This is unlike the previous simulations where innovation was sufficient to get the population “out of its rut”.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 100, dissimilar_influence = 0, similarity_range = 0.5, ideal_trait_value = 2, ideal_strength = 5, innovation = 0.1)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nEven a little bit of knowledge goes a long way. If you prioritize, but do not outright dismiss, dissimilar agents, an innovative population can converge on the optimum.\n\n\nCode\nsim &lt;- continuous_trait_evolution(n_pop = 1000, max_t = 100, dissimilar_influence = 0.1, similarity_range = 0.5, ideal_trait_value = 2, ideal_strength = 5, innovation = 0.1)\n\ntrait_summary &lt;- sim %&gt;%\n    group_by(sim_index, t) %&gt;%\n    summarize(mean_trait = mean(trait))\n\n\n`summarise()` has grouped output by 'sim_index'. You can override using the\n`.groups` argument.\n\n\nCode\nmean_plot &lt;- trait_summary %&gt;%\n    ggplot(aes(x = t, y = mean_trait, color = factor(sim_index))) +\n    geom_line(linewidth = 0.5) +\n    stat_summary(fun = mean, geom = \"line\", linewidth = 1, color = \"black\") +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow() +\n    labs(x = \"Time\", y = \"Mean trait value\", color = \"Sim.\")\n\nfinal_plot &lt;- sim %&gt;%\n    filter(t == max(t)) %&gt;%\n    ggplot(aes(x = factor(sim_index), y = trait, color = factor(sim_index))) +\n    geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.5) +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = \"black\") +\n    scale_color_discreterainbow(guide = \"none\") +\n    labs(x = \"Simulation index\", y = \"Final trait value\", color = \"Sim.\")\n\nmean_plot + final_plot + plot_layout(nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nO’Connor, C., & Weatherall, J. O. (2018). Scientific polarization. European Journal for Philosophy of Science, 8(3), 855–875. https://doi.org/10.1007/s13194-018-0213-9\n\n\nSmith, E. R., & Conrey, F. R. (2007). Agent-based modeling: A new approach for theory building in social psychology. Personality and Social Psychology Review, 11(1), 87–104. https://doi.org/10.1177/1088868306294789",
    "crumbs": [
      "Cognitive models at larger and smaller scales",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Agent-Based Models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Akaike, H. (1974). A new look at the\nstatistical model identification. IEEE Transactions on Automatic\nControl, 19(6), 716–723.\n\n\nArlot, S., & Celisse, A. (2010). A survey of cross-validation\nprocedures for model selection. Statistics Surveys, 4,\n40–79. https://doi.org/10.1214/09-SS054\n\n\nBlurton, S. P., Kesselmeier, M., & Gondan, M. (2012). Fast and\naccurate calculations for cumulative first-passage time distributions in\nwiener diffusion models. Journal of Mathematical Psychology,\n56(6), 470–475. https://doi.org/https://doi.org/10.1016/j.jmp.2012.09.002\n\n\nBroadbent, D. E. (1957). A mechanical model for human attention and\nimmediate memory. Psychological Review, 64(3),\n205–215.\n\n\nBrown, S., & Heathcote, A. (2008). The simplest complete model of\nchoice response time: Linear ballistic accumulation. Cognitive\nPsychology, 57, 153–178.\n\n\nBrowne, M. W. (2000). Cross-validation methods. Journal of\nMathematical Psychology, 44(1), 108–132. https://doi.org/10.1006/jmps.1999.1279\n\n\nBusemeyer, J. R., & Townsend, J. T. (1993). Decision field theory: A\ndynamic–cognitive approach to decision making in an uncertain\nenvironment. Psychological Review, 100(3), 432–459.\n\n\nBusemeyer, J. R., & Wang, Y.-M. (2000). Model comparisons and model\nselections based on generalization criterion methodology. Journal of\nMathematical Psychology, 44(1), 171–189. https://doi.org/10.1006/jmps.1999.1282\n\n\nCox, G. E., & Shiffrin, R. M. (2024). Computational models of event\nmemory. In M. J. Kahana & A. Wagner (Eds.), Oxford handbook of\nhuman memory. Oxford University Press.\n\n\nDennett, D. (1980). The milk of human intentionality. Behavioral and\nBrain Sciences, 3(3), 428–430. https://doi.org/10.1017/s0140525x0000580x\n\n\nDiederich, A. (1997). Dynamic stochastic models for decision making\nunder time constraints. Journal of Mathematical Psychology,\n41(3), 260–274. https://doi.org/10.1006/jmps.1997.1167\n\n\nDonkin, C., Brown, S., Heathcote, A., & Wagenmakers, E.-J. (2011).\nDiffusion versus linear ballistic accumulation: Different models but the\nsame conclusions about psychological processes? Psychonomic Bulletin\n& Review, 55, 140–151.\n\n\nElman, J. L. (1990). Finding structure in time. Cognitive\nScience, 14(2), 179–211. https://doi.org/10.1207/s15516709cog1402_1\n\n\nGarner, W. R., & Felfoldy, G. L. (1970). Integrality of stimulus\ndimensions in various types of information processing. Cognitive\nPsychology, 1, 225–241.\n\n\nGelman, A., Hwang, J., & Vehtari, A. (2014). Understanding\npredictive information criteria for bayesian models. Statistics and\nComputing, 24(6), 997–1016. https://doi.org/10.1007/s11222-013-9416-2\n\n\nGillespie, N. F., & Cox, G. E. (2024). Perception and memory for\nnovel auditory stimuli: Similarity, serial position, and list\nhomogeneity. PsyArXiv. https://doi.org/10.31234/osf.io/n294a\n\n\nGondan, M., Blurton, S. P., & Kesselmeier, M. (2014). Even faster\nand even more accurate first-passage time densities and distributions\nfor the wiener diffusion model. Journal of Mathematical\nPsychology, 60, 20–22. https://doi.org/https://doi.org/10.1016/j.jmp.2014.05.002\n\n\nHarding, B., Goulet, M.-A., Jolin, S., Tremblay, C., Villeneuve, S.-P.,\n& Durand, G. (2016). Systems factorial technology explained to\nhumans. The Quantitative Methods for Psychology,\n12(1), 39–56.\n\n\nHartmann, R., & Klauer, K. C. (2021). Partial derivatives for the\nfirst-passage time distribution in wiener diffusion models. Journal\nof Mathematical Psychology, 103, 102550.\nhttps://doi.org/https://doi.org/10.1016/j.jmp.2021.102550\n\n\nHartmann, R., & Klauer, K. C. (2023). WienR: Derivatives of the\nfirst-passage time density and cumulative distribution function, and\nrandom sampling from the (truncated) first-passage time\ndistribution. https://CRAN.R-project.org/package=WienR\n\n\nHebb, D. O. (1949). The organization of behavior: A\nneuropsychological theory. Wiley.\n\n\nHoupt, J. W., Blaha, L. M., McIntire, J. P., Havig, P. R., &\nTownsend, J. T. (2014). Systems factorial technology with\nR. Behavior Research Methods, 46,\n307–330.\n\n\nKruschke, J. K. (2009). Highlighting: A canonical experiment. In\nPsychology of learning and motivation (Vol. 51, pp. 153–185).\nElsevier.\n\n\nKruschke, J. K. (2011). Models of attentional learning. In E. M. Pothos\n& A. J. Wills (Eds.), Formal approaches in categorization\n(pp. 120–152). Cambridge University Press.\n\n\nLandauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s problem: The latent semantic analysis\ntheory of acquisition, induction, and representation of knowledge.\nPsychological Review, 104(2), 211–240.\n\n\nMackintosh, N. J. (1975). A theory of attention: Variations in the\nassociability of stimuli with reinforcement. Psychological\nReview, 82(4), 276–298.\n\n\nMarr, D. (1982). Vision: A computational investigation into the\nhuman representation and processing of visual information. W.H.\nFreeman.\n\n\nMcClelland, J. L., McNaughton, B. L., & O’Reilly, R. C. (1995). Why\nthere are complementary learning systems in the hippocampus and\nneocortex: Insights from the successes and failures of connectionist\nmodels of learning and memory. Psychological Review,\n102(3), 419–457.\n\n\nMedin, D. L., Altom, M. W., Edelson, S. M., & Freko, D. (1982).\nCorrelated symptoms and simulated medical classification. Journal of\nExperimental Psychology: Learning, Memory, and Cognition,\n8(1), 37–50. https://www.proquest.com/scholarly-journals/correlated-symptoms-simulated-medical/docview/614362118/se-2\n\n\nMurdock, B. B. (1982). A theory for the storage and retrieval of item\nand associative information. Psychological Review,\n89(3), 609–626.\n\n\nNavarro, D. J. (2018). Between the devil and the deep blue sea: Tensions\nbetween scientific judgement and statistical model selection.\nComputational Brain & Behavior. https://doi.org/10.1007/s42113-018-0019-z\n\n\nNavarro, D. J., & Fuss, I. G. (2009). Fast and accurate calculations\nfor first-passage times in wiener diffusion models. Journal of\nMathematical Psychology, 53(4), 222–230.\nhttps://doi.org/https://doi.org/10.1016/j.jmp.2009.02.003\n\n\nNeisser, U. (1967). Cognitive psychology.\nAppleton-Century-Crofts.\n\n\nNosofsky, R. M. (1986). Attention, similarity, and the\nidentification-categorization relationship. Journal of Experimental\nPsychology: General, 115(1), 39–57.\n\n\nNosofsky, R. M. (1992). Similarity scaling and cognitive process models.\nAnnual Review of Psychology, 43, 25–53.\n\n\nNosofsky, R. M., Cox, G. E., Cao, R., & Shiffrin, R. M. (2014). An\nexemplar-familiarity model predicts short-term and long-term probe\nrecognition across diverse forms of memory search. Journal of\nExperimental Psychology: Learning, Memory, and Cognition,\n40(6), 1524–1539.\n\n\nNosofsky, R. M., Little, D. R., Donkin, C., & Fific, M. (2011).\nShort-term memory scanning viewed as exemplar-based categorization.\nPsychological Review, 118(2), 280–315.\n\n\nNosofsky, R. M., & Palmeri, T. J. (1997). An exemplar-based random\nwalk model of speeded classification. Psychological Review,\n104(2), 266–300.\n\n\nO’Connor, C., & Weatherall, J. O. (2018). Scientific polarization.\nEuropean Journal for Philosophy of Science, 8(3),\n855–875. https://doi.org/10.1007/s13194-018-0213-9\n\n\nPhiliastides, M. G., Ratcliff, R., & Sajda, P. (2006). Neural\nrepresentation of task difficulty and decision making during perceptual\ncategorization: A timing diagram. Journal of Neuroscience,\n26(35), 8965–8975. https://doi.org/10.1523/JNEUROSCI.1655-06.2006\n\n\nPiironen, J., & Vehtari, A. (2017). Comparison of bayesian\npredictive methods for model selection. Statistics and\nComputing, 27(3), 711–735. https://doi.org/10.1007/s11222-016-9649-y\n\n\nPosner, M. I., & Keele, S. W. (1968). On the genesis of abstract\nideas. Journal of Experimental Psychology, 77,\n353–363. https://doi.org/10.1037/h0025953\n\n\nPurcell, B. A., Heitz, R. P., Cohen, J. Y., Schall, J. D., Logan, G. D.,\n& Palmeri, T. J. (2010). Neurally constrained modeling of perceptual\ndecision making. Psychological Review, 117(4),\n1113–1143. https://doi.org/10.1037/a0020311\n\n\nPurcell, B. A., Schall, J. D., Logan, G. D., & Palmeri, T. J.\n(2012). From salience to saccades: Multiple-alternative gated stochastic\naccumulator model of visual search. Journal of Neuroscience,\n32(10), 3433–3446. https://doi.org/10.1523/JNEUROSCI.4622-11.2012\n\n\nRaab, D. H. (1962). Statistical facilitation of simple reaction times.\nTransactions of the New York Academy of Sciences, 24(5\nSeries II), 574–590.\n\n\nRae, B., Heathcote, A., Donkin, C., Averell, L., & Brown, S. (2014).\nThe hare and the tortoise: Emphasizing speed can change the evidence\nused to make decisions. Journal of Experimental Psychology:\nLearning, Memory, and Cognition, 40(5), 1226–1243.\n\n\nRaftery, A. E. (1995). Bayesian model selection in social research.\nSociological Methodology, 25, 111–163. https://doi.org/10.2307/271063\n\n\nRatcliff, R. (1978). A theory of memory retrieval. Psychological\nReview, 85(2), 59–108.\n\n\nRatcliff, R., & Rouder, J. N. (1998). Modeling response times for\ntwo-choice decisions. Psychological Science, 9(5),\n347–356.\n\n\nRescorla, R. A., & Wagner, A. R. (1972). A theory of\nPavlovian conditioning: Variations in the effectiveness of\nreinforcement and nonreinforcement. In A. H. Black & W. F. Prokasy\n(Eds.), Classical conditioning II: Current research and\ntheory (pp. 64–99). Appleton–Century–Crofts.\n\n\nRogers, T. T., & McClelland, J. L. (2004). Semantic cognition: A\nparallel distributed processing approach. MIT Press.\n\n\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning\ninternal representations by error propagation. In D. E. Rumelhart &\nJ. L. McClelland (Eds.), Parallel distributed processing: Vol.\nI. The MIT Press.\n\n\nSchwarz, G. (1978). Estimating the dimension of a model. The Annals\nof Statistics, 6(2), 461–464.\n\n\nShadlen, M. N., & Newsome, W. T. (2001). Neural basis of a\nperceptual decision in the parietal cortex (area LIP) of the rhesus\nmonkey. Journal of Neurophysiology, 86(4), 1916–1936.\nhttps://doi.org/10.1152/jn.2001.86.4.1916\n\n\nShepard, R. N. (1962a). The analysis of proximities:\nMultidimensional scaling with an unknown distance function.\nI. Psychometrika, 27(2), 125–140.\nhttps://doi.org/https://doi.org/10.1007/BF02289630\n\n\nShepard, R. N. (1962b). The analysis of proximities:\nMultidimensional scaling with an unknown distance function.\nII. Psychometrika, 27(3), 219–246.\nhttps://doi.org/https://doi.org/10.1007/BF02289621\n\n\nSingmann, H., Kellen, D., Cox, G. E., Chandramouli, S. H., Davis-Stober,\nC. P., Dunn, J. C., Gronau, Q. F., Kalish, M. L., McMullin, S. D.,\nNavarro, D. J., & Shiffrin, R. M. (2022). Statistics in the service\nof science: Don’t let the tail wag the dog. Computational Brain\n& Behavior.\n\n\nSmith, E. R., & Conrey, F. R. (2007). Agent-based modeling: A new\napproach for theory building in social psychology. Personality and\nSocial Psychology Review, 11(1), 87–104. https://doi.org/10.1177/1088868306294789\n\n\nStone, M. (1977). An asymptotic equivalence of choice of model by\ncross-validation and Akaike’s criterion.\nJournal of the Royal Statistical Society. Series B\n(Methodological), 39(1), 44–47.\n\n\nTeodorescu, A. R., & Usher, M. (2013). Disentangling decision\nmodels: From independence to competition. Psychological Review,\n120(1), 1–38.\n\n\nTillman, G., Van Zandt, T., & Logan, G. D. (2020). Sequential\nsampling models without random between-trial variability: The racing\ndiffusion model of speeded decision making. Psychonomic Bulletin\n& Review, 27(5), 911–936. https://doi.org/10.3758/s13423-020-01719-6\n\n\nTownsend, J. T., & Nozawa, G. (1995). Spatio-temporal properties of\nelementary perception: An investigation of parallel, serial, and\ncoactive theories. Journal of Mathematical Psychology,\n39, 321–359.\n\n\nTrueblood, J. S., Holmes, W. R., Seegmiller, A. C., Douds, J., Compton,\nM., Szentirmai, E., Woodruff, M., Huang, W., Stratton, C., &\nEichbaum, Q. (2018). The impact of speed and bias on the cognitive\nprocesses of experts and novices in medical image decision-making.\nCognitive Research: Principles and Implications, 3,\n1–14.\n\n\nTuerlincx, F. (2004). The efficient computation of the cumulative\ndistribution and probability density functions in the diffusion model.\nBehavior Research Methods, Instruments, & Computers,\n36(4), 702–716.\n\n\nTurner, B. M., Forstmann, B. U., Wagenmakers, E.-J., Brown, S. D.,\nSederberg, P. B., & Steyvers, M. (2013). A bayesian framework for\nsimultaneously modeling neural and behavioral data. NeuroImage,\n72, 193–206. https://doi.org/10.1016/j.neuroimage.2013.01.048\n\n\nUsher, M., & McClelland, J. L. (2001). The time course of perceptual\nchoice: The leaky, competing accumulator model. Psychological\nReview, 108(3), 550–592.\n\n\nVehtari, A., & Lampinen, J. (2002). Bayesian model assessment and\ncomparison using cross-validation predictive densities. Neural\nComputation, 14(10), 2439–2468. https://doi.org/10.1162/08997660260293292\n\n\nWidrow, B., & Hoff, M. E. (1960). Adaptive switching circuits.\nIRE WESCON Convention Record, 96–104.\n\n\nZucchini, W. (2000). An introduction to model selection. Journal of\nMathematical Psychology, 44(1), 41–61. https://doi.org/10.1006/jmps.1999.1276",
    "crumbs": [
      "References"
    ]
  }
]