[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Cognitive Models",
    "section": "",
    "text": "Preface\nThis is a collection of notes and materials for an introductory course in computational cognitive modeling. It will be updated and, arguably, improved over the course of the semester.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Some functions of models\nAs Cox & Shiffrin (2024) describe, a computational cognitive model falls on the “causal” end of the spectrum in the graph shown at the top. They enumerate a couple of goals that such a model might serve:",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#some-functions-of-models",
    "href": "intro.html#some-functions-of-models",
    "title": "1  Introduction",
    "section": "",
    "text": "All models are wrong, but causal models are useful when they capture and approximate the causal processes at work and when they generalize to other settings. When modeling complex human behavior, all the models are at best crude approximations to reality, but nonetheless they can prove useful by describing the processes that are most important for explaining behavior.\nModeling allows precise communication among scientists. When theories and hypotheses are proposed verbally and heuristically, their precise meaning is not generally known even to the proposers, and they are often misinterpreted and misunderstood by others. When theoretical constructs are clearly linked to elements of a model and the internal workings of the model are described with mathematical or computational precision (e.g., by including the code running the model simulation), other scientists can use, generalize, and test the model because its application in at least its original domain is well specified.\nModels make clear the links between assumptions and predictions. Different investigators cannot dispute the predictions of the model that are produced by a given set of parameters (the parameters generally index different quantitative variants of a given class of models).\nThe formal specification of the model clarifies the nature of proposed internal cognitive processes. A poor modeler may fail to demonstrate that linkage. A good modeler will explore the parameter space and show how the parameter values change the predictions and how narrow or broad a range of outcomes can be predicted by changes in the parameter values. A good modeler will also explore alterations in the model, perhaps by deleting some processes or by adding others, or by fixing some parameter values at certain values, thus providing diagnostic information about the qualitative features of the outcome space that are primarily due to a process controlled by particular parameters. A bad modeler might claim a fit to data provides support for an underlying causal theory when in fact the fit is primarily due to some parameter or theory not conceptually related to the claim.\nConstructing a model can act as an “intuition pump” (cf. Dennett, 1980). Many modelers try to infer underlying cognitive processes from complex data sets that involve multiple measures (e.g., accuracy and response time) and many conditions which may be difficult or impossible to summarize adequately. Modelers form hypotheses about the processes involved based on the data and their prior knowledge. It is often the case that intuiting predictions for different combinations of processes is not possible due to the limitations of human cognition. Thus, different combinations of processes are instantiated as formal models, enabling a modeler to observe and test the predictions of their hypotheses. In an iterative model building process, the failures in each iteration indicate the way forward to an appropriate account.\nModeling allows different hypothesis classes to be compared, both because the predictions of each are well specified and because model comparison techniques take into account model complexity. The issue of complexity is itself quite complex.\n\nOne issue is due to statistical noise produced by limited data. A flexible model with many parameters can produce what seems to be a good fit with parameter values that explain the noise in the data rather than the underlying processes. The best model comparison techniques penalize models appropriately for extra complexity. However, models are in most cases developed after observing the data, and they are used to explain the patterns observed. To do so, they often incorporate assumptions that are critical but not explicitly parameterized. It thus becomes a difficult and subtle matter to assess complexity adequately. A secondary problem with using fit to compare models is the fact that the most principled ways to control for complexity, such as Bayesian model selection and minimum description length, are difficult to implement computationally and are often replaced by approximations such as the Akaike or Bayes/Schwartz information criteria that often fail to account for key aspects of model complexity.\nSimpler models are also favored for other reasons. A chief one is limited human reasoning: As a model becomes more complex, it becomes more difficult for a human to understand how it works. Simpler models are also favored when analytic predictions can be derived (thereby greatly reducing computational costs) and for qualitative reasons such as “elegance”.\nSimpler models are particularly favored when their core processes generalize well across many tasks.\nThere exists a danger that the modeler will mistake small quantitative differences in “fit” for the important differences among models—differences that generally lie in qualitative patterns of predictions. Knowing one model provides a better fit to a limited set of observations from a narrow experimental setting is not often useful. For example, consider a model that correctly predicts the relative levels of recall accuracy observed across conditions in a given experiment but quantitatively overpredicts the accuracy in a single condition. Meanwhile, another model perfectly predicts the accuracy in that condition but fails to predict the right qualitative pattern of accuracy across conditions. We argue that the qualitatively correct prediction is a reason to favor the first model, even though it might yield a worse quantitative fit. Correct qualitative predictions across conditions are often a sign that a model captures an important and generalizable causal process.",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-importance-of-simulation",
    "href": "intro.html#the-importance-of-simulation",
    "title": "1  Introduction",
    "section": "1.2 The importance of simulation",
    "text": "1.2 The importance of simulation\nThe models that we will be covering are models that simulate behavioral (and maybe even neural) outcomes. Because we are building models of a complex system—namely, the mind—our models can also become complex. Therefore, understanding what kind of behavior a model produces may require us to simulate behavior from that model. This will also help us to understand the relationship between the model’s parameters and its behavior. By simulating how behavior changes as one or more parameters change, we can understand the role played by the theoretical construct represented by that parameter.\nFor example, we might have a parameter that represents the quality or precision with which an event is stored in memory. In a model where this memory representation is used to yield behavior, we can systematically adjust the value of the quality/precision parameter and observe the effect this has on the model’s simulated behavior. Again, because we are dealing with models that can grow quite complex, we may even be surprised by the behavior the model produces!\nTwo analogies may help give some intuition about the value of simulation: If we are cooking, often the only way to know how a particular combination of spices will taste is to actually combine them and taste. Model parameters are like the different amounts of each spice, with the final taste being analogous to the model’s simulated behavior. Alternatively, you can think of model parameters as being like characters in an improv sketch. The characters have different backgrounds and goals which dictate how they will interact and how the story will develop. The background and goals of the characters are like the parameters of a model, with the resulting performance analogous to the model’s predicted behavior.",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-importance-of-fitting-data",
    "href": "intro.html#the-importance-of-fitting-data",
    "title": "1  Introduction",
    "section": "1.3 The importance of fitting data",
    "text": "1.3 The importance of fitting data\n“Fitting” a model to data means finding the combination of parameter values for which the model produces behavior that most closely resembles that produced by a participant in some task. The value of doing this is that it helps us understand why that participant acted the way they did.\nFor example, we might want to know whether someone was fast because they were able to quickly accumulate the evidence they needed, because they were uncautious, because they were biased, or because they could execute motor actions quickly. We can address that question by finding the values of the parameters associated with each construct that best fit their observed performance.\n\n\n\n\nBroadbent, D. E. (1957). A mechanical model for human attention and immediate memory. Psychological Review, 64(3), 205–215.\n\n\nCox, G. E., & Shiffrin, R. M. (2024). Computational models of event memory. In M. J. Kahana & A. Wagner (Eds.), Oxford handbook of human memory. Oxford University Press.\n\n\nDennett, D. (1980). The milk of human intentionality. Behavioral and Brain Sciences, 3(3), 428–430. https://doi.org/10.1017/s0140525x0000580x\n\n\nNeisser, U. (1967). Cognitive psychology. Appleton-Century-Crofts.\n\n\nSingmann, H., Kellen, D., Cox, G. E., Chandramouli, S. H., Davis-Stober, C. P., Dunn, J. C., Gronau, Q. F., Kalish, M. L., McMullin, S. D., Navarro, D. J., & Shiffrin, R. M. (2022). Statistics in the service of science: Don’t let the tail wag the dog. Computational Brain & Behavior.",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ebrw.html",
    "href": "ebrw.html",
    "title": "2  Exemplar of the Exemplar Based Random Walk",
    "section": "",
    "text": "2.1 Summed similarity\nFor concreteness, we imagine an experiment in which several color patches were studied. Colors can be described in terms of three features: hue, chroma (or “saturation”), and luminance (or “brightness”). Imagine that you saw many reddish-hued colors that varied in chroma and luminance. According to the EBRW, each of the colors you saw would leave a trace in memory. Each trace would consist of the values of the color you saw along those three dimensions. In this example, all the colors have the same value for “hue”, which is 0. The value of “0” for “hue” corresponds to a reddish color. The colors differ in their values for chroma and luminance, as illustrated in the graph below:\nCode\ncolDF &lt;- expand_grid(h = hue, c = seq(30, 70, length.out = 5), l = seq(30, 70, length.out = 5)) %&gt;%\n    mutate(col = hcl(h, c, l))\n\ncolDF %&gt;%\n    ggplot(aes(x = c, y = l, fill = col)) +\n    geom_tile(width = 2.5, height = 2.5) +\n    scale_fill_identity() +\n    coord_equal() +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", caption = bquote(plain(hue) == .(round(hue))))\nYour memory traces can also be written as a matrix, where each row corresponds to an item (color) and each column corresponds to a feature (h for “hue”, c for “chroma”, and l for “luminance”):\nCode\nknitr::kable(head(colDF %&gt;% select(h, c, l), 10), row.names = TRUE)\n\n\n\n\n\n\nh\nc\nl\n\n\n\n\n1\n0\n30\n30\n\n\n2\n0\n30\n40\n\n\n3\n0\n30\n50\n\n\n4\n0\n30\n60\n\n\n5\n0\n30\n70\n\n\n6\n0\n40\n30\n\n\n7\n0\n40\n40\n\n\n8\n0\n40\n50\n\n\n9\n0\n40\n60\n\n\n10\n0\n40\n70\nThese coordinates will often be derived using Multidimensional Scaling (MDS) (Nosofsky, 1992; Shepard, 1962a, 1962b). This type of analysis derives a spatial representation of a set of stimuli on the basis of similarity judgments provided by raters.",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#summed-similarity",
    "href": "ebrw.html#summed-similarity",
    "title": "2  Exemplar of the Exemplar Based Random Walk",
    "section": "",
    "text": "2.1.1 Perceived distance and similarity\nThe perceived distance between any two of these colors is the Euclidean distance between their feature values, weighted by the degree of attention given to each feature: \\[\nd_{ij} = \\sqrt{\\sum_{k = 1}^{N_F} w_k \\left(x_{ik} - x_{jk} \\right)^2}\n\\] where \\(w_k\\) is the weight given to feature \\(k\\), \\(x_{ik}\\) is the value of item \\(i\\) on feature \\(k\\), \\(x_{jk}\\) is the value of item \\(j\\) on feature \\(k\\), \\(d_{ij}\\) is the perceived distance between items \\(i\\) and \\(j\\), and \\(N_F\\) is the number of features which in this example is 3 (hue, chroma, and luminance). Note also that while Euclidean distance is appropriate for colors, other types of distance metrics may be more appropriate for other kinds of stimuli.\nPerceived similarity between two items, \\(s_{ij}\\), is an exponential function of the psychological distance between those items: \\[\ns_{ij} = \\exp(-c d_{ij})\n\\] where \\(c\\) is a sensitivity parameter that controls how quickly perceived similarity decreases with distance, as illustrated in the graph below:\n\n\nCode\nexpand_grid(d = seq(0, 10, length.out = 151), c = seq(0.25, 3, by = 0.25)) %&gt;%\n    mutate(s = exp(-c * d)) %&gt;%\n    ggplot(aes(x = d, y = s, color = c, group = c)) +\n    geom_line() +\n    scale_color_continuous_sequential() +\n    labs(x = expression(d[ij]), y = expression(s[ij])) +\n    theme(legend.position = c(1, 1), legend.justification = c(1, 1))\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 Summed similarity\nImagine that you have just been shown two colors from among the set shown earlier. We then present you with a third color—a probe item—and ask whether it was one of the two you just saw. You answer this question on the basis of the summed similarity between the probe item and the two items in memory. The probe item has its own vector of feature values, \\(\\mathbf{x_q}\\). The perceived distance between the probe item and each of the memory items is, as above, the Euclidean distance between the probe’s feature values and those of the memory item: \\[\n\\begin{align}\nd_{qj} & = \\sqrt{\\sum_{k = 1}^{N_F} w_k \\left(x_{qk} - x_{jk} \\right)^2} \\\\\ns_{qj} & = \\exp \\left( -c d_{qj} \\right) \\\\\nS & = \\sum_{j = 1}^{N_M} s_{qj}\n\\end{align}\n\\] and, again, the perceived similarity \\(s_{qj}\\) between the probe \\(q\\) and memory item \\(j\\) is an exponential function of perceived distance \\(d_{qj}\\). Finally, summed similarity \\(S\\) is the sum of the perceived similarities across all \\(N_M\\) items in memory.\nThe graphs below illustrate how this works. The left graph shows contours of equal similarity from each of two study items. The right graph shows summed similarity as a function of the chroma and luminance of a probe item (assuming the same hue).\n\n\nCode\nX &lt;- matrix(\n    c(40, 60,\n      60, 50),\n    nrow = 2,\n    ncol = 2,\n    byrow = TRUE\n)\n\nsens &lt;- 0.05\n\ncolDF &lt;- tibble(h = hue, c = X[,1], l = X[,2]) %&gt;%\n    mutate(col = hcl(h, c, l))\n\nitem_cols &lt;- as.vector(colDF$col)\nnames(item_cols) &lt;- as.character(1:length(item_cols))\n\nsimDF &lt;- expand_grid(c = seq(30, 70, length.out=101), l = seq(30, 70, length.out=101), item = c(1, 2)) %&gt;%\n    mutate(d = sqrt((c - X[item, 1])^2 + (l - X[item, 2])^2)) %&gt;%\n    mutate(s = exp(-sens * d))\n\nconPlot &lt;- colDF %&gt;%\n    ggplot(aes(x = c, y = l)) +\n    geom_contour(aes(z = s, color = factor(item)), data = simDF, alpha = 2/3, breaks = seq(0, 1, by = 0.2)) +\n    geom_tile(aes(fill = col), width = 2.5, height = 2.5) +\n    scale_fill_identity(aesthetics = \"fill\") +\n    scale_color_manual(values = item_cols, aesthetics = \"color\", guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Contours of equal similarity\", caption = bquote(list(plain(hue) == .(round(hue)), c == .(signif(sens, 3)))))\n\nsumSimPlot &lt;- simDF %&gt;%\n    group_by(c, l) %&gt;%\n    summarize(summed_sim = sum(s), .groups = \"keep\") %&gt;%\n    ggplot(aes(x = c, y = l, fill = summed_sim)) +\n    geom_raster() +\n    scale_fill_viridis_c(option = \"magma\", limits = c(0, 3), guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Summed similarity\", caption = bquote(list(plain(hue) == .(round(hue)), c == .(signif(sens, 3)))))\n\nconPlot + sumSimPlot + plot_layout(nrow = 1)\n\n\n\n\n\n\n\n\n\n\n2.1.2.1 Trace strength\nIt is reasonable to believe that some memory traces are stronger than others, likely due to things like primacy and recency. In GCM/EBRW, “strength” is operationalized as a scaling factor \\(m_j\\) applied to perceived similarity: \\[\ns_{qj} = m_j \\exp \\left( -c d_{qj} \\right)\n\\]\nStronger traces have their similarity multiplied by a large value (\\(m_j\\) is large if trace \\(j\\) is strong) while weaker traces have their similarity multiplied by a small value (\\(m_j\\) is small if trace \\(j\\) is weak). This is illustrated in the pair of graphs below. A probe item does not need to be as similar to a strong item in order to evoke the same level of perceived similarity.\n\n\nCode\nX &lt;- matrix(\n    c(40, 60,\n      60, 50),\n    nrow = 2,\n    ncol = 2,\n    byrow = TRUE\n)\n\nsens &lt;- 0.05\nstrength &lt;- c(1, 2)\n\ncolDF &lt;- tibble(h = hue, c = X[,1], l = X[,2]) %&gt;%\n    mutate(col = hcl(h, c, l)) %&gt;%\n    mutate(label = c(\"1\", \"2\"))\n\nitem_cols &lt;- as.vector(colDF$col)\nnames(item_cols) &lt;- as.character(1:length(item_cols))\n\nsimDF &lt;- expand_grid(c = seq(30, 70, length.out=101), l = seq(30, 70, length.out=101), item = c(1, 2)) %&gt;%\n    mutate(d = sqrt((c - X[item, 1])^2 + (l - X[item, 2])^2)) %&gt;%\n    mutate(s = exp(-sens * d) * strength[item])\n\nconPlot &lt;- colDF %&gt;%\n    ggplot(aes(x = c, y = l)) +\n    geom_contour(aes(z = s, color = factor(item)), data = simDF, alpha = 2/3, breaks = seq(0, 2, by = 0.2)) +\n    geom_tile(aes(fill = col), width = 2.5, height = 2.5) +\n    geom_text(aes(label = label), color = \"black\") +\n    scale_fill_identity(aesthetics = \"fill\") +\n    scale_color_manual(values = item_cols, aesthetics = \"color\", guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Contours of equal similarity\", caption = bquote(list(plain(hue) == .(round(hue)), c == .(signif(sens, 3)), m[1] == .(signif(strength[1], 3)), m[2] == .(signif(strength[2], 3)))))\n\nsumSimPlot &lt;- simDF %&gt;%\n    group_by(c, l) %&gt;%\n    summarize(summed_sim = sum(s), .groups = \"keep\") %&gt;%\n    ggplot(aes(x = c, y = l, fill = summed_sim)) +\n    geom_raster() +\n    scale_fill_viridis_c(option = \"magma\", limits = c(0, 3), guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Summed similarity\", caption = bquote(list(plain(hue) == .(round(hue)), c == .(signif(sens, 3)), m[1] == .(signif(strength[1], 3)), m[2] == .(signif(strength[2], 3)))))\n\nconPlot + sumSimPlot + plot_layout(nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n2.1.2.2 Trace specificity\nThe specificity parameter \\(c\\) represents the fact that items are assumed to be encoded with some degree of error/uncertainty. Just as items may be encoded with more or less strength, it is reasonable to assume that items can be encoded in memory with more or less specificity. Thus, we add a subscript to the \\(c\\) parameter corresponding to each study item: \\[\ns_{qj} = m_j \\exp \\left( -c_j d_{qj} \\right)\n\\]\nIf an item is encoded with high specificity, then it will only be perceived as similar to the probe if the probe is very close in psychological space. This is illustrated in the pair of graphs below, where item 2 is not only stronger (\\(m_2 = 2\\) vs. \\(m_1 = 1\\)) but also more precise (\\(c_2 = 0.1\\) vs. \\(c_1 = 0.05\\)).\n\n\nCode\nX &lt;- matrix(\n    c(40, 60,\n      60, 50),\n    nrow = 2,\n    ncol = 2,\n    byrow = TRUE\n)\n\nspecificity &lt;- c(0.05, 0.1)\nstrength &lt;- c(1, 2)\n\ncolDF &lt;- tibble(h = hue, c = X[,1], l = X[,2]) %&gt;%\n    mutate(col = hcl(h, c, l)) %&gt;%\n    mutate(label = c(\"1\", \"2\"))\n\nitem_cols &lt;- as.vector(colDF$col)\nnames(item_cols) &lt;- as.character(1:length(item_cols))\n\nsimDF &lt;- expand_grid(c = seq(30, 70, length.out=101), l = seq(30, 70, length.out=101), item = c(1, 2)) %&gt;%\n    mutate(d = sqrt((c - X[item, 1])^2 + (l - X[item, 2])^2)) %&gt;%\n    mutate(s = exp(-specificity[item] * d) * strength[item])\n\nconPlot &lt;- colDF %&gt;%\n    ggplot(aes(x = c, y = l)) +\n    geom_contour(aes(z = s, color = factor(item)), data = simDF, alpha = 2/3, breaks = seq(0, 2, by = 0.2)) +\n    geom_tile(aes(fill = col), width = 2.5, height = 2.5) +\n    geom_text(aes(label = label), color = \"black\") +\n    scale_fill_identity(aesthetics = \"fill\") +\n    scale_color_manual(values = item_cols, aesthetics = \"color\", guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Contours of equal similarity\", caption = bquote(list(plain(hue) == .(round(hue)), m[1] == .(signif(strength[1], 3)), m[2] == .(signif(strength[2], 3)), c[1] == .(signif(specificity[1], 3)), c[2] == .(signif(specificity[2], 3)))))\n\nsumSimPlot &lt;- simDF %&gt;%\n    group_by(c, l) %&gt;%\n    summarize(summed_sim = sum(s), .groups = \"keep\") %&gt;%\n    ggplot(aes(x = c, y = l, fill = summed_sim)) +\n    geom_raster() +\n    scale_fill_viridis_c(option = \"magma\", limits = c(0, 3), guide = \"none\") +\n    coord_equal(xlim = c(30, 70), ylim = c(30, 70)) +\n    labs(x = \"Chroma (saturation)\", y = \"Luminance (brightness)\", subtitle = \"Summed similarity\", caption = bquote(list(plain(hue) == .(round(hue)), m[1] == .(signif(strength[1], 3)), m[2] == .(signif(strength[2], 3)), c[1] == .(signif(specificity[1], 3)), c[2] == .(signif(specificity[2], 3)))))\n\nconPlot + sumSimPlot + plot_layout(nrow = 1)",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#making-a-recognition-decision-from-random-walk-to-diffusion",
    "href": "ebrw.html#making-a-recognition-decision-from-random-walk-to-diffusion",
    "title": "2  Exemplar of the Exemplar Based Random Walk",
    "section": "2.2 Making a recognition decision: From random walk to diffusion",
    "text": "2.2 Making a recognition decision: From random walk to diffusion\nAccording to the EBRW, each trace \\(j\\) in memory races to be retrieved at a rate proportional to its perceived similarity to the probe item, \\(s_{qj}\\). Traces race not just against one another, but against a criterion. If a memory trace wins the race, this is taken as evidence that the probe item matches something in memory, thus favoring a “yes” recognition response. If the criterion wins the race instead, this is taken as evidence that the probe item does not match anything in memory, thus favoring a “no” recognition response.\nThe idea is that, if the probe item matches something in memory, then the corresponding memory trace (or one sufficiently similar to probably match) should be able to win against the criterion. If nothing wins against the criterion, then this suggests there are no traces in memory that are a good match to the probe.\n\n2.2.1 Accumulating memory evidence\nThe outcome of each race is added to a running tally which starts at zero. The value of the tally at any given time \\(t\\), which we can denote \\(x(t)\\), constitutes the current state of evidence from memory for making a recognition decision. Assume that each race takes \\(\\Delta t\\) seconds to complete. Each time a memory trace wins the race, the tally gets incremented by one; each time the criterion wins the race, the tally gets decremented by one. Put formally, we can write this process as \\[\nx\\left( t + \\Delta t \\right) =\n\\begin{cases}\nx\\left( t \\right) + 1 & \\text{if trace wins} \\\\\nx\\left( t \\right) - 1 & \\text{if criterion wins}\n\\end{cases}\n\\] where \\(x(0) = 0\\).\n\n\n2.2.2 Step probabilities\nAlthough we have specified that whether memory evidence goes up or down depends on whether a trace or the criterion wins the race, we have not yet specified how the outcome of that race is determined. The winner of each race is random but depends on the similarity \\(s_{qj}\\) between each trace \\(j\\) and the probe \\(q\\). Specifically, trace \\(j\\) wins the race with probability: \\[\n\\frac{s_{qj}}{\\sum_{k = 1}^N s_{qk} + \\kappa}\n\\] where \\(\\kappa\\) is a nonnegative number that represents how stringent the criterion is. In other words, the equation above says that the probability that trace \\(j\\) wins the race is the similarity between trace \\(j\\) and the probe \\(q\\) divided by the summed similarity across all traces plus the criterion \\(\\kappa\\). In a sense, we can think of the criterion is like a “virtual memory trace” that races alongside the \\(N\\) actual memory traces.\nRemember that we increment memory strength whenever any trace wins the race, regardless of which one it is. Because only one trace can win each race, the probability that any trace wins is just the sum of the probabilities of winning across all \\(N\\) traces, i.e.: \\[\np = \\sum_{j = 1}^N \\frac{s_{qj}}{\\sum_{k = 1}^N s_{qk} + \\kappa} = \\frac{1}{\\sum_{k = 1}^N s_{qk} + \\kappa} \\left( \\sum_{j = 1}^Ns_{qj} \\right) = \\frac{S}{S + \\kappa}\n\\] where \\(S = \\sum_{j = 1}^N s_{qj}\\) is the summed similarity across all \\(N\\) traces. The quantity \\(p\\) is the probability that the random walk takes a step up.\nThe EBRW models the speed of decision making in terms of how many races must be run until the accumulated win advantage in favor of either a “yes” or “no” response reaches a decision boundary. To convert this to “real time”, we must say how long each race takes and allow for a residual time. Above, we used \\(\\Delta t\\) to stand for the amount of time (in seconds) each race takes to run. It will be convenient later to think instead of \\(\\nu = \\frac{1}{\\Delta t}\\), where \\(\\nu\\) is the number of races per second.\nThe figure below shows an example of how memory evidence evolves during a single trial in which \\(p = 0.55\\), \\(\\nu = 40\\), \\(B_{Upper} = 7\\), \\(B_{Lower} = -8\\), and \\(t_0 = 0.2\\). In addition, the graphs above and below the evidence trajectory illustrate the relative frequency with which, across many identical trials, each type of response would be made at each unit of time. Note that these distributions are discrete because the random walk operates in discrete time intervals, each of duration \\(\\Delta t\\) (which in this example is \\(\\Delta t = \\frac{1}{\\nu} = 0.025\\) seconds).\n\n\nCode\nset.seed(1)\n\nnu &lt;- 40\np &lt;- 0.55\n\nB &lt;- c(-8, 7)\nresid &lt;- 0.2\n\n### RT distributions\n\nY_rw &lt;- seq(B[1], B[2])\nP_rw &lt;- matrix(0, nrow = length(Y_rw), ncol = length(Y_rw))\nP_rw[cbind(2:(nrow(P_rw) - 1), 1:(ncol(P_rw) - 2))] &lt;- 1 - p\nP_rw[cbind(2:(nrow(P_rw) - 1), 3:ncol(P_rw))] &lt;- p\nP_rw[1, 1] &lt;- P_rw[nrow(P_rw), ncol(P_rw)] &lt;- 1\n\n### Simulation\n\nwhile (TRUE) {\n    winner &lt;- 0\n    x_rw &lt;- 0\n\n    while (TRUE) {\n        s &lt;- 2 * (runif(n = 1) &lt; p) - 1\n        x_rw &lt;- c(x_rw, x_rw[length(x_rw)] + s)\n        if (x_rw[length(x_rw)] &lt;= B[1]) {\n            winner &lt;- 1\n            break\n        } else if (x_rw[length(x_rw)] &gt;= B[2]) {\n            winner &lt;- 2\n            break\n        }\n    }\n    \n    if (winner == 2 & (length(x_rw) / nu) &gt; 1.5) {\n        break\n    }\n}\n\nRT_rw &lt;- matrix(0, nrow = 2, ncol = length(x_rw))\nZ_rw &lt;- 1 * c(Y_rw == 0)\n\nfor (i in 1:length(x_rw)) {\n    Z_rw &lt;- Z_rw %*% P_rw\n    RT_rw[1, i] &lt;- Z_rw[1]\n    RT_rw[2, i] &lt;- Z_rw[length(Z_rw)]\n}\n\ndRT_rw &lt;- apply(RT_rw, MARGIN = 1, FUN = diff) * nu / 2\n\nrtPlot1 &lt;- tibble(t = resid + 1:length(x_rw) / nu, p = c(0, dRT_rw[,2])) %&gt;%\n    ggplot(aes(x = t, y = p)) +\n    geom_col(fill = \"#eeb211\", color = NA, width = 1 / nu) +\n    coord_cartesian(xlim = c(0, NA), ylim = c(0, max(c(dRT_rw)))) +\n    labs(x = NULL, y = \"Pr(Respond at time t)\") +\n    theme(axis.text = element_blank(), axis.ticks.y = element_blank())\n\nrtPlot0 &lt;- tibble(t = resid + 1:length(x_rw) / nu, p = c(0, dRT_rw[,1])) %&gt;%\n    ggplot(aes(x = t, y = p)) +\n    geom_col(fill = \"#46166b\", color = NA, width = 1 / nu) +\n    labs(x = NULL, y = \"Pr(Respond at time t)\") +\n    scale_x_continuous(limits = c(0, NA)) +\n    scale_y_reverse(limits = c(max(c(dRT_rw)), 0)) +\n    theme(axis.text = element_blank(), axis.ticks.y = element_blank())\n\nrwPlot &lt;- tibble(t = resid + 1:length(x_rw) / nu, x = x_rw) %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step(linewidth = 1.5) +\n    geom_hline(yintercept = B[2], linetype = \"solid\", color = \"#eeb211\", linewidth = 2) +\n    geom_hline(yintercept = B[1], linetype = \"solid\", color = \"#46166b\", linewidth = 2) +\n    geom_vline(xintercept = resid, linetype = \"dashed\", color = \"#666666\", linewidth = 2) +\n    geom_text(data = tibble(x = 0, y = B[2], label = paste0(\"B[Upper] == \", B[2])), mapping = aes(x = x, y = y, label = label), color = \"#eeb211\", inherit.aes = FALSE, parse = TRUE, hjust = 0, vjust = 2) +\n    geom_text(data = tibble(x = 0, y = B[1], label = paste0(\"B[Lower] == \", B[1])), mapping = aes(x = x, y = y, label = label), color = \"#46166b\", inherit.aes = FALSE, parse = TRUE, hjust = 0, vjust = -1) +\n    geom_text(data = tibble(x = resid, y = 0, label = paste0(\"t[0] == \", resid)), mapping = aes(x = x, y = y, label = label), color = \"#666666\", inherit.aes = FALSE, parse = TRUE, hjust = 1.1, vjust = 1.5) +\n    coord_cartesian(xlim = c(0, NA)) +\n    labs(x = \"Retrieval time (s)\", y = \"Memory evidence\", caption = bquote(list(p == .(p), nu == .(nu))))\n\nrtPlot1 + rwPlot + rtPlot0 + plot_layout(ncol = 1, heights = c(1, 1.75, 1))\n\n\n\n\n\n\n\n\n\n\n\n2.2.3 From discrete random walk to continuous diffusion\nA random walk takes discrete-valued steps either up or down in discrete units of time. A Wiener diffusion process takes continuous-valued steps sampled from a normal distribution in infinitely small units of time, thus effectively operating in continuous time. We are going to approximate the discrete EBRW with a continuous diffusion process (so technically we should call this model the EBD for Exemplar-Based Diffusion, but we will keep calling it the EBRW for posterity).\nIn going from a random walk to a diffusion model, we are making an important psychological claim: We are saying that, instead of memory evidence arriving in discrete units at regular intervals, memory evidence is a continuous value that continually evolves as new information arrives. We can think of this as saying that, instead of only knowing the outcome of each race, you can see who is ahead and who is behind at any given time; this is the move from discrete time to continuous time. Moreover, instead of only scoring each race as a win or loss for the memory traces, the races are assigned a continuous value depending on how clear the winner is; this is the move from discrete evidence to continuous evidence.\n\n2.2.3.1 Mean and standard deviation of diffusion\nWe can write the update equation for the random walk like we did above: \\[\n\\begin{align}\nx \\left( t + \\Delta t \\right) & = \\begin{cases} x(t) + 1 & \\text{with probability } p \\\\ x(t) - 1 & \\text{with probability } 1 - p \\end{cases} \\\\\nx \\left( t + \\Delta t \\right) - x(t) & = \\begin{cases} 1 & \\text{with probability } p \\\\ -1 & \\text{with probability } 1 - p \\end{cases} \\\\\nx \\left( t + \\Delta t \\right) - x(t) & \\sim 2 \\times \\text{Bernoulli} \\left( p \\right) - 1\n\\end{align}\n\\] where we have rearranged terms and used the shorthand in the final line to emphasize the idea that each step of the random walk can be thought of as a sample from a Bernoulli distribution with parameter \\(p\\) that is then transformed from \\(\\lbrace 0, 1 \\rbrace\\) to \\(\\lbrace -1, 1 \\rbrace\\).\nTo turn this into a continuous diffusion process, we need to swap out the transformed Bernoulli distribution with a normal distribution that has the same mean and variance. The mean is \\(2 p - 1\\) and the variance is \\(4 p \\left(1 - p \\right)\\). One more thing: remember that we run \\(\\nu\\) races per second, so we need to multiply the mean and variance by \\(\\nu\\). Therefore, the mean drift rate is \\(v = \\nu \\left(2 p - 1 \\right)\\) and the variance is \\(\\sigma^2 = 4 \\nu p (1 - p)\\).\nNote that this is different from the typical diffusion model where the variance of the evidence samples is arbitrarily fixed to 1. Notice an important property of this variance: It is largest when \\(p = 0.5\\) and approaches zero as \\(p\\) approaches either 0 or 1. In other words, the more uncertain the outcome of each race, the more noise there is in the diffusion. This is illustrated below:\n\n\nCode\nexpand_grid(p = seq(0, 1, length.out = 101), nu = seq(10, 40, by = 10)) %&gt;%\n    mutate(sigma2 = 4 * nu * p * (1 - p)) %&gt;%\n    ggplot(aes(x = p, y = sigma2, color = nu, group = nu)) +\n    geom_line() +\n    labs(x = p, y = expression(sigma^2), color = expression(nu)) +\n    theme(legend.position = c(1, 1), legend.justification = c(1, 1))\n\n\n\n\n\n\n\n\n\nTo summarize, the difference between the random walk and the diffusion is that we have swapped out a discrete binomial distribution of evidence increments per unit time with a continuous normal distribution of evidence increments per unit time. Everything else is the same: You still respond “yes” if and when the accumulated evidence \\(x(t)\\) reaches either the upper boundary or the lower boundary.\n\n\n2.2.3.2 Closeness of predictions\nTo illustrate how well the diffusion process approximates the random walk, the graphs below show the diffusion approximation to the same random walk example used above. The smooth lines in the upper and lower graphs are the probability of responding per unit time (i.e., the probability density function) according to the Wiener diffusion model. The open bars are the same probabilities from the random walk. The diffusion model’s predictions hew very closely to those of the random walk!\n\n\nCode\nmu &lt;- nu * (2 * p - 1)\nsigma2 &lt;- 4 * nu * p * (1 - p)\nboundsep &lt;- B[2] - B[1]\nbias &lt;- (0 - B[1]) / (B[2] - B[1])\ndelta_t &lt;- 0.001\n\nwhile (TRUE) {\n    winner_diff &lt;- NA\n    x_diff &lt;- 0\n    \n    while (TRUE) {\n        x_diff &lt;- c(x_diff, x_diff[length(x_diff)] + rnorm(n = 1, mean = mu * delta_t, sd = sqrt(sigma2 * delta_t)))\n        if (x_diff[length(x_diff)] &lt;= B[1]) {\n            winner_diff &lt;- 1\n            break\n        } else if (x_diff[length(x_diff)] &gt;= B[2]) {\n            winner_diff &lt;- 2\n            break\n        }\n    }\n    \n    if (winner == winner_diff & abs((length(x_diff) * delta_t) - (length(x_rw) / nu)) &lt; (1 / nu)) {\n        break\n    }\n}\n\nx_diff &lt;- pmax(pmin(x_diff, B[2]), B[1])\n\nt &lt;- seq(1, length(x_diff)) * delta_t\n\ndRT_diff &lt;- cbind(\n    WienerPDF(t = t, response = \"lower\", a = boundsep / sqrt(sigma2), v = mu / sqrt(sigma2), w = bias)$value,\n    WienerPDF(t = t, response = \"upper\", a = boundsep / sqrt(sigma2), v = mu / sqrt(sigma2), w = bias)$value\n)\n\nrtPlot1 &lt;- tibble(t = resid + t, p = dRT_diff[,2]) %&gt;%\n    ggplot(aes(x = t, y = p)) +\n    geom_col(data = tibble(t = resid + 1:length(x_rw) / nu, p = c(0, dRT_rw[,2])), color = \"#eeb211aa\", fill = NA, width = 1 / nu) +\n    geom_area(fill = \"#eeb21177\", color = \"#eeb211\") +\n    coord_cartesian(xlim = c(0, NA), ylim = c(0, max(c(dRT_diff)))) +\n    labs(x = NULL, y = \"Pr(Respond at time t)\") +\n    theme(axis.text = element_blank(), axis.ticks.y = element_blank())\n\nrtPlot0 &lt;- tibble(t = resid + t, p = dRT_diff[,1]) %&gt;%\n    ggplot(aes(x = t, y = p)) +\n    geom_col(data = tibble(t = resid + 1:length(x_rw) / nu, p = c(0, dRT_rw[,1])), color = \"#46166baa\", fill = NA, width = 1 / nu) +\n    geom_area(fill = \"#46166b77\", color = \"#46166b\") +\n    labs(x = NULL, y = \"Pr(Respond at time t)\") +\n    scale_x_continuous(limits = c(0, NA)) +\n    scale_y_reverse(limits = c(max(c(dRT_diff)), 0)) +\n    theme(axis.text = element_blank(), axis.ticks.y = element_blank())\n\nrwPlot &lt;- tibble(t = resid + t, x = x_diff) %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_line() +\n    geom_hline(yintercept = B[2], linetype = \"solid\", color = \"#eeb211\", linewidth = 2) +\n    geom_hline(yintercept = B[1], linetype = \"solid\", color = \"#46166b\", linewidth = 2) +\n    geom_vline(xintercept = resid, linetype = \"dashed\", color = \"#666666\", linewidth = 2) +\n    geom_text(data = tibble(x = 0, y = B[2], label = paste0(\"B[Upper] == \", B[2])), mapping = aes(x = x, y = y, label = label), color = \"#eeb211\", inherit.aes = FALSE, parse = TRUE, hjust = 0, vjust = 2) +\n    geom_text(data = tibble(x = 0, y = B[1], label = paste0(\"B[Lower] == \", B[1])), mapping = aes(x = x, y = y, label = label), color = \"#46166b\", inherit.aes = FALSE, parse = TRUE, hjust = 0, vjust = -1) +\n    geom_text(data = tibble(x = resid, y = 0, label = paste0(\"t[0] == \", resid)), mapping = aes(x = x, y = y, label = label), color = \"#666666\", inherit.aes = FALSE, parse = TRUE, hjust = 1.1, vjust = 1.5) +\n    coord_cartesian(xlim = c(0, NA)) +\n    labs(x = \"Retrieval time (s)\", y = \"Memory evidence\", caption = bquote(list(p == .(p), nu == .(nu), v == .(signif(mu, 3)), sigma == .(signif(sqrt(sigma2), 3)))))\n\nrtPlot1 + rwPlot + rtPlot0 + plot_layout(ncol = 1, heights = c(1, 1.75, 1))",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#implementation-in-r",
    "href": "ebrw.html#implementation-in-r",
    "title": "2  Exemplar of the Exemplar Based Random Walk",
    "section": "2.3 Implementation in R",
    "text": "2.3 Implementation in R\nTo find the EBRW parameters that best fit the recognition data from a participant, let’s implement the diffusion version of the EBRW in R using the WienR package. We must define a function to compute the negative log-likelihood (NLL) for a set of observed responses/RT’s, given a set of parameters. The function itself, in outline form, looks like the one below. I have written comments for the things that the function needs to accomplish to get from what is given to the function (in the parentheses following function) to what the function needs to return at the end.\nFor example purposes, this implementation doesn’t include all of the bells and whistles that the full model includes. It will not allow for varying trace strength nor will it include attention weights on each dimension. This is meant to illustrate the basic idea that we can define a diffusion model in which the drift rates are derived from a theory, rather than just estimated.\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    # 2. Calculate the log-likelihood of each observed response/RT on each trial\n    \n    # 3. Return final result\n    return(nll)\n}\n\n\nLet’s now fill in each of those sections in turn.\n\n2.3.1 Parameters\nThe vector par that is the first argument to the ebrw_nll function should be a named vector that has the following entries:\n\n\nCode\npar &lt;- c(\n    \"retrieval_rate\" = 3,     # This is the \"nu\" parameter\n    \"a\" = 2,                  # Response caution\n    \"w\" = 0.5,                # Response bias\n    \"t0\" = 0,                 # Residual time\n    \"specificity\" = 1,        # Specificity of memory representations (the \"c\" parameter)\n    \"criterion\" = 1           # Criterion (the \"kappa\" parameter)\n)\n\n\n\n\n2.3.2 Computing the mean and SD of the drift for each trial\nRecall that the drift rates depend on the distances between each of the stimulus items. Since the function is provided with stim_coords, we can make our job a little easier by using the dist function to compute the matrix of distances between all pairs of items. This saves us from “recomputing” the distances between pairs of items that occur on multiple trials:\n\n\nCode\nstim_dists &lt;- as.matrix(dist(stim_coords))\n\n\nThen, to compute the summed similarity for each trial i, we can use a for loop:\n\n\nCode\nevidence_mean &lt;- rep(0, length(probe_item))\nevidence_sd &lt;- rep(0, length(probe_item))\n\nfor (i in 1:length(probe_item)) {\n    summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n    p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n    \n    evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n    evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n}\n\n\nWe have now completed the second step of writing the ebrw_nll function, as summarized below.\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    stim_dists &lt;- as.matrix(dist(stim_coords))\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    # 2. Calculate the log-likelihood of each observed response/RT on each trial\n    \n    # 3. Return final result\n    return(nll)\n}\n\n\n\n\n2.3.3 Calculating the log-likelihood\nThis step is almost too easy. We are using the WienR package, which means we can use the WienerPDF function like we’ve seen already. There is only one thing we need to do: The WienerPDF function assumes that the standard deviation of the diffusion is always equal to one. As such, we need to standardize the drift rate and boundary separation before we send them to the WienerPDF function by dividing each by evidence_sd:\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    stim_dists &lt;- as.matrix(dist(stim_coords))\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    # 2. Calculate the log-likelihood of each observed response/RT on each trial\n    result &lt;- WienerPDF(\n        t = rt,\n        response = response,\n        a = par[\"a\"] / evidence_sd,\n        w = par[\"w\"],\n        v = evidence_mean / evidence_sd,\n        t0 = par[\"t0\"]\n    )\n    \n    # 3. Return final result\n    return(-sum(result$logvalue))\n}\n\n\n\n\n2.3.4 Error-checking\nIt is important for us to do some error checking. Sometimes, a particular combination of parameters will make it impossible for the WienerPDF function to calculate the log-likelihood. When that happens, it gives an error. In essence, such a result tells us that the model cannot work with that combination of parameters. Thus, rather than an “error”, that is really telling us that we should assign zero likelihood to that set of parameters, which is equivalent to a log-likelihood of \\(-\\infty\\).\nWe can do that kind of check in R by putting the WienerPDF function call within try(). If the WienerPDF function gives an error, then the result that gets stored in trial_wiener is also an error. Otherwise, it just gives us the log-likelihoods that we want.\nLet’s set up an “if…else” structure to do this check:\n\n\nCode\n# Function arguments:\n# par: this is a named vector of parameter values\n# stim_coords: this is a matrix of the coordinates of the stimuli, where each row is a stimulus and each column is a dimension\n# study_items: this is a matrix where each row is a trial and each column indicates the items that were studied on that trial\n# probe_item: this is a vector giving the index of the probe item on each trial\n# response: this is a vector where each value is 2 or 1, depending on whether the participant responsed \"yes\" (2) or \"no\" (1) on that trial\n# rt: this is a vector of the response times from each trial\nebrw_nll &lt;- function(par, stim_coords, study_items, probe_item, response, rt) {\n    # 1. Compute the mean and SD of the drift for each trial\n    \n    stim_dists &lt;- as.matrix(dist(stim_coords))\n    \n    evidence_mean &lt;- rep(0, length(probe_item))\n    evidence_sd &lt;- rep(0, length(probe_item))\n    \n    for (i in 1:length(probe_item)) {\n        summed_sim &lt;- sum(exp(-par[\"specificity\"] * stim_dists[probe_item[i], study_items[i,]]))\n        p &lt;- summed_sim / (summed_sim + par[\"criterion\"])\n        \n        evidence_mean[i] &lt;- par[\"retrieval_rate\"] * (2 * p - 1)\n        evidence_sd[i] &lt;- 2 * sqrt(par[\"retrieval_rate\"] * p * (1 - p))\n    }\n    \n    # 2. Calculate the log-likelihood of each observed response/RT on each trial\n    result &lt;- try(WienerPDF(\n        t = rt,\n        response = response,\n        a = par[\"a\"] / evidence_sd,\n        w = par[\"w\"],\n        v = evidence_mean / evidence_sd,\n        t0 = par[\"t0\"]\n    ))\n    \n    # 3. Return final result\n    if (class(result) == \"try-error\") {\n        return(Inf)\n    } else {\n        return(-sum(result$logvalue))\n    }\n}",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "ebrw.html#a-worked-example",
    "href": "ebrw.html#a-worked-example",
    "title": "2  Exemplar of the Exemplar Based Random Walk",
    "section": "2.4 A worked example",
    "text": "2.4 A worked example\nThis example uses a single participant’s data from Experiment 2 of Gillespie & Cox (2024). In this experiment, each participant made similarity ratings between all pairs of eight items. Each item was an “auditory texture” constructed via Fourier synthesis. We applied Multidimensional Scaling to the similarity ratings from each participant to assign, for each participant, a set of coordinates to each item. The coordinates are such that items that are farther from one another were associated with lower similarity ratings and those that were closer to one another were assigned higher similarity ratings. Be sure to check out the paper itself for additional detail on how this was done, and how we decided that the multidimensional space in which the stimuli are represented had 3 dimensions. These coordinates will be used for the stim_coords argument of the ebrw_nll function.\nIn addition to providing similarity ratings, each participant engaged in a recognition memory task. On each trial of this task, the participant heard two auditory textures, presented sequentially. They then heard a “probe” sound and had to decide whether or not it was one of the two sounds that had just been presented. It is these recognition data that we will model with the EBRW.\nYou can grab the data yourself by running the following chunk of code to download it and load it into your R workspace:\n\n\nCode\ndownload.file(\"https://github.com/gregcox7/choice_rt_models/raw/refs/heads/main/data/ebrw_example_data.rdata\", \"blast_data.rdata\")\nload(\"ebrw_example_data.rdata\")\n\n\n\n2.4.1 Check out the data\nThe stimulus coordinates are saved in a matrix called stim_coords:\n\n\nCode\nprint(stim_coords)\n\n\n           [,1]         [,2]        [,3]\n[1,] -0.4679162 -0.463219039 -0.08056499\n[2,] -0.5549266  0.499390636  0.07099460\n[3,] -0.4742833 -0.132020672  0.43312165\n[4,] -0.5062581  0.005578674 -0.40175331\n[5,]  0.5110397 -0.222844222 -0.30381582\n[6,]  0.4578305  0.336495895 -0.27302187\n[7,]  0.5669455 -0.288366241  0.24624698\n[8,]  0.4675684  0.264984969  0.30879277\n\n\nWe can visualize them using plot_ly:\n\n\nCode\nto_plot &lt;- as.data.frame(stim_coords)\ncolnames(to_plot) &lt;- paste(\"Dimension\", 1:ncol(stim_coords))\nrownames(to_plot) &lt;- paste(\"Item\", 1:nrow(stim_coords))\n\nplot_ly(data = to_plot, x = ~ `Dimension 1`, y = ~ `Dimension 2`, z = ~ `Dimension 3`, type = \"scatter3d\", text = rownames(to_plot))\n\n\nNo scatter3d mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\nIn addition for each trial of the recognition task, the study_items matrix tells us which of the two items had been presented as part of the set to be remembered and the probe_item vector tells us what the probe item was. These numbers refer to row in the stim_coords matrix.\n\n\nCode\nprint(study_items)\n\n\n      [,1] [,2]\n [1,]    3    5\n [2,]    3    6\n [3,]    8    1\n [4,]    1    7\n [5,]    2    4\n [6,]    7    2\n [7,]    6    7\n [8,]    8    3\n [9,]    3    4\n[10,]    5    6\n[11,]    6    8\n[12,]    1    4\n[13,]    4    8\n[14,]    7    4\n[15,]    6    1\n[16,]    1    3\n[17,]    1    2\n[18,]    3    2\n[19,]    6    4\n[20,]    8    2\n[21,]    3    7\n[22,]    4    5\n[23,]    5    2\n[24,]    1    5\n[25,]    7    8\n[26,]    2    6\n[27,]    7    5\n[28,]    5    8\n\n\nCode\nprint(probe_item)\n\n\n [1] 4 2 1 7 2 7 7 2 6 6 6 1 3 4 4 3 8 3 8 4 7 8 4 2 6 2 3 5\n\n\nFinally, the rt and response vectors record the response time (in seconds) and the response (where 2 is “yes” and 1 is “no”) produced by this participant on each trial.\n\n\n2.4.2 Finding optimal parameters\nThe version of the EBRW that we applied in our paper is a bit more complex than the one we will use here, which only has six free parameters. We will use R’s built-in nlminb function to find the best-fitting values of these parameters. To do this, we need to specify initial values for each parameter in a named vector, as shown below. These initial values don’t necessarily need to be anything in particular as long as they don’t cause the ebrw_nll function to return an error or a value of Inf.\n\n\nCode\ninit_par &lt;- c(\n    \"retrieval_rate\" = 3,     # This is the \"nu\" parameter\n    \"a\" = 2,                  # Response caution\n    \"w\" = 0.5,                # Response bias\n    \"t0\" = 0,                 # Residual time\n    \"specificity\" = 1,        # Specificity of memory representations (the \"c\" parameter)\n    \"criterion\" = 1           # Criterion (the \"kappa\" parameter)\n)\n\n\nWe also need to specify the upper and lower values that each of these parameters could possibly take, as shown below:\n\n\nCode\nlower &lt;- c(\n    \"retrieval_rate\" = 0,     # This is the \"nu\" parameter\n    \"a\" = 0,                  # Response caution\n    \"w\" = 0,                  # Response bias\n    \"t0\" = 0,                 # Residual time\n    \"specificity\" = 0,        # Specificity of memory representations (the \"c\" parameter)\n    \"criterion\" = 0           # Criterion (the \"kappa\" parameter)\n)\n\nupper &lt;- c(\n    \"retrieval_rate\" = Inf,     # This is the \"nu\" parameter\n    \"a\" = Inf,                  # Response caution\n    \"w\" = 1,                    # Response bias\n    \"t0\" = min(rt),             # Residual time\n    \"specificity\" = Inf,        # Specificity of memory representations (the \"c\" parameter)\n    \"criterion\" = Inf           # Criterion (the \"kappa\" parameter)\n)\n\n\nNote that these upper and lower values can be Infinite if necessary!\nFinally, let’s use the nlminb function, which we need to provide with each of the ingredients we prepared above. We will save the result as fit:\n\n\nCode\nfit &lt;- nlminb(\n    start = init_par,          # Need to provide initial guess of parameter values\n    objective = ebrw_nll,      # Tell R the name of the function to optimize\n    lower = lower,             # The lower bounds on each parameter\n    upper = upper,             # The upper bounds on each parameter\n    stim_coords = stim_coords, # The coordinates of each stimulus\n    rt = rt,                   # The vector of RT's on each trial\n    response = response,       # The vector of responses on each trial\n    study_items = study_items, # The study items on each trial\n    probe_item = probe_item    # The probe item on each trial\n)\n\n\nWarning in nlminb(start = init_par, objective = ebrw_nll, lower = lower, :\nNA/NaN function evaluation\n\n\nAnd the final result!\n\n\nCode\nfit\n\n\n$par\nretrieval_rate              a              w             t0    specificity \n     1.2474191      2.1459586      0.4488126      0.6884095      5.6470633 \n     criterion \n     0.1035233 \n\n$objective\n[1] 21.59716\n\n$convergence\n[1] 0\n\n$iterations\n[1] 35\n\n$evaluations\nfunction gradient \n      56      243 \n\n$message\n[1] \"relative convergence (4)\"\n\n\nThe beauty of this result is that we have explained why this participant did what they did in terms of\n\nTheir internal representations of the stimuli, modeled as coordinates in a latent psychological space.\nA decision process that involves continuously sampling exemplars from memory until a criterion is reached.\n\n\n\n\n\nGillespie, N. F., & Cox, G. E. (2024). Perception and memory for novel auditory stimuli: Similarity, serial position, and list homogeneity. PsyArXiv. https://doi.org/10.31234/osf.io/n294a\n\n\nNosofsky, R. M. (1986). Attention, similarity, and the identification-categorization relationship. Journal of Experimental Psychology: General, 115(1), 39–57.\n\n\nNosofsky, R. M. (1992). Similarity scaling and cognitive process models. Annual Review of Psychology, 43, 25–53.\n\n\nNosofsky, R. M., Cox, G. E., Cao, R., & Shiffrin, R. M. (2014). An exemplar-familiarity model predicts short-term and long-term probe recognition across diverse forms of memory search. Journal of Experimental Psychology: Learning, Memory, and Cognition, 40(6), 1524–1539.\n\n\nNosofsky, R. M., Little, D. R., Donkin, C., & Fific, M. (2011). Short-term memory scanning viewed as exemplar-based categorization. Psychological Review, 118(2), 280–315.\n\n\nNosofsky, R. M., & Palmeri, T. J. (1997). An exemplar-based random walk model of speeded classification. Psychological Review, 104(2), 266–300.\n\n\nShepard, R. N. (1962a). The analysis of proximities: Multidimensional scaling with an unknown distance function. I. Psychometrika, 27(2), 125–140. https://doi.org/https://doi.org/10.1007/BF02289630\n\n\nShepard, R. N. (1962b). The analysis of proximities: Multidimensional scaling with an unknown distance function. II. Psychometrika, 27(3), 219–246. https://doi.org/https://doi.org/10.1007/BF02289621",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exemplar of the Exemplar Based Random Walk</span>"
    ]
  },
  {
    "objectID": "r_coding.html",
    "href": "r_coding.html",
    "title": "3  Programming with R",
    "section": "",
    "text": "3.1 Tips and strategies",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming with R</span>"
    ]
  },
  {
    "objectID": "r_coding.html#tips-and-strategies",
    "href": "r_coding.html#tips-and-strategies",
    "title": "3  Programming with R",
    "section": "",
    "text": "When trying to write code to accomplish a particular task—or when trying to understand code written by someone else—try to break the task into individual steps that are accomplished in sequence in order to yield the final result.\nIf you are unsure what a particular bit of code will do—for example, if you want to figure out how to code one of the steps you’ve identified above—try to construct a minimal working example. This example should be simple enough that you can figure out what the result should be without doing any code. Then you can try out the code and verify whether the result matches what you expected.\nThe same principles that underlie producing good code also underlie debugging code. This is covered well in the corresponding chapter of Wickham’s Advanced R book, but essentially debugging involves (a) isolating where the problem arises; (b) understanding the conditions that resulted in the problem; (c) revising the code in an attempt to correct the problem or to prevent problematic circumstances from arising; and (d) testing to ensure the solution in fact addresses the problem. Working on small bits of code at a time makes all of the essential steps of debugging easier.\nR supports vectorization of many operations. While vectorization allows for code to run more efficiently, the resulting code can sometimes be harder to understand and debug. As a result, you may want to write a less efficient but easier to read version of your code first, so that you can verify that it works the way you expect. You can then see where you might want to try to make your code more efficient, using your original easy-to-read code to verify that any changes you make don’t alter the expected result.\nUsing named vectors/matrices/arrays can often be quite handy when you want to index values by using a string that describes their meaning or interpretation, rather than a numerical index. Not only can this make your code more interpretable, it avoids issues when you may not know ahead of time which numerical index contains a particular desired value.\nR has a tendency to recycle things in ways you may not expect! For example, when doing any operation involving multiple vectors, if one vector is shorter than the other, R will sometimes “recycle” the elements of the shorter vector to create a new vector that is the same length as the longer one. The rules that govern how R “recycles” are not consistently applied and can be hard to predict, therefore it is important to ensure that your code will not produce this kind of ambiguous situation. You may want to include error checks to ensure that vectors are the same length. Alternatively, if you want to recycle, you can do so explicitly so there is no ambiguity (e.g., x_recycled &lt;- rep(x, length(y))[1:length(y)]).\nAlways remember the drop option when subsetting an array, matrix, or data frame! If the subsetting procedure selects only a single element, unless you use drop = FALSE, the result will be a length-one vector that “throws out” the other dimensions of your data structure. This can result in bugs if your code assumes that the result of the subset will have a consistent number of dimensions.",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming with R</span>"
    ]
  },
  {
    "objectID": "r_coding.html#exercise-fibonnacci",
    "href": "r_coding.html#exercise-fibonnacci",
    "title": "3  Programming with R",
    "section": "3.2 Exercise: Fibonnacci",
    "text": "3.2 Exercise: Fibonnacci\nThe following set of R coding exercises are meant to prepare you for the kind of coding that will be involved in writing our first cognitive model simulations. It is not exhaustive of all the things that you can do with R, but it addresses many of the essentials. It also exemplifies the workflow involved in building a model:\n\nImplement the basic processes involved in a simple case where you know what the correct result should be, so you can ensure you have implemented these basics appropriately.\nBuild a function that generalizes the code you wrote in step 1 so that you can apply it to different parameter settings.\nUse your function to simulate different results using different parameter settings.\nExplore the range of results your function produces across parameter settings.\nOptionally, consider ways that you can generalize your model even further by incorporating additional parameters.\n\nThese exercises are based on everyone’s favorite sequence, the Fibonnacci sequence. The sequence is defined by a simple rule: the next value in the sequence is the sum of the previous two values. Written in Math, that’s: \\[\nf[i] = f[i - 2] + f[i - 1]\n\\] where \\(f[\\cdot]\\) is a value in the Fibonnacci sequence and \\(i\\) is the index of the next value. To get this sequence going, we need to know the first two values, \\(f[1]\\) and \\(f[2]\\). Typically, these are both set to 1. As a result, the beginning of the Fibonnacci sequence goes like this: \\[\n1, 1, 2, 3, 5, 8, 13, \\ldots\n\\]\nAnyway, let’s begin!\n\n3.2.1 Exercise 1\nWrite two versions of a chunk of code that will create a vector called fib that contains the first 20 values in the Fibonnacci sequence. Assume that the first two values in the sequence are 1 and 1. Write one version of the code that creates the vector by appending each new value to the end of fib. Write another version that assigns values to the corresponding entries in fib directly using the appropriate index (for this second version, you may want to use the rep function to create the fib vector).\n\n\n3.2.2 Exericse 2\nBased on the code you wrote for the previous exercise, write a function that returns a vector containing the first N terms of the Fibonnacci sequence. Your function should take two arguments, the value N and a vector called init that contains the first two values in the sequence. Give those arguments sensible default values. The chunk below gives a sense of the overall structure your function should have:\n\n\nCode\nfib_func &lt;- function(N = ___, init = ___) {\n    ...\n    return(fib)\n}\n\n\n\n\n3.2.3 Exercise 3\nWrite code that calls the function you wrote in the previous exercise several times, each time using a different value for the second value in the init argument (but the same value for N and for init[1]). Collect the results from each function call in a single data frame or tibble. The data frame or tibble should have a column that stores the second initial value, a column for the vector returned from the function, and a third column that is the value’s index within the sequence. An example of the kind of result you’re looking for is given below:\n\n\n# A tibble: 8 × 3\n  init2   fib     i\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     1     1     1\n2     1     1     2\n3     1     2     3\n4     1     3     4\n5     4     1     1\n6     4     4     2\n7     4     5     3\n8     4     9     4\n\n\n\n\n3.2.4 Exercise 4\nWrite code that uses the ggplot2 library to plot the values of the Fibonnacci sequence on the y-axis against their position in the sequence on the x-axis. Distinguish between different init2 values by using different colored lines. The result should look something like the plot below.\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 Extension exercise\nWrite a new function that takes a third argument, n_back, which specifies how many of the previous values to add up to create the next value in the sequence. For the Fibonnacci sequence, n_back = 2, but in principle we could define other kinds of sequences too. Adapt the code you wrote for your previous exercises to explore what happens with different values of n_back. You may also want to include some code at the beginning of your function that checks to ensure that the number of initial values provided in the init argument is sufficient!",
    "crumbs": [
      "Perspectives",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming with R</span>"
    ]
  },
  {
    "objectID": "random_walk.html",
    "href": "random_walk.html",
    "title": "4  Building a random walk model to simulate choice and RT",
    "section": "",
    "text": "4.1 Out for a random walk\nThe random walk model is designed to account for two aspects of behavior: choice and response time (RT). These and similar models are applied in situations where a person (or other organism!) has to decide between a small number of possible alternatives, often just two. Such situations abound in experimental psychology, including lexical decision, recognition memory, detection tasks, search tasks, categorization tasks, etc. The models are designed to help us understand two things:",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#out-for-a-random-walk",
    "href": "random_walk.html#out-for-a-random-walk",
    "title": "4  Building a random walk model to simulate choice and RT",
    "section": "",
    "text": "Why did a participant make the choices they made?\nWhy did it take the participant a certain amount of time to make the choices they made?\n\n\n4.1.1 Key theoretical constructs\nThe vast majority of models of choice and RT, including random walk and diffusion models, address the two questions above by invoking four basic theoretical constructs:\nEvidence accumulation: Choosing from among a set of options is assumed to require accumulating “evidence” that weighs either in favor of or against each option. This evidence may come from perception (e.g., deciding which of two stimuli is brighter), from memory (e.g., deciding whether an item was or was not on a previously-studied list), or from deliberation (e.g., deciding which of two products to buy). As such, the idea that decisions are made by accumulating evidence helps explain not only which choice was made (it was the option that was most favored by the accumulated evidence) and how long it took to made the choice (the time needed to accumulate sufficient evidence to commit to a decision).\nResponse caution: If decisions are made by accumulating evidence, there must be a policy that terminates the accumulation process, otherwise someone would keep accumulating evidence forever—and this is exactly what Buridan’s ass needs to avoid! The construct of “response caution” refers to the idea that, depending on the situation, a decision maker may adopt a policy of deciding quickly on the basis of very little evidence (low response caution) or deciding slowly by waiting for more evidence to accumulate (high response caution). Thus, this construct is directly related to the idea of speed-accuracy trade-off.\nResponse bias: It may be that a decision maker is willing to commit to some options more readily than others; in that case, we say they are “biased” in favor of those responses. Typically, this bias is modeled by assuming lower response caution for some options than others. In other words, a participant may be willing to commit to some decisions on the basis of less accumulated evidence than others.\nResidual time: The time needed to accumulate sufficient evidence to make a decision is not the only thing that contributes to observed response times. After all, it takes time to realize that a trial of a task has actually begun. It may also take time to retrieve relevant information from memory, to focus attention on relevant features in the environment, or to evaluate a potential outcome. Finally, it takes some time to execute the motor actions associated with the chosen option (e.g., to press a button, move a lever, etc.). The time for all of these additional processes is often called non-decision time (NDT) or encoding and response execution time (\\(T_{ER}\\)). However, I prefer to call it simply “residual time” because that is what it is—it is the time “left over” besides the time needed for evidence accumulation.\n\n\n4.1.2 Representing the current state of evidence\nThe random walk model assumes that, at any given time, a decision maker represents the current balance of evidence between two options as a number. We will creatively refer to this representation as \\(x(t)\\), where \\(x\\) stands for evidence and \\((t)\\) stands for the fact that it is the evidence at a specific time \\(t\\). The sign and magnitude of \\(x(t)\\) represents the extent to which the current value of evidence favors one option over the other.\nIf \\(x(t)\\) equals zero, then the evidence at time \\(t\\) does not favor either option. This is akin to the situation when Buridan’s ass first encounters the two piles of hay. If \\(x(t) &gt; 0\\), then the evidence favors one of the two options. For Buridan’s ass, perhaps positive values of evidence represent evidence in favor of going toward the pile of hay on the right. If \\(x(t) &lt; 0\\), then the evidence favors the other option. For Buridan’s ass, maybe negative values of evidence represent evidence in favor of going toward the pile of hay on the left. Notice that we could just as easily do it the other way around: positive evidence favors going left while negative evidence favors going right. The important thing is just that the two options are associated with opposite signs of evidence.\nIn a cognitive task, the two choices might be “word” and “non-word” in a lexical decision task, “old” and “new” in a recognition memory task, “present” and “absent” in a visual search task, “same” and “different” in a change detection task, “category A” and “category B” in a categorization task, etc. Again, the point is that, at any given time, the degree to which the decision maker’s accumulated evidence at time \\(t\\) favors one option or the other is represented by the value of a number \\(x(t)\\), with each option associated with opposite signs.\n\n\n4.1.3 Accumulating evidence\nThe value of \\(x(t)\\) represents the evidence that has been accumulated by time \\(t\\). But what does it mean to “accumulate” evidence? And what is the “evidence” that is accumulated?\nIn a random walk model, we assume that at regular time intervals (each interval has duration \\(\\Delta t\\)), the decision maker receives a “sample” of evidence, which we will label \\(\\Delta x(t)\\). This sample can take one of two values, \\(+1\\) or \\(-1\\). If it is \\(+1\\), the sample favors the option associated with positive evidence values (e.g., the pile of hay on the right) and if it is \\(-1\\), the sample favors the option associated with negative evidence values (e.g., the pile of hay on the left). To accumulate evidence means to add the new sample \\(\\Delta x(t)\\) to the current value of the accumulated evidence, i.e.: \\[\n\\overbrace{x(t + \\Delta t)}^{\\text{Updated evidence}} = \\overbrace{x(t)}^{\\text{Current accumulated evidence}} + \\overbrace{\\Delta x(t)}^{\\text{Current sample of evidence}}\n\\] Thus, the accumulated evidence \\(x(t)\\) is the sum of all the samples of evidence that were obtained by time \\(t\\).\n\n\n4.1.4 What is evidence?\nAt this point, it would be reasonable to ask where these samples of evidence come from. There is no single answer to this question because the random walk model, like most of the models of choice and RT we will consider, treats evidence in a very abstract sense. Later in the course, we will encounter models that instantiate specific theories of the “evidence” a decision maker may use in a specific context.\nTo return to Buridan’s ass, the evidence might be perceptual in nature: For an interval of time, the donkey looks at both piles of hay. Even though both piles are, by assumption, equally big, that may not always be visually apparent. During any finite interval of time, one pile might happen to look ever so slightly larger than the other, perhaps due to a quirk of the light, a sheaf fluttering in the breeze, the donkey’s visual acuity, etc. If the pile on the right happened to look a bit bigger than the one on the left during one of those intervals, then the sample of evidence for that interval would be \\(+1\\). Otherwise, it would be \\(-1\\). Because these minute differences are due to essentially chance factors, and they are equally likely to favor either pile, we can say that the probability of getting a sample that is either \\(+1\\) or \\(-1\\) is \\(0.5\\). While the evidence might not favor one pile over the other in the long run, it will favor one option over a finite interval of time, which is all any real decision maker has at their disposal. As we shall see shortly, this is the key to saving Buridan’s ass.\nTreating evidence as due, at least in part, to chance factors is why this model is called a “random” walk. It also highlights the fact that the evidence samples need not occur with equal frequency. Perhaps samples come up \\(+1\\) with probability \\(p\\) and otherwise come up \\(-1\\), like the proverbial biased coin flip. If the evidence consistently favors one option, that means that \\(p\\) is close to either 1 or 0. To the extent that chance factors influence the evidence, \\(p\\) will be closer to \\(0.5\\). We have now been introduced to the first parameter of the random walk model: \\(p\\), the probability of getting a sample of evidence that favors the option associated with positive evidence.\nThe figure below illustrates different ways that evidence might accumulate over time. Each step up or down is driven by the sample of evidence that was obtained at that time, which is assumed to be random with probability \\(p\\). The figure also illustrates why this model is called a random “walk”, because each trajectory kind of looks like a path that someone might have walked.\n\n\nCode\nexpand_grid(p = c(0.2, 0.5, 0.8), sim_index = 1:5, t = 1:20) %&gt;%\n    mutate(x_sample = 2 * rbinom(n = n(), size = 1, prob = p) - 1) %&gt;%\n    group_by(p, sim_index) %&gt;%\n    mutate(x_accum = cumsum(x_sample)) %&gt;%\n    ggplot(aes(x = t, y = x_accum, color = factor(p), group = interaction(p, sim_index))) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_step(alpha = 0.5) +\n    labs(x = \"Time interval\", y = \"Accumulated evidence\", color = \"p\")\n\n\n\n\n\n\n\n\n\nWhat about evidence in cognitive tasks? Buridan’s ass relies on the same kind of sensory evidence as one needs to do, for example, psychophysical tasks like picking which stimulus is brighter, more leftward-oriented, etc. Evidence derived from memory can also be noisy—perhaps when retrieving an event, you sometimes recall the color of an object as blue and sometimes as green. When deciding between different gambles or products, we may also shift attention to different features of those options, leading us to judge them as better or worse depending on which features we attend to (Busemeyer & Townsend, 1993; Diederich, 1997).\n\n\n4.1.5 Doing some code\nHaving now familiarized ourselves with how the random walk model represents a decision maker’s evidence and how it processes that evidence via accumulation, let’s see how we would write that model in code. Specifically, we will be writing code that simulates different possible random walks. The way we will do this is more of an intellectual exercise, since we will not be striving for efficiency (later on, we will use special-purposes R packages for that). Rather, the point here is to see how the conceptual aspects of a model can be implemented in code. We will add on to this code as we go.\nFor now, we know that we will have a line that looks something like the accumulation equation above:\n\n\nCode\nx &lt;- x + x_sample\n\n\nHere, x stands for the value of the accumulated evidence and x_sample stands for the current sample of evidence (which is either 1 or -1). The &lt;- evaluates the expression on the right and assigns it to the thing on the left, so the code above says “take the current value of x, add the new sample x_sample, and put it back as the new value of x”.\nWith the code above as the core of the model, we now need to do three things: first, specify how to get x_sample; second, obtain many such samples; third, keep a record of how the accumulated evidence changes over time.\n\n4.1.5.1 Sampling evidence\nTo get a value for x_sample, we will use R’s rbinom function, which generates a random sample from a binomial distribution. Specifically, the line rbinom(n = 1, size = 1, prob = 0.5) will generate a sample that equals 1 with probability 0.5, otherwise it equals zero. It is perhaps easiest to think of it in terms of a coin flip: The n parameter of the rbinom function says how many sets of coins to flip, size says how many coins we flip in each set, and prob is the probability that any single flip comes up heads. The number that rbinom gives is the number of heads in a particular set of flips.\nFor Buridan’s ass, the sample of evidence favors each pile equally often, so prob = 0.5 makes sense. Note that because rbinom returns either a 0 or a 1, we need to do some math to turn the result into \\(+1\\) or \\(-1\\). This is shown below.\n\n\nCode\nx_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\nx &lt;- x + x_sample\n\n\n\n\n4.1.5.2 Obtaining many samples\nThere are a few ways we can write code that will obtain many samples. To anticipate what we will be doing later, we will use the while control structure. We can use it to specify a condition such that, so long as the condition is met, a block of code will continue to be executed in a loop.\nOur condition will depend on the current time. Remember that, in the random walk, each sample of evidence arrives at fixed intervals of time. We will therefore need to keep track of the current time as well as the accumulated evidence. Similar to how we updated the evidence, we will need to keep track of t, the current time. We will also need to specify dt, the duration of each interval (otherwise known as \\(\\Delta t\\)), and t_max, the amount of time to keep accumulating evidence.\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n}\n\n\nNotice that we specified values for t_max and dt outside the while loop. We can specify initial values for x and t the same way:\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nx &lt;- 0\nt &lt;- 0\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n}\n\n\n\n\n4.1.5.3 Keeping a record\nThe chunk of code above will work just fine! But unfortunately it does not leave a record of accumulated evidence over time that we can then examine, like we did with the graph above. In the chunk below, we use a fun trick to keep a record of each value of x and t: We create two vectors x_record and t_record and use the c function to concatenate the current values of x and t to these vectors:\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nx &lt;- 0\nt &lt;- 0\n\nx_record &lt;- x\nt_record &lt;- t\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n    x_record &lt;- c(x_record, x)\n    t_record &lt;- c(t_record, t)\n}\n\n\n\n\n4.1.5.4 Visualizing the record\nNow that we are keeping a record of evidence over time, let’s visualize it! The code below uses base R for that purpose, although the graph above uses ggplot2 which we will use again later. The type = \"s\" setting in the plot function at the end give the “step-like” plot.\n\n\nCode\nt_max &lt;- 5\ndt &lt;- 0.05\n\nx &lt;- 0\nt &lt;- 0\n\nx_record &lt;- x\nt_record &lt;- t\n\nwhile (t &lt; t_max) {\n    x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1\n    x &lt;- x + x_sample\n    t &lt;- t + dt\n    x_record &lt;- c(x_record, x)\n    t_record &lt;- c(t_record, t)\n}\n\nplot(t_record, x_record, type = \"s\", xlab = \"Time\", ylab = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nTry copy-pasting the code above and running it yourself a few times to see what it looks like!\n\n\n4.1.5.5 Making a function\nIf we have a chunk of code that we want to re-run many times, we would do better to write a function that we can call instead of having to re-run the whole chunk. Writing a function also makes it easier to deal with parameters that can have different settings, like dt and t_max. We will also make the probability \\(p\\) a parameter too. Finally, the values for these three parameters in the function line are defaults.\n\n\nCode\nrw_sim &lt;- function(p = 0.5, dt = 0.05, t_max = 5) {\n    x &lt;- 0\n    t &lt;- 0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (t &lt; t_max) {\n        x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = p) - 1\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(tibble(t = t_record, x = x_record))\n}\n\n\nNow we can call the function rw_sim with different settings to simulate different random walks. Note that, because the function returns t_record and x_record as different columns of a tibble, we can easily use ggplot2 to plot the results, as in the examples below.\n\n\nCode\nsim_result1 &lt;- rw_sim(p = 0.5, dt = 0.05, t_max = 5)\n\nsim_result1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.5\")\n\n\n\n\n\n\n\n\n\nCode\nsim_result2 &lt;- rw_sim(p = 0.2, dt = 0.05, t_max = 5)\n\nsim_result2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.2\")\n\n\n\n\n\n\n\n\n\nCode\nsim_result3 &lt;- rw_sim(p = 0.8, dt = 0.05, t_max = 5)\n\nsim_result3 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.8\")\n\n\n\n\n\n\n\n\n\nGo ahead, try it out yourself with different values of p, dt, and/or t_max. It’s fun! And if you don’t think the step graphs are too interesting, just imagine that each of those steps is Simon the donkey trying to decide between his two piles of hay.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#making-a-decision",
    "href": "random_walk.html#making-a-decision",
    "title": "4  Building a random walk model to simulate choice and RT",
    "section": "4.2 Making a decision",
    "text": "4.2 Making a decision\nSo far, we have built a simple model of evidence accumulation. In this model, samples of “evidence” arrive at regular intervals, with the sample supporting either one option (\\(+1\\)) or the other (\\(-1\\)) with probability \\(p\\), and the decision maker accumulates these samples by summation. The resulting accumulated evidence thus starts at zero and takes a “random walk” that can drift upward (if \\(p &gt; 0.5\\)), downward (if \\(p &lt; 0.5\\)), or in no particular direction (if \\(p = 0.5\\)).\n\n4.2.1 Response boundaries\nWhat we have not done is say how the decision maker uses this accumulated evidence to decide between their two options. According to the random walk model, the decision maker sets two values prior to accumulating evidence. These values are called thresholds, criteria, or boundaries (these terms are often used interchangeably). There is one positive boundary and one negative boundary. If and when the accumulated evidence crosses one of these boundaries, the decision maker selects the corresponding option.\nFor example, say that Buridan’s ass will pick the pile on the right if his accumulated evidence ever gets greater than \\(+5\\) and he will pick the pile on the left if his accumulated evidence ever gets less than \\(-5\\). We can visualize this situation by overlaying lines at those two boundaries on the “random walk” of accumulating evidence:\n\n\nCode\nburidan_sim1 &lt;- rw_sim(p = 0.5)\n\nburidan_sim1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nHere’s another one:\n\n\nCode\nburidan_sim2 &lt;- rw_sim(p = 0.5)\n\nburidan_sim2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nGo ahead and try it out yourself!\nThe point is that we can read from these graphs which option the donkey ends up picking by seeing which boundary gets crossed first. We can also see when the donkey makes his decision based on how long it took for that first boundary-crossing to occur. This is how the random walk model ultimately produces both a choice (which boundary was crossed first) and an RT (how long it took). It is also why the random walk saves Buridan’s ass: Even if the evidence in the long run does not favor either option, by chance the accumulated evidence will at some point cross one of the boundaries, enabling the donkey to make a decision.\n\n\n4.2.2 Response bias\nIn the examples above, Buridan’s ass set his response boundaries to be of equal distance from the initial evidence value of zero. Burdian’s ass might be more willing to go to the leftward pile than the rightward one—maybe it is more aesthetically appealing or the donkey has a limp that makes it easier for him to walk left than right. This would amount to a bias in favor of one option (going left) over the other (going right).\nWe can instantiate this bias in the random walk model via the donkey’s response boundaries. For example, the donkey may go to the left if the accumulated evidence ever gets less than \\(-4\\) but would only be willing to go to the right if the accumulated evidence ever gets greater than \\(+6\\). The following two graphs illustrate these biased response boundaries.\n\n\nCode\nburidan_bias_sim1 &lt;- rw_sim(p = 0.5)\n\nburidan_bias_sim1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"Response bias, simulation 1\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nburidan_bias_sim2 &lt;- rw_sim(p = 0.5)\n\nburidan_bias_sim2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"Response bias, simulation 2\")\n\n\n\n\n\n\n\n\n\nIntuitively, it seems reasonable to expect that, if one boundary is closer to the start than the other, that two things will happen: First, the option associated with the closer boundary will be picked more often (at least if the evidence against that option is not too strong). Second, the decision maker will tend to be faster to pick the option associated with the closer boundary. We will verify these intuitions later, but for now you can rest assured that these intuitions are correct.\n\n\n4.2.3 Revising our function\nNow that we have gotten acquainted with the notion of response boundaries and how they can be biased, let’s incorporate them into our random walk simulation function from earlier. This will involve two things: First, we will need to add two parameters to the function, one for each boundary. Second, we will need to change the condition in the while loop so that the random walk stops when it reaches a boundary. As a corrollary to this second step, we will keep the t_max condition but adjust the default value of t_max.\nThe revised function is shown below, with some additional explanation following:\n\n\nCode\nrw_sim &lt;- function(p = 0.5, b_upper = 5, b_lower = -5, dt = 0.05, t_max = Inf) {\n    x &lt;- 0\n    t &lt;- 0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = p) - 1\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(tibble(t = t_record, x = x_record))\n}\n\n\nThe key changes we made to the rw_sim function are:\n\nAdding parameters b_upper and b_lower for the upper and lower response boundaries, respectively.\nChanging the default value of t_max to Inf for “infinity”. This means that, by default, reaching a boundary is the only way the random walk will stop. However, by leaving t_max as a parameter, it means that we can set it to some real number like 5 or 10 to force the random walk to stop eventually.\nChanging the condition in the while loop. Now the walk will continue so long as the evidence x is below the upper boundary (x &lt; b_upper) and above the lower boundary (x &gt; b_lower) and so long as the maximum time hasn’t been reached (t &lt; t_max). Note that the & is a “logical and” operator.\n\nHere are a few simulation runs—try it out yourself!\n\n\nCode\nboundary_sim_result1 &lt;- rw_sim(p = 0.5, b_upper = 5, b_lower = -5)\n\nboundary_sim_result1 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.5, b_upper = 5, b_lower = -5\")\n\n\n\n\n\n\n\n\n\nCode\nboundary_sim_result2 &lt;- rw_sim(p = 0.5, b_upper = 6, b_lower = -4)\n\nboundary_sim_result2 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.5, b_upper = 6, b_lower = -4\")\n\n\n\n\n\n\n\n\n\nCode\nboundary_sim_result3 &lt;- rw_sim(p = 0.7, b_upper = 6, b_lower = -4)\n\nboundary_sim_result3 %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    geom_hline(yintercept = c(-4, 6), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"p = 0.7, b_upper = 6, b_lower = -4\")",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#residual-time",
    "href": "random_walk.html#residual-time",
    "title": "4  Building a random walk model to simulate choice and RT",
    "section": "4.3 Residual time",
    "text": "4.3 Residual time\nWe are nearly done with our simulation model! We can model accumulating evidence and making a decision. The final ingredient arises from the fact that, while a decision maker might select one option at a particular time, we can only observe the behavioral consequences of that decision. Those behavioral consequences might be hitting a key, clicking a button, pressing a lever, or walking toward a pile of hay. Executing that behavior takes time in addition to the time needed to accumulate evidence and reach a response boundary. That additional time goes by many names, often “non-decision time” (NDT) or “encoding and responding” time (\\(T_{ER}\\)), but I prefer to simply call it residual time.\nFor now, we will adopt a simple assumption that this residual time is constant. Therefore, the observed response time will be the sum of the time needed for the random walk to reach a boundary plus the residual time associated with all the other processes that are involved in taking an action but which our model doesn’t explicitly enumerate.\nTo make this concrete, let’s introduce a parameter called t0 that will stand for residual time. While I cannot speak to what a plausible value of t0 would be for Buridan’s ass, in many cognitive tasks, it tends to be around 0.2 or 0.3 seconds, to account for the time needed to execute a simple motor action like hitting a button.\n\n\nCode\nrw_sim &lt;- function(p = 0.5, b_upper = 5, b_lower = -5, t0 = 0.2, dt = 0.05, t_max = Inf) {\n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- 2 * rbinom(n = 1, size = 1, prob = p) - 1\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(tibble(t = t_record, x = x_record))\n}\n\n\nNote that the main change to our rw_sim function is that the initial value for the time t is no longer 0 but t0, i.e., the value of the residual time parameter.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#simulating-many-trials",
    "href": "random_walk.html#simulating-many-trials",
    "title": "4  Building a random walk model to simulate choice and RT",
    "section": "4.4 Simulating many trials",
    "text": "4.4 Simulating many trials\nOur rw_sim function can now simulate single realizations of a random walk decision process. As we have seen, though, each realization of this process is different because the samples of evidence are random. If we want to get a sense of the kind of behavior the model tends to produce, we need to simulate many realizations of the decision and examine the distribution of choices and RT’s produced by the model. This is the same reason why, in a typical cognitive task, we collect multiple trials from each participant in each condition. With a real participant, we are limited by the time and energy that a participant is willing to commit. With a model, we are still limited by time and energy, but they are our time and the computer’s energy. Nonetheless, it is worth keeping in mind that all the techniques below for visualizing choices and RT’s apply to observed data as well as they apply to simulated data.\n\n4.4.1 Running and saving many simulation results\nWe will need to write some code that repeatedly calls our rw_sim function a large number of times and saves the results so we can examine them later. What follows is not necessarily the most efficient way of accomplishing those goals, but it is conceptually transparent and introduces the for loop. The comments (following the # marks) explain what is going on with the line below.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- rw_sim(p = 0.5, b_upper = 5, b_lower = -5, t0 = 0.2, dt = 0.05)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Get a quick sense of what the results look like\nglimpse(sim_results)\n\n\nRows: 25,884\nColumns: 3\n$ t         &lt;dbl&gt; 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, …\n$ x         &lt;dbl&gt; 0, -1, -2, -3, -2, -3, -4, -3, -2, -3, -4, -3, -4, -3, -2, -…\n$ sim_index &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, …\n\n\n\n\n4.4.2 Visualizing the random walks\nWhat we are about to do may be a bit silly but helps build some intuitions about what is going on in the model. We are going to make a plot that overlays all 1000 simulated random walks on top of each other. The point is to get a sense of how much variability there is from one realization to the next.\n\n\nCode\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x, group = sim_index)) +\n    geom_step(alpha = 0.1) +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\")\n\n\n\n\n\n\n\n\n\nOkay, maybe it is a bit silly after all. But it is possible to see that things “thin out” at longer times as more and more random walks end by hitting a boundary. If you check out the code that generates the plot, note how group = sim_index was used to make sure each individual simulation, indexed by sim_index, got its own step-line on the graph. Also note the use of alpha = 0.1 to make each line semi-transparent so they could be overlayed on one another.\nLet’s try a different approach to visualize the same thing, using a heatmap that indicates the relative frequency with which the accumulated evidence takes different values at different times:\n\n\nCode\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    geom_hline(yintercept = c(-5, 5), linetype = \"dashed\") +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\")\n\n\n\n\n\n\n\n\n\nAgain, what is important to see above is that all the random walks start at the same time and evidence value (the yellow region) and then “fan out” over time.\n\n\n4.4.3 Joint distributions of choice and RT\nWhat we visualized in the previous section are the internal states of the model, that is, how the model represents the decision maker’s current balance of evidence between their two options. Remember, though, that the model is ultimately judged on its externally-observable behavior, since that is all we have to compare it against. We are finally going to visualize the choices and response times produced by the model. As we shall see, however, there are a few ways to do this!\n\n4.4.3.1 Extracting choices and RT’s\nFor each simulation, the RT is the final value of t, since that is the time (plus residual time) at which the first boundary was crossed. Meanwhile, the choice is whether the evidence x is positive or negative. The chunk of code below takes our simulation results and extracts the final choices and RT from each simulation.\n\n\nCode\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\nglimpse(choice_rt)\n\n\nRows: 1,000\nColumns: 3\n$ sim_index &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ choice    &lt;fct&gt; lower, lower, lower, upper, upper, lower, upper, lower, uppe…\n$ rt        &lt;dbl&gt; 1.05, 2.35, 0.95, 0.75, 0.55, 2.45, 1.15, 0.45, 1.35, 3.25, …\n\n\n\n\n4.4.3.2 Joint frequency plot\nThe code below plots the frequency with which each choice (upper or lower) is made at different times. This kind of plot is not terribly common, but is a quick way to get a sense of both how often each choice is made as well as the shape of the distributions of RT’s.\n\n\nCode\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_freqpoly(binwidth = 0.2) +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\")\n\n\n\n\n\n\n\n\n\n\n\n4.4.3.3 Conditional RT density\nThe code below plots the conditional density of the RT’s for each choice. This kind of plot is much more common, but doesn’t convey any information about the relative frequency with which different choices are made. Nonetheless, it illustrates how the random walk produces distributions of RT’s with a pronounced right skew, similar to RT distributions that are actually observed in choice tasks. Note that the conditional RT distributions for each choice are pretty similar to one another too.\n\n\nCode\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\")\n\n\n\n\n\n\n\n\n\n\n\n4.4.3.4 Quantile-probability plots\nIn the choice-RT modeling world, it is common to make “quantile-probability plots”, sometimes abbreviated to QP plots. These plots can be a bit confusing at first, but are useful because they convey information about choice proportions and RT distributions in a single graph.\nThe horizontal axis of a QP plot corresponds to the probability of having made a particular choice. In this case, that is the proportion of simulations that resulted in each choice. We can get that information in numerical form from our choice_rt tibble:\n\n\nCode\nchoice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\n\n# A tibble: 2 × 3\n  choice     n p_resp\n  &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 upper    507  0.507\n2 lower    493  0.493\n\n\nThe vertical axis of a QP plot corresponds to different quantiles of the conditional RT distributions for each choice. Typically, those quantiles are the RT’s at the 10th, 30th, 50th, 70th, and 90th percentiles of the distribution. The reason for all of these quantiles is that they convey information about different aspects of the distribution: The 50th percentile, otherwise known as the median, conveys the central tendency. The 30th and 70th percentiles indicate where the “bulk” of the RT’s tend to fall. Finally, the 10th and 90th percentiles convey information about the lower and upper tails of the distribution, respectively. We can obtain those quantiles numerically like so:\n\n\nCode\nchoice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\n\n# A tibble: 10 × 2\n   choice  rt_q\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 upper   0.55\n 2 upper   0.75\n 3 upper   1.05\n 4 upper   1.55\n 5 upper   2.65\n 6 lower   0.55\n 7 lower   0.85\n 8 lower   1.25\n 9 lower   1.75\n10 lower   2.75\n\n\nTo make a QP plot, we need to “join” together the response proportions and RT quantiles into the same tibble:\n\n\nCode\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q)\n\n\nJoining with `by = join_by(choice)`\n\n\n# A tibble: 10 × 4\n   choice     n p_resp  rt_q\n   &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 upper    507  0.507  0.55\n 2 upper    507  0.507  0.75\n 3 upper    507  0.507  1.05\n 4 upper    507  0.507  1.55\n 5 upper    507  0.507  2.65\n 6 lower    493  0.493  0.55\n 7 lower    493  0.493  0.85\n 8 lower    493  0.493  1.25\n 9 lower    493  0.493  1.75\n10 lower    493  0.493  2.75\n\n\nThat joined tibble can then be used as the basis for our QP plot:\n\n\nCode\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#putting-it-all-together",
    "href": "random_walk.html#putting-it-all-together",
    "title": "4  Building a random walk model to simulate choice and RT",
    "section": "4.5 Putting it all together",
    "text": "4.5 Putting it all together\nWe have used R to build a random walk model of decision making, implemented via a function called rw_sim, that accumulates samples of evidence until the accumulated evidence reaches either an upper or lower boundary. This model depends on several parameters, of which the most theoretically important are:\n\np: The probability that any given sample of evidence favors the option associated with the upper response boundary.\nb_upper: The upper response boundary.\nb_lower: The lower response boundary.\nt0: Residual time.\n\nWe also saw different ways that we can visualize both the internal states and external behavior of the model. It may be useful at this point to put together everything we have done so far into a single chunk of code. This will make your own explorations of this model easier.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- rw_sim(p = 0.5, b_upper = 5, b_lower = -5, t0 = 0.2, dt = 0.05)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "random_walk.html#exercises",
    "href": "random_walk.html#exercises",
    "title": "4  Building a random walk model to simulate choice and RT",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\n\nSet the p parameter to something other than 0.5, so that the evidence tends to favor one option over the other. Do one set of simulations in which the response boundaries are equidistant from the starting value of 0 (you may need to play around to find values that you like). Do another set of simulations in which you keep the boundaries equidistant but make them closer to the starting point. What is the effect on the model’s choices and RT’s of having boundaries that are closer to the starting point?\nRun one set of simulations with the p parameter to 0.6 and the response boundaries equidistant from the starting point. Run another set of simulations keeping the response boundaries the same but increasing the p parameter to 0.8. What is the effect of increasing the p parameter on the RT distributions for making the “upper” choice? What is the effect of increasing the p parameter on the RT distributions for making the “lower” choice?\nImagine that, instead of each sample of evidence equalling either \\(+1\\) or \\(-1\\), the evidence could also equal \\(0\\). Write code to simulate this model and use your simulations to see how this model might differ from the random walk model we developed in this chapter.\n\nYou will need to introduce a new parameter to the model that represents the probability of getting a sample that equals zero. What ways can you think of to implement this aspect of the model? Which method did you pick and why?\nHow does the shape of the predicted RT distributions differ, if at all, from that predicted by the original random walk model? (Hint: you may want to explore settings in which there is zero probability of taking a step either up or down. It may also help to visualize the random walks themselves too.)\nWhat cognitive tasks might be better modeled by allowing for evidence to have a value of zero?\n\nTry implementing a model in which the residual time can vary randomly according to some distribution. Since residual time must be non-negative, you might consider distributions like the Gamma distribution or a uniform distribution between two positive values.\n\nHow did you implement random residual times?\nHow does random residual time affect the shape of the predicted RT distributions?\nWhat psychological factors might contribute to variability in residual time?\n\n\n\n\n\n\nBusemeyer, J. R., & Townsend, J. T. (1993). Decision field theory: A dynamic–cognitive approach to decision making in an uncertain environment. Psychological Review, 100(3), 432–459.\n\n\nDiederich, A. (1997). Dynamic stochastic models for decision making under time constraints. Journal of Mathematical Psychology, 41(3), 260–274. https://doi.org/10.1006/jmps.1997.1167",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building a random walk model to simulate choice and RT</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html",
    "href": "diffusion_sim.html",
    "title": "5  From random walk to diffusion",
    "section": "",
    "text": "5.1 Discrete to continuous evidence\nLike most models of choice and RT, both random walk and diffusion models are premised on the idea that making a decision requires accumulating samples of “evidence” until the accumulated evidence reaches a response boundary. The “evidence” in these models is deliberately abstract because these models are meant to be applied in a variety of situations. The important thing is that “evidence” can be represented in these models as a number, where a sample of evidence supporting one option takes a positive value while a sample supporting the other option takes a negative value. A diffusion model differs from a random walk model in two aspects regarding the nature of the evidence that is accumulated:\nIn other words, the random walk model treats evidence as taking discrete values that are sampled at discrete intervals, whereas a diffusion model treats evidence as taking continuous values that are sampled continuously in time.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#discrete-to-continuous-evidence",
    "href": "diffusion_sim.html#discrete-to-continuous-evidence",
    "title": "5  From random walk to diffusion",
    "section": "",
    "text": "Samples of evidence take continuous values, rather than discrete values.\nSamples of evidence arrive continually, rather than at regular intervals.\n\n\n\n5.1.1 Evidence sampled from a normal distribution\nIn the random walk model, the magnitude of each sample of evidence was always equal to one. Each sample was either \\(+1\\) or \\(-1\\). In a diffusion model, evidence can take any real value, such that its magnitude is now important. Conceptually, this has some intuitive appeal. Some samples of evidence strongly favor one option, some samples only weakly support one option, and some are equivocal.\nIn a diffusion model, samples of evidence are specifically assumed to come from a normal distribution. The standard deviation of this distribution is typically fixed to some value like 0.1 or 1. Here, we will fix it to the value of 1. The reason for fixing this value is that “evidence” is abstract and therefore has no natural scale. We could multiply or divide all the evidence samples by a constant amount without changing their underlying meaning.\nThe mean of the evidence distribution represents how strongly the evidence tends to favor one option over the other, similar in meaning to the \\(p\\) parameter in the random walk model. The mean of the evidence distribution in a diffusion model is termed the drift rate, as it reflects the tendency for accumulated evidence to “drift” either upward or downward over time. As illustrated in the graph below, the mean of the evidence distribution governs the degree to which samples support one option versus the other.\n\n\nCode\nexpand_grid(v = c(-2, -1, 0, 1, 2), x = seq(-4, 4, length.out = 201)) %&gt;%\n    mutate(d = dnorm(x, mean = v, sd = 1)) %&gt;%\n    ggplot(aes(x = x, y = d, color = v, group = v)) +\n    geom_vline(xintercept = 0, linetype = \"dashed\") +\n    geom_line() +\n    scale_color_gradient2(mid = \"#444444\", midpoint = 0) +\n    labs(x = \"Value of evidence sample\", y = \"Relative frequency\", color = \"Mean of evidence\\ndistribution\")\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Evidence sampled continuously in time\nHere we come to a bit of a subtle issue: In the random walk, evidence arrived in discrete units at regular intervals, but the duration of the interval was not related to the magnitude of the evidence. In a diffusion model, we assume that evidence arrives continuously in time. One way to think about this—indeed, the way that we will simulate this—is that evidence is sampled in many very short intervals of time, each of which has duration \\(\\Delta t\\). When \\(\\Delta t\\) is small enough, those many little intervals will look like one continuous span of time. This principle is illustrated in the graph below.\n\n\nCode\ndiffusion_sim &lt;- expand_grid(dt = c(0.1, 0.01, 0.001)) %&gt;%\n    group_by(dt) %&gt;%\n    reframe(t = seq(0, 3, by = dt)) %&gt;%\n    ungroup() %&gt;%\n    mutate(x_sample = rnorm(n = n(), mean = 0, sd = 1 * sqrt(dt))) %&gt;%\n    group_by(dt) %&gt;%\n    mutate(x = cumsum(x_sample))\n\nscaled_plot &lt;- diffusion_sim %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    geom_step() +\n    facet_wrap(\"dt\", labeller = label_bquote(Delta * t == .(dt))) +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = expression(\"Evidence scaled by \" * Delta * t))\n\nunscaled_plot &lt;- diffusion_sim %&gt;%\n    ggplot(aes(x = t, y = x / sqrt(dt))) +\n    geom_step() +\n    facet_wrap(\"dt\", labeller = label_bquote(Delta * t == .(dt))) +\n    labs(x = \"Time\", y = \"Accumulated evidence\", title = \"Evidence not scaled\")\n\nscaled_plot / unscaled_plot\n\n\n\n\n\n\n\n\n\nThe top set of graphs above show how, when \\(\\Delta t\\) is sufficiently small, the trajectory of accumulated evidence looks essentially continuous—you can no longer see the “jumps” from one interval to the next.\nThe bottom set of graphs illustrate the subtlety I mentioned earlier. If we divide time into many small intervals but leave the mean and standard deviation of the evidence distribution the same, then we are essentially getting many more samples of evidence. As a result, accumulated evidence has a much larger scale than it would have if we had picked a smaller \\(\\Delta t\\). From a theoretical standpoint, this doesn’t make sense—the rate at which evidence accumulates for a decision should not be affected by the modeler’s arbitrary choice of \\(\\Delta t\\).\nSo what we do is scale the evidence samples by \\(\\Delta t\\). That’s what was done in the top set of graphs, but not the bottom set. The idea is that if you have very small time intervals, you shouldn’t be able to get as large of a sample of evidence. Again, this makes theoretical sense, if evidence is something that takes time to accumulate.\nSpecifically, a diffusion model assumes that each sample of evidence is drawn from a normal distribution with a mean of \\(v \\times \\Delta t\\), where \\(v\\) is the drift rate parameter, and a standard deviation of \\(\\sqrt{\\Delta t}\\). Why \\(\\sqrt{\\Delta t}\\) instead of just \\(\\Delta t\\)? Because it is the mean and variance that need to be scaled by \\(\\Delta t\\).\n\n\n5.1.3 A new simulation function\nLet’s take our rw_sim function from the previous chapter and turn it into a diffusion model. To do this, we make two modifications: First, we swap out the p parameter representing the probability of getting a positive sample for a parameter called v which is the drift rate. Second, instead of getting each evidence sample x_sample from a binomial distribution, we will get it from a normal distribution using R’s rnorm function. These changes are illustrated below.\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, b_upper = 1, b_lower = -1, t0 = 0.2, dt = 0.01, t_max = Inf) {\n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = v * dt, sd = sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nIn the code above, I also took the liberty of adjusting the default values of b_upper, b_lower, and dt so that the simulated choices and RT’s would look a bit more like those observed in cognitive tasks, but of course you may feel free to adjust those yourself as you like.\n\n\n5.1.4 Putting it all together—again\nAt the end of the last chapter, I included a chunk of code that simulated a random walk and produced some visualizations to help us understand both its internal states and its overt behavior (choices and RT). By swapping out rw_sim with the appropriately adjusted diffusion_sim line, we can apply the same chunk of code to the diffusion model! In the chunk below, I picked some arbitrary but reasonable values for the parameters.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, b_upper = 1, b_lower = -1)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\nYou may or may not be surprised to see that the RT distributions produced by the diffusion model closely resemble those produced by the random walk! The diffusion model also demonstrates an interesting feature of a random walk, namely, that the conditional RT distribution depends on the boundaries but not on the drift rate. In the example above, I set \\(v = 0.5\\), such that evidence would tend to favor the positive option. Even though the model ends up choosing that option more often, it does not do so any faster or slower than it chooses the negative option. This is something we will return to at the end of this chapter.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#response-caution-and-response-bias",
    "href": "diffusion_sim.html#response-caution-and-response-bias",
    "title": "5  From random walk to diffusion",
    "section": "5.2 Response caution and response bias",
    "text": "5.2 Response caution and response bias\nBefore confronting the issue of invariant RT distributions, it behooves us to consider a different way of specifying the response boundaries in our model. So far, we have specified those boundaries directly. We can speak of response caution in terms of how far those boundaries are from the starting point and response bias in terms of whether the boundaries are equidistant from the starting point.\nSpecifically, we could define a term \\(A\\) that is the total distance between the starting point (zero) and the two boundaries. If \\(B_{\\text{Upper}}\\) and \\(B_{\\text{Lower}}\\) are the upper and lower boundaries, respectively, then \\[\nA = B_{\\text{Upper}} - B_{\\text{Lower}}\n\\] In other words, \\(A\\) is how far apart the two boundaries are, called boundary separation. The term \\(A\\) can be seen to operationalize the construct of response caution in that a decision maker who wants to wait to accumulate evidence would put their response boundaries far apart.\nWe can also operationalize the construct of response bias by defining a term \\(w\\). This term will be a number between 0 and 1 that represents the degree to which response boundaries favor one choice over the other. Specifically, let \\[\nw = \\frac{-B_{\\text{Lower}}}{B_{\\text{Upper}} - B_{\\text{Lower}}}\n\\] As shown in the graph below, \\(w = 0.5\\) when the boundaries are equidistant from zero, \\(w &lt; 0.5\\) when the boundaries are biased in favor of the negative option, and \\(w &gt; 0.5\\) when the boundaries are biased in favor of the positive option.\n\n\nCode\nexpand_grid(b_upper = seq(1, 5), b_lower = seq(-1, -5)) %&gt;%\n    mutate(A = b_upper - b_lower) %&gt;%\n    mutate(w = -b_lower / A) %&gt;%\n    pivot_longer(c(A, w), names_to = \"par\", values_to = \"val\") %&gt;%\n    mutate(par = factor(par, levels = c(\"A\", \"w\"), labels = c(\"Response caution (A)\", \"Response bias (w)\"))) %&gt;%\n    ggplot(aes(x = b_upper, y = val, color = b_lower, group = b_lower)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(\"par\", scales = \"free_y\", strip.position = \"left\") +\n    labs(x = expression(B[\"Upper\"]), y = NULL, color = expression(B[\"Lower\"])) +\n    theme(strip.placement = \"outside\", strip.background = element_blank())\n\n\n\n\n\n\n\n\n\nHaving defined \\(A\\) and \\(w\\) as ways of operationalizing response caution and response bias, respectively, why not treat these values as parameters instead of the boundaries themselves? The value in doing so is that we can then specify these constructs directly, rather than having to work backwards from the boundaries. Specifically, if we pick values of \\(A\\) and \\(w\\) we can immediately compute what the upper and lower boundaries should be: \\[\\begin{align}\nB_{\\text{Upper}} & = w A \\\\\nB_{\\text{Lower}} & = -\\left(1 - w \\right) A \\\\\n\\end{align}\\]\nAnd we can adjust our diffusion_sim code accordingly to have a and w as parameters instead of b_upper and b_lower, which now get calculated in the function itself:\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf) {\n    b_upper &lt;- (1 - w) * a\n    b_lower &lt;- -w * a\n    \n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#trial-by-trial-variability",
    "href": "diffusion_sim.html#trial-by-trial-variability",
    "title": "5  From random walk to diffusion",
    "section": "5.3 Trial-by-trial variability",
    "text": "5.3 Trial-by-trial variability\nRecall that both the random walk and the diffusion model have the following property: The response times they produce depend on the distance between the starting point and the response boundary, not on the drift rate \\(v\\) or the step probability \\(p\\). To see why this might be problematic from a psychological perspective, imagine that the upper boundary corresponds to making a correct response while the lower boundary corresponds to making an error. Assume that \\(v &gt; 0\\), such that the evidence tends to favor making a correct response. The fact that response times do not depend on drift rates means that the model predicts that correct and error responses will be made in the same amount of time. To be more precise, the distribution of RT’s conditional on accuracy are the same.\nOften, errors are either faster or slower than correct responses. For example, it may be that errors occur more often when the decision maker happens to get poor evidence. In that case, we might expect errors to be slow because they result from the decision maker deliberating longer in the face of this poor evidence. On the other hand, maybe a decision maker tends to be correct when they take their time, but will sometimes “jump the gun” and pick the wrong option, in which case we would expect errors to be faster than correct responses.\nThe critical factor that Ratcliff (1978) introduced to the diffusion model that has made it into such a useful tool is that the drift rate is not the same on every trial, but varies randomly from trial to trial. On some trials, you happen to get a drift rate in the high positive tail of the distribution of drift rates, in which case you would probably make a fast correct response. On other trials, you happen to get a drift rate that is close to zero or even falls below zero by chance, in which case you would be more likely to make an error and would tend to do so more slowly. Thus, trial-by-trial variability in drift rates accounts for slow errors.\nWhat about fast errors? Ratcliff & Rouder (1998) showed that these can result if your response boundaries are not always fixed, but can also vary randomly from trial to trial. Sometimes, they happen to be very close to the starting point such that it takes very little evidence to commit to a response. Such rapid responses would be more likely to be errors, since they don’t give much time to accumulate evidence. Thus, trial-by-trial variability in boundaries (or, equivalently, in starting point) accounts for fast errors.\nThere is a final thing that can vary from trial to trial, and that is residual time. After all, if the time needed to accumulate evidence can vary between trials, so can the time needed to accomplish all the other processes involved in any given decision task. Trial-by-trial variability in residual time does not, of course, affect the probability of choosing either option, but it does affect the form of the RT distributions.\n\n5.3.1 Adding variability to our simulation code\nTo model each of these kinds of trial-by-trial variability, we need to decide how each of the values above (drift rate, boundaries, and residual time) can vary. This will also inform us as to what new parameters we will need to add to our model to specify that variability. In what follows, we will adopt common assumptions in the literature that are also implemented in the model-fitting functions we will use in later chapters. Check out the exercises (or explore on your own) to consider other forms of trial-by-trial variability!\n\n5.3.1.1 Trial-by-trial variability in drift rates\nOur model already has a parameter called \\(v\\) that stands for the “drift rate”. Let us instead treat \\(v\\) as the mean of a normal distribution of drift rates, which has standard deviation \\(s_v\\). If \\(s_v = 0\\), then we have our original diffusion model with the same drift rate on every trial. On the other hand, if \\(s_v &gt; 0\\), then the drift rate on any given trial will sometimes be greater or less than \\(v\\), even if the average drift rate across all trials is \\(v\\).\nTo implement this in our simulation code, we need to\n\nAdd a new parameter sv.\nAdd a line that randomly samples the drift rate (called trial_v) from a normal distribution with mean v and standard deviation sv.\nReplace v when drawing samples of evidence with trial_v.\n\nThese changes are reflected in the following adjusted code:\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf, sv = 0) {\n    trial_v &lt;- rnorm(n = 1, mean = v, sd = sv)\n    \n    b_upper &lt;- (1 - w) * a\n    b_lower &lt;- -w * a\n    \n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = trial_v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nThe chunk of code below has the same settings as that shown above, only now sv = 1. As you can see, responses on the lower boundary have a different RT distribution, which tends to be slower, than responses on the upper boundary. (Note, too, that I am using our revised code that uses a and w to define the response boundaries.)\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0.5)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\n\n\n5.3.1.2 Trial-by-trial variability in boundaries/starting point\nThere are a number of ways that we could introduce variability in the starting point and/or boundaries. To be consistent with the model-fitting we will do later, we will assume that the bias parameter \\(w\\) is not fixed, but is sampled from a uniform distribution that goes from \\(w - \\frac{s_w}{2}\\) to \\(w + \\frac{s_w}{2}\\). Thus, the average bias is still \\(w\\) but has a range defined by parameter \\(s_w\\). As above, we need to add this new parameter and randomly sample a trial_w value at the top of our code. Note that the line that samples trial_w does some checking using the min and max functions to make sure that \\(w\\) never falls below 0 or greater than 1.\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf, sv = 0, sw = 0) {\n    trial_v &lt;- rnorm(n = 1, mean = v, sd = sv)\n    trial_w &lt;- runif(n = 1, min = max(0, w - 0.5 * sw), max = min(1, w + 0.5 * sw))\n    \n    b_upper &lt;- (1 - trial_w) * a\n    b_lower &lt;- -trial_w * a\n    \n    x &lt;- 0\n    t &lt;- t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = trial_v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nThe simulations below set \\(s_v = 0\\) and \\(s_w = 0.9\\), while \\(v = 0.5\\). In the simulations below, when the model picks the “incorrect” option associated with the lower boundary, it is predicted to do so faster than when it responds by choosing the “correct” option associated with the upper boundary.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0, sw = 0.9)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\n\n\n5.3.1.3 Trial-by-trial variability in residual time\nOur final amendment to our diffusion simulation code involves adding variability to the residual time. Again, there are many ways we could do this, but we will adopt the same conventional approach used in our model-fitting later: We will assume that the residual time on any given trial is sampled from a uniform distribution that ranges from \\(t_0\\) to \\(t_0 + s_{t_0}\\). The code below adds the new st0 parameter and samples a residual time trial_t0 from a uniform distribution at the beginning of the simulation:\n\n\nCode\ndiffusion_sim &lt;- function(v = 0, a = 2, w = 0.5, t0 = 0.2, dt = 0.01, t_max = Inf, sv = 0, sw = 0, st0 = 0) {\n    trial_v &lt;- rnorm(n = 1, mean = v, sd = sv)\n    trial_w &lt;- runif(n = 1, min = max(0, w - 0.5 * sw), max = min(1, w + 0.5 * sw))\n    trial_t0 &lt;- runif(n = 1, min = t0, max = t0 + st0)\n    \n    b_upper &lt;- (1 - trial_w) * a\n    b_lower &lt;- -trial_w * a\n    \n    x &lt;- 0\n    t &lt;- trial_t0\n    \n    x_record &lt;- x\n    t_record &lt;- t\n    \n    while (x &lt; b_upper & x &gt; b_lower & t &lt; t_max) {\n        x_sample &lt;- rnorm(n = 1, mean = trial_v * dt, sd = 1 * sqrt(dt))\n        x &lt;- x + x_sample\n        t &lt;- t + dt\n        x_record &lt;- c(x_record, x)\n        t_record &lt;- c(t_record, t)\n    }\n    \n    return(data.frame(t = t_record, x = x_record))\n}\n\n\nThe simulations below again assume that \\(v = 0.5\\) and set \\(s_v = s_w = 0\\) while \\(s_t = 0.5\\). Note that the resulting RT distributions end up having a longer early tail, reflecting greater variability in the fastest RT’s due to variability in residual time.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0, sw = 0, st0 = 0.6)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n\n\n\n\n\n\n\n\nCode\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n\n\n\n\n\n\n\n\nCode\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")\n\n\nJoining with `by = join_by(choice)`\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Putting it all together (finally!)\nFinally, for completeness, let’s collect our code to run a set of simulations that allows for all three kinds of variability. This code is not executed here, but is included so it can serve as the basis for your own simulations and explorations.\n\n\nCode\n# Specify the number of simulations to run\nn_sims &lt;- 1000\n\n# This is initially empty, but will eventually save all our random walk simulations\nsim_results &lt;- c()\n\n# The for loop increments a counter (called \"i\" here) over a specified range (from 1 up to n_sims)\nfor (i in 1:n_sims) {\n    # Simulate a single realization of the random walk with the given parameters\n    current_result &lt;- diffusion_sim(v = 0.5, a = 2, w = 0.5, sv = 0.5, sw = 0.2, st0 = 0.4)\n    \n    # \"Bind\" the current simulation to the ongoing record of simulation results\n    sim_results &lt;- rbind(\n        sim_results,\n        # Add a new column that identifies which simulation this was\n        current_result %&gt;% mutate(sim_index = i)\n    )\n}\n\n# Visualize the internal states of the model\nsim_results %&gt;%\n    ggplot(aes(x = t, y = x)) +\n    stat_density2d_filled() +\n    labs(x = \"Time\", y = \"Accumulated evidence\", fill = \"Relative frequency\", title = \"Internal evidence states over time\")\n\n# Extract simulated choices and RT's\nchoice_rt &lt;- sim_results %&gt;%\n    group_by(sim_index) %&gt;%\n    summarize(\n        choice = factor(last(x) &gt; 0, levels = c(TRUE, FALSE), labels = c(\"upper\", \"lower\")),\n        rt = last(t)\n    )\n\n# Plot conditional RT distributions\nchoice_rt %&gt;%\n    ggplot(aes(x = rt, color = choice)) +\n    geom_density() +\n    labs(x = \"Response time\", y = \"Frequency\", color = \"Choice\", title = \"Conditional RT distributions\")\n\n# Quantile-probability plot\n\nsim_choice_p &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(p_resp = n / sum(n))\n\nsim_rt_q &lt;- choice_rt %&gt;%\n    group_by(choice) %&gt;%\n    reframe(rt_q = quantile(rt, probs = c(0.1, 0.3, 0.5, 0.7, 0.9)))\n\nfull_join(sim_choice_p, sim_rt_q) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = choice)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response proportion\", y = \"RT Quantile\", title = \"Quantile-Probability Plot\")",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#shiny-app",
    "href": "diffusion_sim.html#shiny-app",
    "title": "5  From random walk to diffusion",
    "section": "5.4 Shiny App",
    "text": "5.4 Shiny App\nTo have a good deal of fun exploring how the different parameters of a diffusion model influence its predicted choice and RT distributions, download this Shiny app and run it from RStudio. You will also need to download this R script into the same directory as the Shiny app. The Shiny app has some additional functionality that we will see more of in the next chapter.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_sim.html#exercises",
    "href": "diffusion_sim.html#exercises",
    "title": "5  From random walk to diffusion",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nUnder what circumstances do you think it is more likely for errors to be faster than correct responses? What about circumstances in which errors are more likely to be slower than correct responses? What do you think about the psychological implications of how the diffusion model produces either fast or slow errors?\nWrite a new diffusion simulation that uses a different distribution of drift rates from trial to trial—you might try distributions that are skewed (like an ExGaussian) or have heavy tails (like the T distribution with few degrees of freedom).\n\nDescribe the distribution you picked and whether it corresponds to a theory or hypothesis about how evidence may vary from trial-to-trial in a particular cognitive task.\nDescribe any differences you find between the model you wrote and the model in the chapter that assumes a normal distribution of drift rates across trials.\n\n\n\n\n\n\nRatcliff, R. (1978). A theory of memory retrieval. Psychological Review, 85(2), 59–108.\n\n\nRatcliff, R., & Rouder, J. N. (1998). Modeling response times for two-choice decisions. Psychological Science, 9(5), 347–356.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From random walk to diffusion</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html",
    "href": "diffusion_fit.html",
    "title": "6  Fitting a diffusion model to data",
    "section": "",
    "text": "6.1 Fitting a model\nFitting a model to data means finding the values of that model’s parameters that assign the highest likelihood to the observed data. Given a set of parameter values, we can use the model to compute how likely it would be to have seen each observation in our dataset.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#fitting-a-model",
    "href": "diffusion_fit.html#fitting-a-model",
    "title": "6  Fitting a diffusion model to data",
    "section": "",
    "text": "6.1.1 Parameters, likelihoods, and log-likelihoods\nUsing \\(\\theta\\) to stand for a set of parameter values and \\(x_i\\) to stand for a particular observation, we can use \\(f \\left( x \\mid \\theta \\right)\\) to stand for the likelihood of the datum \\(x_i\\) given the parameter values \\(\\theta\\) and a model with a likelihood function \\(f\\).\nConsider the familiar normal distribution. This distribution has two parameters, a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\). So for the normal distribution, \\(\\theta\\) is actually a vector with two elements: \\(\\theta = \\left[ \\mu, \\sigma \\right]\\). The normal distribution is a simple model in that it says that observed values fall in a particular shape around the mean \\(\\mu\\) with a spread described by \\(\\sigma\\). The likelihood function for the normal distribution, written below, indicates how likely it would be to observe datum \\(x_i\\) given specific values of \\(\\mu\\) and \\(\\sigma\\): \\[\nf_{\\text{Normal}} \\left( x_i \\mid \\mu, \\sigma \\right) = \\frac{1}{ \\sigma \\sqrt{2 \\pi}} \\exp \\left[ - \\frac{\\left(x_i - \\mu \\right)^2}{2 \\sigma^2} \\right]\n\\]\nThe graph below assumes that we have just a single observation, \\(x_i = 0\\). The plots show how the likelihood assigned to that datum depends on both parameters of the normal distribution, \\(\\mu\\) and \\(\\sigma\\).\n\n\nCode\nnorm_like_df &lt;- expand_grid(mu = seq(-3, 3, by = 0.1), sigma = seq(0, 3, by = 0.1), x = 0) %&gt;%\n    mutate(d = dnorm(x, mean = mu, sd = sigma))\n\nmu_plot &lt;- norm_like_df %&gt;%\n    filter(sigma == 1) %&gt;%\n    ggplot(aes(x = mu, y = d)) +\n    geom_line() +\n    labs(x = expression(mu), y = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, 1), \")\")), title = expression(\"Assuming \" * sigma == 1))\n\nsigma_plot &lt;- norm_like_df %&gt;%\n    filter(mu == 1) %&gt;%\n    ggplot(aes(x = sigma, y = d)) +\n    geom_line() +\n    labs(x = expression(sigma), y = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(1, sigma), \")\")), title = expression(\"Assuming \" * mu == 1))\n\njoint_plot &lt;- norm_like_df %&gt;%\n    ggplot(aes(x = mu, y = sigma, fill = d)) +\n    geom_raster(interpolate = TRUE) +\n    geom_vline(xintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    geom_hline(yintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    scale_fill_viridis_c() +\n    coord_equal() +\n    labs(x = expression(mu), y = expression(sigma), fill = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, sigma), \")\")), title = \"Joint likelihood\")\n\nmu_plot + sigma_plot + joint_plot + plot_layout(design = \"12\\n33\", guides = \"keep\", heights = c(1, 2))\n\n\n\n\n\nData: (0)\n\n\n\n\nWhen we have more than one observation, the likelihood of all observations is the product of the likelihoods of each individual observation: \\[\nf \\left( \\mathbf{x} \\mid \\theta \\right) = \\prod_{i = 1}^N f \\left( x_i \\mid \\theta \\right)\n\\] This situation is illustrated in the graphs below. These graphs again assume a normal distribution as the model, but now we have observed the values \\(\\mathbf{x} = \\left[ 0, 1 \\right]\\).\n\n\nCode\nnorm_like_df &lt;- expand_grid(mu = seq(-3, 3, by = 0.1), sigma = seq(0, 3, by = 0.1), x = c(0, 1)) %&gt;%\n    mutate(d = dnorm(x, mean = mu, sd = sigma)) %&gt;%\n    group_by(mu, sigma) %&gt;%\n    summarize(d = prod(d), .groups = \"keep\")\n\nmu_plot &lt;- norm_like_df %&gt;%\n    filter(sigma == 1) %&gt;%\n    ggplot(aes(x = mu, y = d)) +\n    geom_line() +\n    labs(x = expression(mu), y = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, 1), \")\")), title = expression(\"Assuming \" * sigma == 1))\n\nsigma_plot &lt;- norm_like_df %&gt;%\n    filter(mu == 1) %&gt;%\n    ggplot(aes(x = sigma, y = d)) +\n    geom_line() +\n    labs(x = expression(sigma), y = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(1, sigma), \")\")), title = expression(\"Assuming \" * mu == 1))\n\njoint_plot &lt;- norm_like_df %&gt;%\n    ggplot(aes(x = mu, y = sigma, fill = d)) +\n    geom_raster(interpolate = TRUE) +\n    geom_vline(xintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    geom_hline(yintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    scale_fill_viridis_c() +\n    coord_equal() +\n    labs(x = expression(mu), y = expression(sigma), fill = expression(f[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, sigma), \")\")), title = \"Joint likelihood\")\n\nmu_plot + sigma_plot + joint_plot + plot_layout(design = \"12\\n33\", guides = \"keep\", heights = c(1, 2))\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\nData: (0, 1)\n\n\n\n\n\n\n6.1.2 Log-likelihood\nYou may have noticed that the scale on the bottom graph in the second set of plots (where the data were \\(0, 1\\)) has a smaller maximum than it did in the bottom graph in the first set of plots (where the data were just \\(0\\)). This is because the likelihood of the whole dataset is a product of many small numbers, so the result also tends to be small. Unfortunately, this can lead to technical issues when we have many observations because the resulting likelihood may be too small for the computer to accurately calculate.\nAs a result, we often work with the natural logarithm of the likelihood function. Because multiplication turns into addition on the log-scale, this saves the computer from needing to work with very tiny numbers. We often write \\(LL\\) to stand for this “log-likelihood”: \\[\nLL \\left( \\mathbf{x} \\mid \\theta \\right) = \\log \\left[ \\prod_{i = 1}^N f \\left( x_i \\mid \\theta \\right) \\right] = \\sum_{i = 1}^N \\log f \\left( x_i \\mid \\theta \\right)\n\\]\nFortunately, since the logarithm is a monotonic transformation, the parameters that maximize the likelihood are the very same parameters that maximize the log-likelihood. To get a sense of this, the graphs below replicate the graph above with observed values \\(\\mathbf{x} = \\left[ 0, 1 \\right]\\), but showing the log-likelihood instead of the likelihood.\n\n\nCode\nnorm_like_df &lt;- expand_grid(mu = seq(-3, 3, by = 0.1), sigma = seq(0, 3, by = 0.1), x = c(0, 1)) %&gt;%\n    mutate(ll = dnorm(x, mean = mu, sd = sigma, log = TRUE)) %&gt;%\n    group_by(mu, sigma) %&gt;%\n    summarize(ll = sum(ll), .groups = \"keep\")\n\nmu_plot &lt;- norm_like_df %&gt;%\n    filter(sigma == 1) %&gt;%\n    ggplot(aes(x = mu, y = ll)) +\n    geom_line() +\n    labs(x = expression(mu), y = expression(LL[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, 1), \")\")), title = expression(\"Assuming \" * sigma == 1))\n\nsigma_plot &lt;- norm_like_df %&gt;%\n    filter(mu == 1) %&gt;%\n    ggplot(aes(x = sigma, y = ll)) +\n    geom_line() +\n    labs(x = expression(sigma), y = expression(LL[Normal] * bgroup(\"(\", x[i] * \"|\" * list(1, sigma), \")\")), title = expression(\"Assuming \" * mu == 1))\n\njoint_plot &lt;- norm_like_df %&gt;%\n    ggplot(aes(x = mu, y = sigma, fill = ll)) +\n    geom_raster(interpolate = TRUE) +\n    geom_vline(xintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    geom_hline(yintercept = 1, linetype = \"dashed\", color = \"lightgrey\") +\n    scale_fill_viridis_c() +\n    coord_equal() +\n    labs(x = expression(mu), y = expression(sigma), fill = expression(LL[Normal] * bgroup(\"(\", x[i] * \"|\" * list(mu, sigma), \")\")), title = \"Total log-likelihood\")\n\nmu_plot + sigma_plot + joint_plot + plot_layout(design = \"12\\n33\", guides = \"keep\", heights = c(1, 2))\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\nData: (0, 1)\n\n\n\n\n\n\n6.1.3 Maximizing log-likelihood\nThe combination of parameter values that assigns the highest total log-likelihood to all the data is where the bright spot is in the bottom graph of each set of plots above. For a normal model, the values of \\(\\mu\\) and \\(\\sigma\\) that assign the highest total log-likelihood to the data can be computed directly: they are the sample mean and the population standard deviation computed on the same (i.e., where you divide by \\(N\\) instead of \\(N - 1\\)).\nFor a more complex model, like our diffusion model, we will not be able to calculate these “optimal” parameter values directly. Instead, we will need to use the computer to search for these values. The topic of parameter optimization is a large one that we cannot fully address here. However, the functions supplied with this tutorial make use of two search methods: the Nelder-Mead simplex algorithm and the ucminf function from the ucminf R package.\nThe essence of these parameter optimization algorithms is this: They begin with an initial “guess” of the parameter values and compute the log-likelihood of the data given that initial guess. They then “explore” by calculating the log-likelihood of the data using slightly adjusted parameter values. If this exploration finds a set of adjusted parameter values that assign a higher log-likelihood to the data than the original guess, then these adjusted values are considered a “better guess”. The search process then begins again starting from that better guess. As a result, the “guess” gets gradually better on each step of the algorithm until it can no longer find any way to adjust the parameters to find a higher log-likelihood.\nOne can think of this parameter search as like trying to find a mountain peak in a snowstorm: The snowstorm obscures your visibility so you can only search areas in your immediate vicinity (similar to how the parameter optimizer tries to adjust values of the parameters starting from an initial guess). However, you can still figure out which direction is “uphill” from your current position (similar to how the algorithm finds adjusted parameters that yield a higher log-likelihood than the current best guess). Eventually, you will reach a point where you can’t go uphill any more.\nThe metaphor in the previous paragraph serves to highlight the fact that parameter optimizers are not necessarily guaranteed to find the “global optimum”—the best possible set of parameter values. It is possible that they instead find a “local optimum”—a set of values for which no small adjustment yields an improvement, but which is not the best you could possibly do. To return to the metaphor, the tallest peak may be a mile away from the one you found, but you’ll never know that because you can’t see it.\nThe utility functions we will use in this tutorial are generally pretty good at finding global optima, but it is not guaranteed! This is why it is important to develop some intuitions about how model parameters manifest in behavior, so you can detect when you might have reached a local optimum instead of a global one.\n\n\n6.1.4 Minimizing negative log-likelihood\nDue to the vagaries of history, algorithms that search for optimal parameter values are often written not to maximize something, but to minimize it instead. As a result, what a parameter optimization algorithm actually does is minimize the negative log-likelihood. Basically, it flips the “maximization” problem over, although it does not change anything conceptually. However, it does mean that a more appropriate metaphor is finding the lowest point in a valley rather than the highest peak of a mountain range.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#fitting-a-diffusion-model",
    "href": "diffusion_fit.html#fitting-a-diffusion-model",
    "title": "6  Fitting a diffusion model to data",
    "section": "6.2 Fitting a diffusion model",
    "text": "6.2 Fitting a diffusion model\nTo find the set of parameters for a diffusion model that assign the highest likelihood to the data, we need to be able to compute the likelihood of making a specific choice at a specific time, given a set of diffusion model parameters. This computation is not something we can do by hand—we have to rely on the computer. Fortunately, many folks have contributed to the development of efficient means of doing this computation (e.g., Blurton et al., 2012; Gondan et al., 2014; Hartmann & Klauer, 2021; Navarro & Fuss, 2009; Tuerlincx, 2004).\nWe will make use specifically of an R package that implements these methods called WienR (Hartmann & Klauer, 2023). The name comes from the fact that Norbert Wiener was associated with the development of the “bare-bones” diffusion model without any of the trial-by-trial variability introduced by Ratcliff (1978). That said, the WienR package allows for all of those additional forms of variability as described in the previous chapter.\nBe sure you have the WienR package installed and loaded!\n\n\nCode\nlibrary(WienR)\n\n\n\n6.2.1 Diffusion model likelihood\nThe WienR package provides a function called WienerPDF which returns both the likelihood and the log-likelihood of a set of responses, given a set of diffusion model parameters.\nUnlike with the normal distribution model, where an observation was characterized by a single number, an observation for a diffusion model is characterized by a choice and a response time. Thus, if we want to compute the likelihood of responding at the upper boundary in 1 second given a drift rate of \\(v = 0.5\\), response caution of \\(a = 2\\), response bias of \\(w = 0.5\\), and residual time \\(t_0 = 0.2\\), we use\n\n\nCode\nWienerPDF(t = 1, response = \"upper\", a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\n\n\nFirst-passage time PDF\n\n[1] 0.4362052\n\n---------------------------\n\nLog of first-passage time PDF\n\n[1] -0.8296426\n\n---------------------------\n\n\nThe “First-passage time PDF” is the likelihood of having made that response at that time.\nLet’s imagine that we observed a few more trials, one in which a decision-maker responded at the upper boundary in 1.5 seconds and one in which the decision-maker responded at the lower boundary in 2 seconds. We can compute the likelihood and log-likelihood of all of those trials:\n\n\nCode\nWienerPDF(t = c(1, 1.5, 2), response = c(\"upper\", \"upper\", \"lower\"), a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\n\n\nFirst-passage time PDF\n\n[1] 0.43620517 0.22137809 0.04128626\n\n---------------------------\n\nLog of first-passage time PDF\n\n[1] -0.8296426 -1.5078832 -3.1872255\n\n---------------------------\n\n\nNote two things: First, t and response are allowed to be vectors, where the \\(i\\)th entry in the t vector corresponds to the \\(i\\)th entry in the response vector. Second, we get the likelihood and log-likelihood for each trial individually. So if we want the total log-likelihood, we have to do that ourselves:\n\n\nCode\nresult &lt;- WienerPDF(t = c(1, 1.5, 2), response = c(\"upper\", \"upper\", \"lower\"), a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\n(total_log_likelihood &lt;- sum(result$logvalue))\n\n\n[1] -5.524751\n\n\nThe upshot of this is that WienerPDF gives us a way to evaluate the log-likelihood of a set of choices and response times given a set of values for the diffusion model’s parameters. WienerPDF also allows us to include variability in drift rates via the sv parameter, variability in boundaries via the sw parameter, and variability in residual time via the st0 parameter, like so:\n\n\nCode\nWienerPDF(t = c(1, 1.5, 2), response = c(\"upper\", \"upper\", \"lower\"), a = 2, v = 0.5, w = 0.5, t0 = 0.2, sv = 0.5, sw = 0.1, st0 = 0.4)\n\n\n\nFirst-passage time PDF\n\n[1] 0.56113640 0.26562047 0.06165999\n\n---------------------------\n\nLog of first-passage time PDF\n\n[1] -0.5777913 -1.3256868 -2.7861200\n\n---------------------------\n\n\nNotice that the likelihoods and log-likelihoods changed when we included those three new parameters (which, by default, are set to zero).\n\n\n6.2.2 Finding the best-fitting parameters\nTo find the diffusion model parameters that assign the highest log-likelihood to a given set of choices and RT’s, I have provided a helpful function called fit_wienr. To use this function, download the wienr_fit_utils.r R script to your working directory and run\n\n\nCode\nsource(\"wienr_fit_utils.r\")\n\n\nWe shall see later that the fit_wienr function has some useful bells-and-whistles, but for now let’s just see how it works in its basic form. To do this, let’s use another function from the WienR package called sampWiener which simulates a sample of data from a diffusion model, just like we did in the last chapter (actually, you could even use the diffusion_sim function you built for that purpose!):\n\n\nCode\n(sim_data &lt;- sampWiener(N = 50, a = 2, v = 0.5, w = 0.5, t0 = 0.2))\n\n\n$q\n [1] 1.0145267 0.7785001 0.4830922 0.7476763 0.6889558 0.3996333 0.7652827\n [8] 1.0678441 1.0589824 1.1778599 1.1916915 2.0070563 0.4737943 0.5687575\n[15] 0.9735880 0.7182124 0.7907633 1.5717658 0.6528604 0.8988441 0.6722051\n[22] 0.3394198 0.8275921 2.0176933 0.6527308 1.3267482 1.0114098 0.7283250\n[29] 1.6299191 1.8353343 1.2566600 1.2643379 2.0130723 1.5419533 1.0620593\n[36] 0.5713716 2.5825087 0.7561459 1.1029754 0.6381128 0.5454809 0.6359990\n[43] 3.0614166 1.6769220 0.8270179 2.9357647 1.0710890 0.3431315 0.4737332\n[50] 3.6870701\n\n$response\n [1] \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\"\n[10] \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\"\n[19] \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\"\n[28] \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\" \"upper\"\n[37] \"upper\" \"upper\" \"upper\" \"lower\" \"lower\" \"lower\" \"lower\" \"lower\" \"lower\"\n[46] \"lower\" \"lower\" \"lower\" \"lower\" \"lower\"\n\n$call\nsampWiener(N = 50, a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\nattr(,\"class\")\n[1] \"Diffusion_samp\"\n\n\nThe simulated choices are in sim_data$response and the simulated RT’s are in sim_data$q.\nBecause we are using the diffusion model to simulate data, it’s worth asking how likely the data are given the parameter values we actually used for the simulation:\n\n\nCode\noriginal_likelihood &lt;- WienerPDF(t = sim_data$q, response = sim_data$response, a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\nsum(original_likelihood$logvalue)\n\n\n[1] -66.06148\n\n\nRemember, though, that the optimization algorithm will actually be minimizing the negative log-likelihood. The negative log-likelihood given the original values of the parameters is\n\n\nCode\n-sum(original_likelihood$logvalue)\n\n\n[1] 66.06148\n\n\nIt may be that the “best-fitting” parameters differ from those used to simulate the data, just due to sampling variability. Thus, what we are about to do is a form of parameter recovery, in that we are seeing how well we can “recover” the original model parameters, despite this sampling variability.\nThe following code uses the fit_wienr function to find the best-fitting diffusion model parameter values for the simulated data above:\n\n\nCode\n(fit &lt;- fit_wienr(rt = sim_data$q, response = sim_data$response))\n\n\n$par\n     a[1]      v[1]      w[1]     t0[1] \n2.0672467 0.7208941 0.4525921 0.2034778 \n\n$value\n[1] 65.09442\n\n$convergence\n[1] 1\n\n$message\n[1] \"Stopped by small gradient (grtol).\"\n\n$invhessian.lt\n [1]  0.0233247552  0.0059470005 -0.0003678730 -0.0039030749  0.0343417878\n [6] -0.0053634481 -0.0010979615  0.0025483504  0.0004887555  0.0017438879\n\n$info\n maxgradient     laststep      stepmax        neval \n3.672341e-08 2.012289e-07 1.500625e-02 1.700000e+01 \n\nattr(,\"class\")\n[1] \"ucminf\"\n\n\nThe best negative log-likelihood that the algorithm was able to find is fit$value—as we can see it is slightly lower than the negative log-likelihood associated with the original simulating parameters, suggesting that the best-fitting parameters really do a slightly better job of “fitting” the data. Those parameter estimates reside in fit$par, where we can compare them against the values used to actually simulate the data.\n\n\n6.2.3 Visualizing model fit\nMinimizing the negative log-likelihood is well and good, but it doesn’t tell us much about how well the model actually “fits” the data. Does it predict similar choice and RT patterns?\nTo address this, it is helpful to visually inspect the fit of the model. There are many ways to do this, but here we will make use of the quantile-probability plots we introduced in previous chapters. We will overlay the quantile-probability plots of the original data with those that would be predicted by the best-fitting diffusion model parameters. To the extent that the model and data are close to one another, we can feel confident that the model is accurately reproducing the major features of the data.\nThe qp_fit function in the wienr_fit_utils.r script calculates the response probabilities and RT quantiles for both the observed data and the fitted model, like so:\n\n\nCode\n(obs_fit_data &lt;- qp_fit(rt = sim_data$q, response = sim_data$response, par = fit$par))\n\n\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response)`\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response, p_resp, rt_p, rt_q, source)`\n\n\n# A tibble: 20 × 12\n# Groups:   drift_index, bound_index, resid_index, sv_index, sw_index,\n#   st0_index [1]\n   drift_index bound_index resid_index sv_index sw_index st0_index response\n         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   \n 1           1           1           1        1        1         1 lower   \n 2           1           1           1        1        1         1 lower   \n 3           1           1           1        1        1         1 lower   \n 4           1           1           1        1        1         1 lower   \n 5           1           1           1        1        1         1 lower   \n 6           1           1           1        1        1         1 upper   \n 7           1           1           1        1        1         1 upper   \n 8           1           1           1        1        1         1 upper   \n 9           1           1           1        1        1         1 upper   \n10           1           1           1        1        1         1 upper   \n11           1           1           1        1        1         1 upper   \n12           1           1           1        1        1         1 upper   \n13           1           1           1        1        1         1 upper   \n14           1           1           1        1        1         1 upper   \n15           1           1           1        1        1         1 upper   \n16           1           1           1        1        1         1 lower   \n17           1           1           1        1        1         1 lower   \n18           1           1           1        1        1         1 lower   \n19           1           1           1        1        1         1 lower   \n20           1           1           1        1        1         1 lower   \n# ℹ 5 more variables: n_resp &lt;int&gt;, p_resp &lt;dbl&gt;, rt_p &lt;dbl&gt;, rt_q &lt;dbl&gt;,\n#   source &lt;chr&gt;\n\n\nWe can then use the result to overlay the quantile-probabilities from the fitted model over those computed from the observed data:\n\n\nCode\nobs_fit_data %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = source, shape = response)) +\n    geom_line(aes(group = interaction(rt_p, source), linetype = source), alpha = 0.5) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response probability\", y = \"RT Quantile (s)\", color = NULL, linetype = NULL, shape = \"Response\", title = \"50 simulated trials\")\n\n\n\n\n\n\n\n\n\nThis visualization highlights a few things: While the diffusion model fits the response probabilities and central tendency of the RT’s quite well, it doesn’t do as good a job with the error RT’s nor with the tails of the RT distributions. This is not because of a qualitative difference between the model and data—after all, we used the diffusion model to simulate these data! Rather, this apparent misfit is due to sampling error: With a small sample, it is harder to estimate RT’s for rare responses (like errors) and it is harder to estimate the tails of the RT distributions (since, by definition, we have fewer observations in the tails).\nSo let’s see what happens if we simulate 1000 trials instead of just 50.\n\n\nCode\nsim_data_large &lt;- sampWiener(N = 1000, a = 2, v = 0.5, w = 0.5, t0 = 0.2)\n\n(fit_large &lt;- fit_wienr(rt = sim_data_large$q, response = sim_data_large$response))\n\n\n$par\n     a[1]      v[1]      w[1]     t0[1] \n2.0145409 0.4891320 0.4895752 0.1936399 \n\n$value\n[1] 1394.557\n\n$convergence\n[1] 4\n\n$message\n[1] \"Stopped by zero step from line search\"\n\n$invhessian.lt\n [1] 1 0 0 0 1 0 0 1 0 1\n\n$info\nmaxgradient    laststep     stepmax       neval \n  0.3267429   0.0000000   1.0000000   1.0000000 \n\nattr(,\"class\")\n[1] \"ucminf\"\n\n\nCode\nobs_fit_data_large &lt;- qp_fit(rt = sim_data_large$q, response = sim_data_large$response, par = fit_large$par)\n\n\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response)`\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response, p_resp, rt_p, rt_q, source)`\n\n\nCode\nobs_fit_data_large %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = source, shape = response)) +\n    geom_line(aes(group = interaction(rt_p, source), linetype = source), alpha = 0.5) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response probability\", y = \"RT Quantile (s)\", color = NULL, linetype = NULL, shape = \"Response\", title = \"1000 simulated trials\")\n\n\n\n\n\n\n\n\n\nMuch better! And notice that the best-fitting parameter values (fit_large$par, above) are quite close to those used to simulate the data.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#fitting-multiple-conditions",
    "href": "diffusion_fit.html#fitting-multiple-conditions",
    "title": "6  Fitting a diffusion model to data",
    "section": "6.3 Fitting multiple conditions",
    "text": "6.3 Fitting multiple conditions\nImagine that we are modeling someone doing a recognition memory task. In this task, participants study a set of items like words or images. Later, during a test phase participants are shown many items and, for each one, they have to decide whether it had been on the study list or not. Therefore, on any given test trial, the item shown could have been studied—called a target—or not studied—called a foil. The participant needs to accumulate evidence from their memory in order to decide whether the item had or had not been studied. It seems reasonable to assume that, if the item is a target, the drift rate for that evidence would tend to be positive, i.e., supporting the choice that the item had been studied. If the item is a foil, the drift rate would tend to be negative, since the samples of evidence from memory would support the choice that the item wasn’t studied. Modeling this task would therefore require estimating two drift rate parameters, one for trials in which a target is shown and one for trials in which a foil is shown. However, because the participant cannot know which type of item they were shown, their response boundaries and residual time should be the same regardless of whether the trial shows a target or a foil.\nThe example above is just one case in which we need to model a decision task by assuming that some parameters differ between conditions (like between targets and foils) while others stay the same (like the response boundaries and residual time). For example, we could simulate the situation above by assuming that \\(v_1 = 0.5\\) is the drift rate for targets while \\(v_2 = -0.5\\) is the drift rate for foils, while keeping \\(a = 2\\), \\(w = 0.5\\), and \\(t_0 = 0.2\\) constant:\n\n\nCode\ntarget_trials &lt;- sampWiener(N = 80, a = 2, v = 0.5, w = 0.5, t0 = 0.2)\nfoil_trials &lt;- sampWiener(N = 80, a = 2, v = -0.5, w = 0.5, t0 = 0.2)\n\n(all_trials &lt;- tibble(\n    rt = c(target_trials$q, foil_trials$q),\n    response = c(target_trials$response, foil_trials$response),\n    item = factor(rep(c(\"Target\", \"Foil\"), each = 80), levels = c(\"Target\", \"Foil\"))\n))\n\n\n# A tibble: 160 × 3\n      rt response item  \n   &lt;dbl&gt; &lt;chr&gt;    &lt;fct&gt; \n 1 1.77  upper    Target\n 2 1.95  upper    Target\n 3 0.486 upper    Target\n 4 0.396 upper    Target\n 5 0.543 upper    Target\n 6 1.40  upper    Target\n 7 0.748 upper    Target\n 8 0.721 upper    Target\n 9 0.938 upper    Target\n10 1.99  upper    Target\n# ℹ 150 more rows\n\n\nWe can use the fit_wienr function to fit these data by supplying it with a drift_index vector. This is a vector of positive integers (1, 2, 3, …) that indicate which drift rate to use when computing the log-likelihood of a particular trial. In this case, since we want drift rate to depend on the type of item shown, we can use the item column of our simulated data to define the drift index:\n\n\nCode\nas.numeric(all_trials$item)\n\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[149] 2 2 2 2 2 2 2 2 2 2 2 2\n\n\nHere is how we supply that to fit_wienr:\n\n\nCode\n(recog_fit &lt;- fit_wienr(rt = all_trials$rt, response = all_trials$response, drift_index = as.numeric(all_trials$item)))\n\n\n$par\n      a[1]       v[1]       v[2]       w[1]      t0[1] \n 2.0345129  0.3167632 -0.7274032  0.5145780  0.2058107 \n\n$value\n[1] 219.4175\n\n$convergence\n[1] 4\n\n$message\n[1] \"Stopped by zero step from line search\"\n\n$invhessian.lt\n [1] 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1\n\n$info\nmaxgradient    laststep     stepmax       neval \n 0.04339481  0.00000000  1.00000000  1.00000000 \n\nattr(,\"class\")\n[1] \"ucminf\"\n\n\nAs we can see, the estimated drift rates v[1] and v[2] go in the direction we would expect, given that drift_index = 1 indicates a target (positive drift) and drift_index = 2 indicates a foil (negative drift).\nAnd how well does the best-fitting model do? Again, we can supply drift_index to the qp_fit function and make a set of quantile-probability plots.\n\n\nCode\nrecog_obs_fit_data &lt;- qp_fit(rt = all_trials$rt, response = all_trials$response, par = recog_fit$par, drift_index = as.numeric(all_trials$item))\n\n\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response)`\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response, p_resp, rt_p, rt_q, source)`\n\n\nCode\nrecog_obs_fit_data %&gt;%\n    mutate(drift_index_label = factor(drift_index, levels = 1:2, labels = c(\"Target\", \"Foil\"))) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = source, shape = response)) +\n    geom_line(aes(group = interaction(rt_p, source), linetype = source), alpha = 0.5) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    facet_wrap(\"drift_index_label\") +\n    labs(x = \"Response probability\", y = \"RT Quantile (s)\", color = NULL, linetype = NULL, shape = \"Response\")\n\n\n\n\n\n\n\n\n\nNot too bad, though as we saw earlier, the fit isn’t as good for errors or for the tails of the RT distributions.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#comparing-model-fits",
    "href": "diffusion_fit.html#comparing-model-fits",
    "title": "6  Fitting a diffusion model to data",
    "section": "6.4 Comparing model fits",
    "text": "6.4 Comparing model fits\nContinuing with the recognition memory example, imagine that we wanted to address the question: Can this participant distinguish between studied and unstudied items? That question can be reframed in terms of diffusion model parameters: is the drift rate the same or different between target and foil trials?\nTo address that question, we can fit one version of a diffusion model that uses the drift_index vector to estimate separate drift rates for targets and foils, like we just did. We can then fit another version that does not include drift_index, so that it estimates a single drift rate for all trials. The second model would correspond to the hypothesis that the participant cannot distinguish between targets and foils, because they get the same quality of evidence from their memory either way. By comparing how well those two models account for the data, taking into account the fact that the one-drift-rate model is less complex, then we can get evidence to help us address our question.\nLater in the course, we will delve more deeply into the issue of model comparison. For now, we will just introduce the basic idea of “scoring” a model based on (a) how well it fits data; and (b) how complex the model is. As noted below, a more complex model will, in general, be able to fit a wider variety of data. Thus, we should only favor a more complex model to the extent that it fits data better than would be expected based on its complexity. As we will discuss later in the course, there are other reasons why we should be reticent to favor more complex models, but these reasons can sometimes be hard to quantify. Thus, for now, we introduce some simple quantitative methods by which models can be compared while still accounting for their complexity.\n\n6.4.1 Information criteria\nIn the previous section, we already fit the two-drift-rate model. Now let’s fit the one-drift-rate version:\n\n\nCode\n(recog_fit_onedrift &lt;- fit_wienr(rt = all_trials$rt, response = all_trials$response))\n\n\n$par\n      a[1]       v[1]       w[1]      t0[1] \n 1.9376960 -0.1943884  0.5129813  0.2166252 \n\n$value\n[1] 239.2321\n\n$convergence\n[1] 4\n\n$message\n[1] \"Stopped by zero step from line search\"\n\n$invhessian.lt\n [1] 1 0 0 0 1 0 0 1 0 1\n\n$info\nmaxgradient    laststep     stepmax       neval \n 0.06124639  0.00000000  1.00000000  1.00000000 \n\nattr(,\"class\")\n[1] \"ucminf\"\n\n\nTo compare the fit of the two models, we have a number of techniques at our disposal, but one of the most common is to use an information criterion. These values essentially “score” a model, with higher scores being worse (like golf). The score takes into account how well the model fits the data as well as how complex the model is. Taking into account complexity is critical because, generally speaking, a more complex model will be able to fit data better, so it needs to be “handicapped” to account for this advantage (again, sort of like golf). The information criteria we will use here measure a model’s complexity in terms of the number of free parameters it has.\n\n6.4.1.1 Akaike Information Criterion (AIC)\nThe Akaike Information Criterion (AIC, Akaike, 1974) is defined: \\[\nAIC = 2 \\times NLL + 2 \\times k\n\\] where \\(NLL\\) is the negative log-likelihood of the fitted model (i.e., the quantity that is minimized during model fitting) and \\(k\\) is the number of free parameters in the model.\nWhen we use fit_wienr, the negative log-likelihood is the $value entry in the result. Meanwhile, we can get the number of free parameters as the length of the vector of best-fitting parameter estimates, as illustrated below:\n\n\nCode\naic_twodrift &lt;- 2 * recog_fit$value + 2 * length(recog_fit$par)\naic_onedrift &lt;- 2 * recog_fit_onedrift$value + 2 * length(recog_fit_onedrift$par)\n\nc(\"Two drift\" = aic_twodrift, \"One drift\" = aic_onedrift)\n\n\nTwo drift One drift \n 448.8351  486.4642 \n\n\nRecall that lower scores are better. Since the two-drift model has the lower AIC, we have evidence that this (simulated) participant is able to distinguish between targets and foils.\n\n\n6.4.1.2 Bayesian Information Criterion (BIC)\nThe Bayesian Information Criterion (BIC, Schwarz (1978)) is similar to the AIC but places a stronger penalty on the number of free parameters. Specifically, the penalty scales up with the logarithm of the number of observations. Letting \\(N\\) stand for the number of observed trials, the BIC is defined \\[\nBIC = 2 \\times NLL + k \\log N\n\\] where, again, \\(NLL\\) is the negative log-likelihood and \\(k\\) is the number of free parameters. We can now calculate BIC for each model:\n\n\nCode\nbic_twodrift &lt;- 2 * recog_fit$value + log(nrow(all_trials)) * length(recog_fit$par)\nbic_onedrift &lt;- 2 * recog_fit_onedrift$value + log(nrow(all_trials)) * length(recog_fit_onedrift$par)\n\nc(\"Two drift\" = bic_twodrift, \"One drift\" = bic_onedrift)\n\n\nTwo drift One drift \n 464.2109  498.7649 \n\n\nOnce again, the two-drift model gets the lower—and therefore better—score.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#shiny-app",
    "href": "diffusion_fit.html#shiny-app",
    "title": "6  Fitting a diffusion model to data",
    "section": "6.5 Shiny App",
    "text": "6.5 Shiny App\nThe “Manual parameter fitting” tab of the Shiny app from the previous chapter allows you to try searching for optimal parameters yourself. You will find how difficult it is to do by hand! There is also a “Parameter recovery” tab that replicates what we did in this chapter, namely, using the diffusion model to simulate data and then fitting the diffusion model to the simulated data. Finally, note that the App allows you to play around with other parameters, like the trial-by-trial variability parameters, that we did not use in this chapter.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "diffusion_fit.html#exercises",
    "href": "diffusion_fit.html#exercises",
    "title": "6  Fitting a diffusion model to data",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\n\nConsider a dataset with one particularly short RT. Would you want to allow for variability in residual time when fitting these data? Why or why not?\nUsing the Shiny App, see if you can “fool” the model! Specifically, try simulating data until there is a notable discrepancy between the best-fitting parameter values and the values you used to simulate the data. What features of the simulated data may have resulted in this discrepancy?\nWe saw an example in the chapter of how drift rates might vary between conditions of an experiment even while the other parameters of the model would stay the same.\n\nWhat other examples can you think of where the properties of the evidence change but other model parameters do not?\nGive an example of a situation in which you would expect response boundaries to differ between conditions, but not drift rates.\nGive an example of a situation in which you would expect residual time to differ between conditions, but not drift rates or boundaries.\n\nRepeat our recognition memory simulations and model comparisons, but instead of simulating data where the drift rate actually differs between targets and foils, simulate data in which targets and foils have the same drift rate. Use both AIC and BIC to compare the fits of the two-drift and one-drift models. Do these information criteria favor the “correct” model?\n\n\n\n\n\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6), 716–723.\n\n\nBlurton, S. P., Kesselmeier, M., & Gondan, M. (2012). Fast and accurate calculations for cumulative first-passage time distributions in wiener diffusion models. Journal of Mathematical Psychology, 56(6), 470–475. https://doi.org/https://doi.org/10.1016/j.jmp.2012.09.002\n\n\nGondan, M., Blurton, S. P., & Kesselmeier, M. (2014). Even faster and even more accurate first-passage time densities and distributions for the wiener diffusion model. Journal of Mathematical Psychology, 60, 20–22. https://doi.org/https://doi.org/10.1016/j.jmp.2014.05.002\n\n\nHartmann, R., & Klauer, K. C. (2021). Partial derivatives for the first-passage time distribution in wiener diffusion models. Journal of Mathematical Psychology, 103, 102550. https://doi.org/https://doi.org/10.1016/j.jmp.2021.102550\n\n\nHartmann, R., & Klauer, K. C. (2023). WienR: Derivatives of the first-passage time density and cumulative distribution function, and random sampling from the (truncated) first-passage time distribution. https://CRAN.R-project.org/package=WienR\n\n\nNavarro, D. J., & Fuss, I. G. (2009). Fast and accurate calculations for first-passage times in wiener diffusion models. Journal of Mathematical Psychology, 53(4), 222–230. https://doi.org/https://doi.org/10.1016/j.jmp.2009.02.003\n\n\nRatcliff, R. (1978). A theory of memory retrieval. Psychological Review, 85(2), 59–108.\n\n\nSchwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6(2), 461–464.\n\n\nTuerlincx, F. (2004). The efficient computation of the cumulative distribution and probability density functions in the diffusion model. Behavior Research Methods, Instruments, & Computers, 36(4), 702–716.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitting a diffusion model to data</span>"
    ]
  },
  {
    "objectID": "model_comparison.html",
    "href": "model_comparison.html",
    "title": "7  Model complexity and model comparison",
    "section": "",
    "text": "7.1 Complexity and generalization\nIn a way, the issue with model complexity boils down to the same issue we have in statistics when using a sample to estimate some population quantity. In statistics, where we use models on the “descriptive” end of the modeling continuum, our goal is to identify patterns in our sample of data that we would expect to see in other samples from some broader population. In that way, we generalize whatever conclusions we draw about our sample to this broader population. We have essentially the same goal when applying a cognitive model, even though it falls on the “causal” end of the modeling continuum: By fitting the model to a sample of data, we are hoping to draw some inferences about how a sample of participants accomplished a particular task. We hope that those inferences apply more broadly, i.e., that we can make a general statement about how some broader population accomplishes that task.\nThe challenge we face in both statistics and cognitive modeling is that we know that not every sample from the same population is identical. This sampling variability has two consequences: First, it is possible that our observed data sample is biased, in the sense that it has some idiosyncratic property that is not representative of what we would expect to see in the population more broadly. In that case, what we conclude about our sample may not generalize to the broader population. Second, even if our sample were unbiased, variability in the population means that we cannot expect our conclusions to generalize equally well to every member of the population—all we can hope is that our conclusions apply on average.\nAddressing the consequences of sampling variability is challenging because, by definition, we do not know how variable the population is nor whether our sample is biased or not. In statistics, we address this lack of omniscience by constructing a descriptive model which enables us to estimate how wrong we might be. This is the meaning behind the “standard error” in traditional statistics or the posterior distribution in Bayesian statistics. In the end, we confine ourselves to conclusions that are supported by estimates that are strong enough to overcome this baseline level of wrongness, in which case we call our results “significant” or “credible”. Of course, this does not completely inoculate us from drawing improper generalizations, but it helps us pay attention to data patterns that are more likely to generalize while still acknowledging our uncertainty.\nOur techniques for comparing computational cognitive models serve the same function as a “standard error” or posterior distribution in descriptive statistical modeling. A good model fit may be due to idiosyncracies of the model or of the sample, neither of which may be representative of the broader population to which we are hoping to generalize. Because a more complex model has more flexibility to fit any possible sample, we want to avoid favoring a complex model unless it fits better over and above the degree to which we would expect it to fit better just due to its flexibility. As in statistics, model comparison is not guaranteed to identify the “true” model that explains performance on some task more generally. However, model comparison is a valuable tool that helps us identify the aspects of a model that are most essential for explaining performance and which are most likely to generalize.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "model_comparison.html#cross-validation",
    "href": "model_comparison.html#cross-validation",
    "title": "7  Model complexity and model comparison",
    "section": "7.2 Cross-validation",
    "text": "7.2 Cross-validation\nThe motivation behind many issues in model comparison is exemplified by the approach known as cross-validation (Arlot & Celisse, 2010; Browne, 2000; Zucchini, 2000). In cross-validation, one divides the sample of data into two parts, a training set and a testing set. The model is fit to the data in the training set and we then compute the log-likelihood of the data in the testing set, using the parameter values obtained by fitting the model to the training set. A model is preferred to the extent that is able to assign higher likelihood to the data in the training set. The rationale behind cross-validation is, thus, to evaluate a model on its ability to generalize from the training data to the test data. A model that is too flexible will tend to “over-fit” the various idiosyncratic features of the training data that are not reproduced in the testing data, meaning it will perform worse on average than a model that captures the systematic aspects that are common to both the training and testing data.\n\n7.2.1 Example\nTo make this situation concrete, let’s use a diffusion model to simulate some data and then use cross-validation to compare different potential models we could use to fit that data. Let’s again assume we are doing a recognition memory task, where target items have positive drift rates and foil items have negative drift rates. We will also assume that there is trial-by-trial variability in drift rates (the sv parameter) and that it is the same for both targets and foils.\n\n\nCode\nn_trials &lt;- 100\n\ntarget_trials &lt;- sampWiener(N = n_trials, a = 2, v = 0.5, w = 0.5, t0 = 0.2, sv = 0.3)\nfoil_trials &lt;- sampWiener(N = n_trials, a = 2, v = -0.5, w = 0.5, t0 = 0.2, sv = 0.3)\n\n(all_trials &lt;- tibble(\n    rt = c(target_trials$q, foil_trials$q),\n    response = factor(c(target_trials$response, foil_trials$response), levels = c(\"lower\", \"upper\")),\n    item = factor(rep(c(\"Target\", \"Foil\"), each = n_trials), levels = c(\"Target\", \"Foil\"))\n))\n\n\n# A tibble: 200 × 3\n      rt response item  \n   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt; \n 1 1.29  upper    Target\n 2 0.973 upper    Target\n 3 0.695 upper    Target\n 4 1.75  upper    Target\n 5 1.17  upper    Target\n 6 1.18  upper    Target\n 7 1.91  upper    Target\n 8 1.46  upper    Target\n 9 0.988 upper    Target\n10 0.554 upper    Target\n# ℹ 190 more rows\n\n\nIf we were coming to this data “fresh”, like we would in a real experiment, we might consider applying a few models, since we wouldn’t know which one best accounts for our data. For the sake of the present example, let’s focus on three possible models:\n\nModel A assumes that both the mean drift rate (v) and the drift rate standard deviation (sv) are the same for both targets and foils. This model is “incorrect”, in the sense that it assumes equal parameters for both targets and foils. Nonetheless, we may want to verify that participants are actually able to distinguish between targets and foils. In a different experiment, we might be interested in comparing two conditions to see whether drift rate differs between them. In any case, we can think of model A as a sort of “null” model.\nModel B assumes that the mean drift rate (v) varies between targets and foils, but the drift rate standard deviation (sv) is the same for both targets and foils. This model is “correct”, in the sense that it allows parameters to vary in the same way that they did in our simulations.\nModel C assumes that both the mean drift rate (v) and drift rate standard deviation (sv) vary between targets and foils. This model is “incorrect”, in the sense that it is too flexible relative to how the parameters varied in simulation. Nonetheless, we expect that this model will probably fit better than the “correct” model (B) since the additional drift rate variability parameter will enable it to fit any quirks in the simulated data.\n\n\n7.2.1.1 The steps of cross-validation\nTo see how cross-validation works, let’s go through a single example of applying it to our simulated data. First, we need to split our data into “training” and “testing” sets. We will do this randomly, so as not to introduce any bias. Relatedly, we will need to make sure that all the conditions of the experiment are represented in both testing and training sets in the same proportion that they are in the full data. Again, this avoids introducing bias by not “over-representing” one condition or the other.\nFor our first pass, let’s have the training and testing data be of equal size. We will use R’s sample function to randomly assign each trial within each condition (defined by item) to either the training or testing set.\n\n\nCode\nall_trials_traintest &lt;- all_trials %&gt;%\n    group_by(item) %&gt;%\n    mutate(set = factor(\n            # In the line below, `n()` is the number of trials within the groups defined by the variables in the `group_by` line above\n            sample(rep(c(\"training\", \"testing\"), round(c(0.5, 0.5) * n())))[1:n()],\n            levels = c(\"training\", \"testing\")\n        )\n    )\n\n\nYou may already have noticed something important about cross-validation: Because we have to divide the data up at random, it can give different results each time you do it! We will return to this issue.\nFor now, though, once we have divided up our data, we need to fit each model to only the training data. This looks just like it did in the previous chapter, where we are using fit_wienr to find parameter estimates for each model.\n\n\nCode\nfit_a &lt;- with(\n    all_trials_traintest %&gt;%\n        filter(set == \"training\"),\n    fit_wienr(rt = rt, response = response, fit_sv = TRUE)\n)\n\nfit_b &lt;- with(\n    all_trials_traintest %&gt;%\n        filter(set == \"training\"),\n    fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), fit_sv = TRUE)\n)\n\nfit_c &lt;- with(\n    all_trials_traintest %&gt;%\n        filter(set == \"training\"),\n    fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE)\n)\n\n\nWe can take a look at each of the fits to see what the estimated parameters are and how well each model fared on the training data:\n\n\nCode\nfit_a\n\n\n$par\n        a[1]         v[1]         w[1]        t0[1]        sv[1] \n2.091667e+00 1.159507e-01 4.695512e-01 1.982995e-01 1.731908e-10 \n\n$value\n[1] 165.1481\n\n$convergence\n[1] 1\n\n$message\n[1] \"Stopped by small gradient (grtol).\"\n\n$invhessian.lt\n [1]  9.527092e-03  1.491059e-04  1.064512e-04 -1.656450e-03  3.294452e-04\n [6]  1.339079e-02 -2.183984e-03  2.129479e-04  2.940070e-05  1.135847e-03\n[11] -1.118024e-04 -7.466557e-06  8.637283e-04 -1.958818e-04  7.797146e-01\n\n$info\n maxgradient     laststep      stepmax        neval \n2.435416e-08 2.894598e-07 1.838266e-03 2.200000e+01 \n\nattr(,\"class\")\n[1] \"ucminf\"\n\n\nCode\nfit_b\n\n\n$par\n      a[1]       v[1]       v[2]       w[1]      t0[1]      sv[1] \n 2.3376486  0.8377254 -0.4882018  0.4610377  0.1776323  0.4922548 \n\n$value\n[1] 148.8284\n\n$convergence\n[1] 1\n\n$message\n[1] \"Stopped by small gradient (grtol).\"\n\n$invhessian.lt\n [1]  0.0541570486  0.0358211608 -0.0340917434  0.0001095013 -0.0048551547\n [6]  0.0931226495  0.0635757612 -0.0216317695 -0.0030307138 -0.0015420380\n[11]  0.0813262103  0.0578574019 -0.0027352232  0.0020638460 -0.0799306514\n[16]  0.0012060421 -0.0001094818  0.0001507737  0.0012252126 -0.0052349758\n[21]  0.2343820376\n\n$info\n maxgradient     laststep      stepmax        neval \n5.156144e-09 7.943901e-07 1.838266e-03 2.300000e+01 \n\nattr(,\"class\")\n[1] \"ucminf\"\n\n\nCode\nfit_c\n\n\n$par\n      a[1]       v[1]       v[2]       w[1]      t0[1]      sv[1]      sv[2] \n 2.3478544  0.8629154 -0.4818190  0.4583973  0.1775218  0.5569940  0.4829696 \n\n$value\n[1] 148.8213\n\n$convergence\n[1] 1\n\n$message\n[1] \"Stopped by small gradient (grtol).\"\n\n$invhessian.lt\n [1]  0.0588198407  0.0499271148 -0.0276548552 -0.0016398157 -0.0047462573\n [6]  0.1222239863  0.0814171946  0.1050278210 -0.0079168106 -0.0075676409\n[11] -0.0013898078  0.1741511816  0.0577339592  0.0584003179 -0.0039679555\n[16]  0.0019161317 -0.0412546271 -0.0804351293  0.0016883348 -0.0001103656\n[21] -0.0109935932  0.0020953431  0.0012042522 -0.0045784184 -0.0048449821\n[26]  0.4517784322  0.1613120906  0.2311467565\n\n$info\n maxgradient     laststep      stepmax        neval \n3.801980e-08 2.683823e-07 5.252187e-03 2.100000e+01 \n\nattr(,\"class\")\n[1] \"ucminf\"\n\n\nAs expected, model A had the highest negative log-likelihood (i.e., the worst fit), followed by model B, with model C only doing barely better than model B. The estimated parameters for the “correct” model (B) are pretty close to those we used to simulate the data. Meanwhile, the estimated parameters for models A and C also tend to correspond pretty well with those used to generate the data (for example, the boundary separation a, response bias w, and residual time t0 for those models are all pretty close to the values we used in simulation).\nBut the real question is how well each model does with the testing data. To do that, we need to compute the negative log-likelihood of the data using the parameters estimated above. We can do that by passing the par element of the fits above as the init_par argument to the fit_wienr function and setting the return_nll argument to TRUE.\n\n\nCode\nwith(\n    all_trials_traintest %&gt;%\n        filter(set == \"testing\"),\n    fit_wienr(rt = rt, response = response, init_par = fit_a$par, fit_sv = TRUE, return_nll = TRUE)\n)\n\n\n[1] 146.1184\n\n\nCode\nwith(\n    all_trials_traintest %&gt;%\n        filter(set == \"testing\"),\n    fit_wienr(rt = rt, response = response, init_par = fit_b$par, drift_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n)\n\n\n[1] 139.8552\n\n\nCode\nwith(\n    all_trials_traintest %&gt;%\n        filter(set == \"testing\"),\n    fit_wienr(rt = rt, response = response, init_par = fit_c$par, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n)\n\n\n[1] 140.3905\n\n\nBased on these results, model A is the worst of the three, as we might have expected. But when evaluated on the testing data, model B actually fares slightly better than model C, despite the fact that model C achieved a better negative log-likelihood on the training data. This is an example of cross-validation working as intended—it has identified that model C is too flexible in this context. Model C “overfit” the training data to such an extent that it did not generalize as well to the testing data as model B.\n\n\n7.2.1.2 Repeating cross-validation\nAs noted above, though, we would get different results from cross-validation if we split the data into training/testing sets differently. To get a sense of which models are consistently able to generalize better, we need to replicate the cross-validation procedure several times, each with a different training/test split. In the code below, I use a for loop to do this. In the cv_results tibble, I keep track of the negative log-likelihood that each model achieves on both the training and testing data, so I can plot those at the end.\n\n\nCode\nn_cv &lt;- 100\n\ncv_results &lt;- c()\n\nfor (cv_index in 1:n_cv) {\n    # Split data into training/testing sets\n    all_trials_traintest &lt;- all_trials %&gt;%\n        group_by(item) %&gt;%\n        mutate(set = factor(\n                sample(rep(c(\"training\", \"testing\"), round(c(0.5, 0.5) * n())))[1:n()],\n                levels = c(\"training\", \"testing\")\n            )\n        )\n    \n    # Fit each model to the training data\n    fit_a &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"training\"),\n        fit_wienr(rt = rt, response = response, fit_sv = TRUE)\n    )\n    \n    fit_b &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"training\"),\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    fit_c &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"training\"),\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    # Evaluate each model on the testing data\n    test_nll_a &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"testing\"),\n        fit_wienr(rt = rt, response = response, init_par = fit_a$par, fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_b &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"testing\"),\n        fit_wienr(rt = rt, response = response, init_par = fit_b$par, drift_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_c &lt;- with(\n        all_trials_traintest %&gt;%\n            filter(set == \"testing\"),\n        fit_wienr(rt = rt, response = response, init_par = fit_c$par, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    # Save results of current iteration\n    cv_results &lt;- rbind(\n        cv_results,\n        tibble(\n            set = \"training\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(fit_a$value, fit_b$value, fit_c$value)\n        ),\n        tibble(\n            set = \"testing\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(test_nll_a, test_nll_b, test_nll_c)\n        )\n    )\n}\n\ncv_results %&gt;%\n    mutate(set = factor(set, levels = c(\"training\", \"testing\"))) %&gt;%\n    ggplot(aes(x = model, y = nll, color = model)) +\n    geom_point(position = position_jitter(width = 0.4), alpha = 0.1, size = 0.5) +\n    stat_summary(fun.data = mean_cl_boot) +\n    facet_wrap(\"set\", scales = \"free_y\") +\n    labs(x = \"Model\", y = \"Negative log-likelihood\")\n\n\n\n\n\nSmall, light points show each individual cross-validation run, the large points show the mean and bootstrapped 95% confidence interval across runs.\n\n\n\n\nNotice that model C does, on average, achieve a slightly better NLL than model B on the training data. Specifically, the average NLL for model B on the training data is 141.9735778 and for model C is 141.4932351. However, model B achieves a slightly better NLL than model C on the testing data (146.5512123 for model B, 146.6566674 for model C). These differences are not particularly large, of course, but they show the basic idea behind cross-validation as an approach to model comparison.\n\n\n\n7.2.2 \\(K\\)-fold cross-validation\nIn the example above, the testing and training sets were the same size. This is not terribly efficient. Because the models are only being fit to half the data, there is more variability/uncertainty in the estimated parameters than there would be if they were fit to the entire dataset. As such, in most applications of cross-validation, the training set is larger than the testing set.\nThese applications are often referred to as “\\(K\\)-fold cross-validation” because they involve splitting the data into \\(K\\) evenly-sized sets and then performing cross-validation \\(K\\) times. Each time, a different one of the \\(K\\) sets is treated as the “testing” data, with the remaining \\(K - 1\\) sets used for training. A common choice for \\(K\\) is 10, such that the proportion of data “left out” for testing is 0.1, not 0.5.\nLet’s see how we would implement \\(K\\)-fold cross-validation in our running example. The first step is to split the data into \\(K\\) equal sets. The code below shows one way to do this using the sample function like we did in the example above. Notice that we use the rep function to repeat each index ceiling(n() / K) times. The ceiling function rounds any fractional amounts up, so we will always have at least as many indexes to sample from as we have trials. The [1:n()] truncates the vector of repeated indices so that it has exactly n() elements.\n\n\nCode\nK &lt;- 10\n\nall_trials_split &lt;- all_trials %&gt;%\n    group_by(item) %&gt;%\n    mutate(set = sample(rep(1:K, ceiling(n() / K))[1:n()]))\n\n\nThe result looks like this, although it is worth keeping in mind that different runs will produce different splits since they are done randomly.\n\n\nCode\nall_trials_split\n\n\n# A tibble: 200 × 4\n# Groups:   item [2]\n      rt response item     set\n   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt;\n 1 1.29  upper    Target     4\n 2 0.973 upper    Target     9\n 3 0.695 upper    Target     6\n 4 1.75  upper    Target     1\n 5 1.17  upper    Target     4\n 6 1.18  upper    Target     8\n 7 1.91  upper    Target     2\n 8 1.46  upper    Target    10\n 9 0.988 upper    Target     3\n10 0.554 upper    Target     2\n# ℹ 190 more rows\n\n\nOnce we have split the data, we can adapt the for loop we used earlier so that it loops over the \\(K\\) folds in the splitted data.\n\n\nCode\nk_fold_cv_results &lt;- c()\n\nfor (fold in 1:K) {\n    # Fit each model to the training data\n    fit_a &lt;- with(\n        all_trials_split %&gt;%\n            filter(set != fold),\n        fit_wienr(rt = rt, response = response, fit_sv = TRUE)\n    )\n    \n    fit_b &lt;- with(\n        all_trials_split %&gt;%\n            filter(set != fold),\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    fit_c &lt;- with(\n        all_trials_split %&gt;%\n            filter(set != fold),\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    # Evaluate each model on the testing data\n    test_nll_a &lt;- with(\n        all_trials_split %&gt;%\n            filter(set == fold),\n        fit_wienr(rt = rt, response = response, init_par = fit_a$par, fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_b &lt;- with(\n        all_trials_split %&gt;%\n            filter(set == fold),\n        fit_wienr(rt = rt, response = response, init_par = fit_b$par, drift_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_c &lt;- with(\n        all_trials_split %&gt;%\n            filter(set == fold),\n        fit_wienr(rt = rt, response = response, init_par = fit_c$par, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    # Save results of current iteration\n    k_fold_cv_results &lt;- rbind(\n        k_fold_cv_results,\n        tibble(\n            fold = fold,\n            set = \"training\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(fit_a$value, fit_b$value, fit_c$value)\n        ),\n        tibble(\n            fold = fold,\n            set = \"testing\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(test_nll_a, test_nll_b, test_nll_c)\n        )\n    )\n}\n\nk_fold_cv_results %&gt;%\n    mutate(set = factor(set, levels = c(\"training\", \"testing\"))) %&gt;%\n    ggplot(aes(x = model, y = nll, color = model)) +\n    geom_point(position = position_jitter(width = 0.1), alpha = 0.5, size = 0.5) +\n    stat_summary(fun.data = mean_cl_boot) +\n    facet_wrap(\"set\", scales = \"free_y\") +\n    labs(x = \"Model\", y = \"Negative log-likelihood\")\n\n\n\n\n\n\n\n\n\nThe result looks pretty similar to what we had previously, in that model C fits slightly better than model B on the training data, but they fare about equally well on the testing data.\n\n\n7.2.3 Leave-one-out cross-validation\nAs a reminder, each time we run \\(K\\)-fold cross-validation, we will get a slightly different result because of the random way in which we split the data. Moreover, using \\(K\\)-fold CV was motivated by an attempt to make efficient use of the data at hand, so as not to artificially inflate our uncertainty about estimated model parameters. If we take these two issues—randomness and efficiency—seriously, then the best way to do cross-validation would actually be to have as many “folds” as we have observations. In other words, we fit each model to all but one observation and then test them on the one that we left out and repeat this process for all \\(N\\) observations in our dataset. That solves the efficiency problem, since the models are able to train on essentially all of the data. It also solves the randomness problem because instead of doing CV with random subsets, we do it exhaustively, once for each observation. This approach is, prosaically, referred to as Leave-One-Out Cross-Validation (LOOCV).\nWe said LOOCV resolves the “efficiency” issue with cross-validation, but only in the sense that the models are able to make use of nearly all the data. LOOCV is certainly not efficient in terms of computing time, since it requires fitting each model \\(N\\) times, once for each left-out observation. We typically apply computational cognitive models to data from experiments where we have a few hundred trials per participant (and we would need to replicate LOOCV for each participant too). Moreover, as we’ve seen, estimating best-fitting parameters even for a relatively simple cognitive model like a diffusion model is not trivial. Therefore, LOOCV is almost never used in practice.\nFor fun, though, let’s try it with our running example, where the code below adapts the \\(K\\)-fold CV code we used in the previous section. Note the use of the “negative indexing” trick to exclude each observation i from the training data in the for loop.\n\n\nCode\nloocv_results &lt;- c()\n\nfor (i in 1:nrow(all_trials)) {\n    # Fit each model to the training data\n    fit_a &lt;- with(\n        all_trials[-i,],\n        fit_wienr(rt = rt, response = response, fit_sv = TRUE)\n    )\n    \n    fit_b &lt;- with(\n        all_trials[-i,],\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    fit_c &lt;- with(\n        all_trials[-i,],\n        fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE)\n    )\n    \n    # Evaluate each model on the testing data\n    test_nll_a &lt;- with(\n        all_trials[i,],\n        fit_wienr(rt = rt, response = response, init_par = fit_a$par, fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_b &lt;- with(\n        all_trials[i,],\n        fit_wienr(rt = rt, response = response, init_par = fit_b$par, drift_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    test_nll_c &lt;- with(\n        all_trials[i,],\n        fit_wienr(rt = rt, response = response, init_par = fit_c$par, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE, return_nll = TRUE)\n    )\n    \n    # Save results of current iteration\n    loocv_results &lt;- rbind(\n        loocv_results,\n        tibble(\n            fold = i,\n            set = \"training\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(fit_a$value, fit_b$value, fit_c$value)\n        ),\n        tibble(\n            fold = i,\n            set = \"testing\",\n            model = c(\"A\", \"B\", \"C\"),\n            nll = c(test_nll_a, test_nll_b, test_nll_c)\n        )\n    )\n}\n\nloocv_results %&gt;%\n    mutate(set = factor(set, levels = c(\"training\", \"testing\"))) %&gt;%\n    ggplot(aes(x = model, y = nll, color = model)) +\n    geom_point(position = position_jitter(width = 0.1), alpha = 0.1, size = 0.5) +\n    stat_summary(fun.data = mean_cl_boot) +\n    facet_wrap(\"set\", scales = \"free_y\") +\n    labs(x = \"Model\", y = \"Negative log-likelihood\")\n\n\n\n\n\n\n\n\n\nConsistent with the other varieties of CV above, LOOCV finds that model A generalizes the worst on average (mean testing NLL = 1.5718645), followed by model C (mean testing NLL = 1.4550586) then closely by model B ((mean testing NLL = 1.453186)). Again, the difference between models B and C is not dramatic, but consider that model C consistently outperforms model B on the training data—the message that we get from LOOCV is that this advantage is due to overfitting, not because model C captures anything systematic beyond that which is captured by model B. Therefore, we should prefer the simpler model B when deciding which model best explains our data.\n\n\n7.2.4 Summary\nCross-validation is not always the most practical approach to assessing model fit vs. complexity. That said, it shows one reason why we might prefer a simpler model: Such a model is less likely to “overfit” our data and is therefore better able to generalize to new data. This has practical advantages if we are using the model to make predictions about future unseen data. It is also theoretically meaningful because a model that generalizes better is probably one that has mechanisms that are important for producing the systematic features of our data.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "model_comparison.html#akaike-information-criterion",
    "href": "model_comparison.html#akaike-information-criterion",
    "title": "7  Model complexity and model comparison",
    "section": "7.3 Akaike Information Criterion",
    "text": "7.3 Akaike Information Criterion\nThe practical issues with cross-validation mean that it is rarely used to compare cognitive models. That said, one of the model comparison approaches we saw in the last chapter, the Akaike Information Criterion (AIC; Akaike (1974)), is in fact an asymptotic approximation to LOOCV. We won’t prove this fact here, but check out Stone (1977). For our purposes, we can simply appreciate that the asymptotic equivalence of AIC and LOOCV is very convenient because it means that we can often reasonably approximate LOOCV while only needing to fit the model once.\nLet’s calculate the AIC for each of the three models in our running example. To do this, we will first need to fit each model to the full dataset (no more splitting into testing/training sets). This is done in the chunk of code below.\n\n\nCode\nfit_a &lt;- with(\n    all_trials,\n    fit_wienr(rt = rt, response = response, fit_sv = TRUE)\n)\n\nfit_b &lt;- with(\n    all_trials,\n    fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), fit_sv = TRUE)\n)\n\nfit_c &lt;- with(\n    all_trials,\n    fit_wienr(rt = rt, response = response, drift_index = as.numeric(item), sv_index = as.numeric(item), fit_sv = TRUE)\n)\n\n\nNow recall that the AIC is defined as\n\\[\nAIC = 2 \\times NLL + 2 \\times N_p\n\\]\nwhere \\(NLL\\) is the negative log-likelihood of the fitted model and \\(N_p\\) is the number of free parameters in the model. Thus, the code below computes the AIC for each of the three models\n\n\nCode\n2 * fit_a$value + 2 * length(fit_a$par)\n\n\n[1] 630.4553\n\n\nCode\n2 * fit_b$value + 2 * length(fit_b$par)\n\n\n[1] 583.0139\n\n\nCode\n2 * fit_c$value + 2 * length(fit_c$par)\n\n\n[1] 583.6266\n\n\nLike with cross-validation above, AIC finds that model A is the worst and that model B has a slight advantage over model C.\nReturning to LOOCV for a moment, recall that the value we obtained was the mean negative log-likelihood across each of the \\(N\\) left-out observations. Meanwhile, the \\(NLL\\) we get from fitting the full model is the summed negative log-likelihood across all \\(N\\) observations. So if we want to put the results from LOOCV on the same scale as the results we get from AIC, we need to multiply them by \\(2N\\). I do this in the chunk of code below.\n\n\nCode\nloocv_results %&gt;%\n    filter(set == \"testing\") %&gt;%\n    group_by(model) %&gt;%\n    summarize(rescaled_result = 2 * sum(nll))\n\n\n# A tibble: 3 × 2\n  model rescaled_result\n  &lt;chr&gt;           &lt;dbl&gt;\n1 A                629.\n2 B                581.\n3 C                582.\n\n\nAlthough the reader is again referred to Stone (1977) for a formal proof, this example shows that, when appropriately rescaled, AIC and LOOCV give very similar results and will generally lead us to the same conclusions regarding which of a set of models to prefer.\nThis rough equivalence also shows that AIC ultimately assesses models on their predictive performance, that is, their ability to fit future unseen data generated by the same processes that produced our original data.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "model_comparison.html#bayes-factors-and-bayesian-information-criterion",
    "href": "model_comparison.html#bayes-factors-and-bayesian-information-criterion",
    "title": "7  Model complexity and model comparison",
    "section": "7.4 Bayes Factors and Bayesian Information Criterion",
    "text": "7.4 Bayes Factors and Bayesian Information Criterion\nIn the last chapter, we were introduced to another model comparison metric, the so-called “Bayesian” Information Criterion [BIC; Schwarz (1978)]. The BIC is, under certain very restrictive circumstances, asymptotically equivalent to a Bayes Factor (Raftery, 1995). The relationship between AIC/LOOCV and the Bayes factor/BIC can be summarized like this: AIC/LOOCV assess the ability of a model to fit future data conditional on the data that has already been observed; Bayes factors/BIC assess the ability of a model to fit any data, irrespective of the data that has already been observed. In other words, AIC/LOOCV assess the posterior predictive ability of a model whereas BIC/Bayes Factors assess the prior predictive ability of a model (Gelman et al., 2014; Piironen & Vehtari, 2017; Vehtari & Lampinen, 2002).\nIt is worth repeating that BIC does not have the same formal relationship to Bayes factors that AIC has to LOOCV, so BIC should not be thought of, outside of very special cases, as equivalent to a Bayes factor. Nonetheless, it has the same underlying motivation, which is to favor models that make more limited predictions a priori. This is why the formula for BIC imposes a stronger penalty for the number of free parameters in a model, because the flexibility afforded by those parameters doesn’t just allow the model to “overfit” the data we observed, it allows it to overfit any data we might have observed.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "model_comparison.html#simplicity-vs.-complexity",
    "href": "model_comparison.html#simplicity-vs.-complexity",
    "title": "7  Model complexity and model comparison",
    "section": "7.5 Simplicity vs. Complexity",
    "text": "7.5 Simplicity vs. Complexity\nUltimately, model comparison allows us to answer the question, “what is the simplest model, among those I am considering, that is sufficient to achieve a good quantiative fit to the data in my sample?” By laying out this question explicitly, we are in a position to see three of the important qualifiers on any conclusions we draw based on model comparisons:\n\nDefining “simplicity”: Different model comparison metrics have different operational definitions of “simplicity”. AIC and BIC each define it in terms of the number of free parameters in a model. Cross-validation defines it in terms of how well a model fit to training data can account for test data. A Bayes factor defines it in terms of the prior predictive distribution of a model, etc.\nConditional on the set of models being compared: Although it may be possible to identify a “preferred” model using a model comparison metric, that preference is only with respect to the set of models being compared. It is entirely possible that an additional unconsidered model would be preferred if it were included. It may also be the case that the “preferred” model is only the “least bad” model among those under consideration—that’s why it is always important to verify that a model is actually reproducing the data patterns that you think are most important in your application.\nConditional on the sample: It may be that a different sample would have lead to a different “preferred” model, although as noted above, model comparison metrics are usually designed to account for this form of sampling variability. This qualification is more important when attempting to generalize more broadly, for example, to other kinds of related tasks or to the same task but with different materials.\n\nOften, model comparison is analogized to “Occam’s Razor”, the famous principle that, if many explanations are available, we should prefer the simplest one. The issue with this analogy is that it conflates two ways in which a model can be “simple”: A model can be “simple” according to one of the operational definitions of simplicity/complexity employed by a particular model comparison metric. But a model can also be “simple” in the sense that it is easier for a scientist to understand or to describe to someone else. The first sense of “simplicity” can be quantified (as in the methods reviewed in this chapter), but the second sense of “simplicity” is more to do with the background and expertise of particular scientists, the means by which they communicate, and the broader culture in which they are working. In other words, the second sense of “simplicity” has to do with the fact that a causal model is not meant just to fit data, but also to help people understand why the data turned out that way. As the bumper sticker says, scientists are people too and, being limited creatures, cannot understand everything. This second sense of simplicity should not be dismissed, though: If someone can understand a model more easily, they may also be able to devise predictions, tests, and extensions of the model more easily too.\nBecause the two senses of “simplicity” are separate, they are not guaranteed to align with one another. There may be cases in which a model that is “simple” in the sense of having few free parameters or a narrow prior predictive distribution may be very difficult to explain or describe. It is also possible that a model that is easier to explain or describe might be more flexible or have more parameters than needed to account for any particular sample of data. The latter situation is likely to occur if a model is designed to account for a wide variety of phenomena—such a model may contain mechanisms (with associated parameters) that are only relevant for certain phenomena.\nIt is also worth repeating that a simpler model—regardless of the sense of “simplicity”—is not guaranteed to be any more “true” or “correct” than a complex model. The “truth”, whatever that is, is almost certainly more complex than any model we would devise. Rather, given that all models are deliberate simplifications, the virtue of a simpler model is that (a) it is more likely to generalize well because it is less likely that its ability to fit data is due to some idiosyncratic property of the model or the sample; and (b) it is often (but not always) easier to describe and explain.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "model_comparison.html#exercises",
    "href": "model_comparison.html#exercises",
    "title": "7  Model complexity and model comparison",
    "section": "7.6 Exercises",
    "text": "7.6 Exercises\n\nThe discussions of cross-validation and generalization in this chapter focused on situations in which we wanted to “generalize” to data from the same (simulated) participant in the same (simulated) task. How would you adapt cross-validation to assess other kinds of generalization, such as from one participant to another? Or from one task to another? In formulating your thoughts, you may want to read Busemeyer & Wang (2000) and Navarro (2018).\nUnlike in cognitive modeling, where cross-validation is rarely used, machine learning models are often compared using cross-validation. Models in machine learning sit on the “descriptive” end of the modeling spectrum. Machine learning models are typically applied to very large datasets and have a lot of free parameters (e.g., each weight in a neural network model is technically a free parameter). Why do you think cross-validation is more common in machine learning than in cognitive modeling?\nGiven that AIC and BIC judge models according to different criteria, which do you think is better suited for identifying the model the “best explains” a given set of data? What reasons might there be to prefer one approach over the other? Could the term “explain” have different interpretations in different applications?\n\n\n\n\n\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6), 716–723.\n\n\nArlot, S., & Celisse, A. (2010). A survey of cross-validation procedures for model selection. Statistics Surveys, 4, 40–79. https://doi.org/10.1214/09-SS054\n\n\nBrowne, M. W. (2000). Cross-validation methods. Journal of Mathematical Psychology, 44(1), 108–132. https://doi.org/10.1006/jmps.1999.1279\n\n\nBusemeyer, J. R., & Wang, Y.-M. (2000). Model comparisons and model selections based on generalization criterion methodology. Journal of Mathematical Psychology, 44(1), 171–189. https://doi.org/10.1006/jmps.1999.1282\n\n\nGelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for bayesian models. Statistics and Computing, 24(6), 997–1016. https://doi.org/10.1007/s11222-013-9416-2\n\n\nNavarro, D. J. (2018). Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection. Computational Brain & Behavior. https://doi.org/10.1007/s42113-018-0019-z\n\n\nPiironen, J., & Vehtari, A. (2017). Comparison of bayesian predictive methods for model selection. Statistics and Computing, 27(3), 711–735. https://doi.org/10.1007/s11222-016-9649-y\n\n\nRaftery, A. E. (1995). Bayesian model selection in social research. Sociological Methodology, 25, 111–163. https://doi.org/10.2307/271063\n\n\nSchwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6(2), 461–464.\n\n\nStone, M. (1977). An asymptotic equivalence of choice of model by cross-validation and Akaike’s criterion. Journal of the Royal Statistical Society. Series B (Methodological), 39(1), 44–47.\n\n\nVehtari, A., & Lampinen, J. (2002). Bayesian model assessment and comparison using cross-validation predictive densities. Neural Computation, 14(10), 2439–2468. https://doi.org/10.1162/08997660260293292\n\n\nZucchini, W. (2000). An introduction to model selection. Journal of Mathematical Psychology, 44(1), 41–61. https://doi.org/10.1006/jmps.1999.1276",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model complexity and model comparison</span>"
    ]
  },
  {
    "objectID": "blast_example.html",
    "href": "blast_example.html",
    "title": "8  A worked example",
    "section": "",
    "text": "8.1 The data\nThe data for this example were reported originally by Trueblood et al. (2018). There’s a lot about this study that we won’t get to here, and I encourage you to check out the original paper.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A worked example</span>"
    ]
  },
  {
    "objectID": "blast_example.html#the-data",
    "href": "blast_example.html#the-data",
    "title": "8  A worked example",
    "section": "",
    "text": "8.1.1 Participants and procedures\nParticipants in this study did several blocks of a categorization task. The stimuli used in this task were images of cells that were either indicative of cancer—these are called “blast” cells—or normal—these are “non-blast” cells. The images were further subdivided into “easy” and “hard” versions, based on expert judgments. The image below illustrates the kinds of images that participants would see in this task.\n\n\nCode\nknitr::include_graphics(\"img/blast_example_stimuli.png\")\n\n\n\n\n\n(a) An easy blast image. (b) A hard blast image. (c) An easy non-blast image. (d) A hard non-blast image.\n\n\n\n\nAfter several blocks of training in which participants became familiar with these kinds of images (if they were not already; see below), participants moved on to the categorization task. On each trial of this task, an image was shown. Blast and non-blast images were shown equally often. Easy and hard versions of each type were also shown at the same rates. The participant’s job was to decide whether or not each image was a “blast” cell. The categorization task was itself divided into several blocks, each of which was a different type. We will be looking at data from two types of block: “Accuracy” blocks in which participants were encouraged to take their time and be accurate in their categorization of each image; and “Speed” blocks in which participants were encouraged to make their decisions quickly without regard to accuracy.\nThe participants in this study came from three different groups. Novice participants were just that—typical undergraduate university students who had no prior experience with these kinds of medical images. Inexperienced participants were pathologists who had just begun their training, so while they would be knowledgeable about these kinds of images, they might not have much practice categorizing them. Experienced participants were pathologists who had completed at least four training rotations who would have had plenty of practice dealing with these kinds of images.\nFinally, I note that, in addition to the blast/non-blast categorization task, all participants did a “Novel Object Memory Task” (NOMT) designed to measure their general ability to recognize visual objects, not just medical images of cells.\n\n\n8.1.2 Getting the data\nYou can download the data from this study that we will be examining in this tutorial by running the code below. The first line downloads the data to a file called blast_data.rdata in your current working directory. The second line loads that data into your R environment.\n\n\nCode\ndownload.file(\"https://github.com/gregcox7/choice_rt_models/raw/refs/heads/main/data/blast_data.rdata\", \"blast_data.rdata\")\nload(\"blast_data.rdata\")\n\n\nThe data should now be in your R environment in a data frame called blast_data. Let’s take a look at that data now:\n\n\nCode\nglimpse(blast_data)\n\n\nRows: 21,628\nColumns: 15\n$ dateCompleted    &lt;chr&gt; \"30/6/2017 @ 10:15:24\", \"30/6/2017 @ 10:15:26\", \"30/6…\n$ block            &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ blockType        &lt;fct&gt; Speed, Speed, Speed, Speed, Speed, Speed, Speed, Spee…\n$ trial            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17…\n$ stimulus         &lt;chr&gt; \"nonBlastEasy/SNE_25598565.jpg\", \"blastHard/BL_406213…\n$ difficulty       &lt;fct&gt; Easy, Hard, Easy, Hard, Hard, Easy, Easy, Hard, Easy,…\n$ response         &lt;fct&gt; Non-blast, Blast, Blast, Blast, Non-blast, Non-blast,…\n$ rt               &lt;dbl&gt; 0.662, 0.496, 0.528, 0.431, 0.817, 0.495, 0.540, 0.68…\n$ correct_response &lt;fct&gt; Non-blast, Blast, Blast, Blast, Blast, Non-blast, Non…\n$ bias_shown       &lt;fct&gt; Bias not shown, Bias not shown, Bias not shown, Bias …\n$ subject          &lt;chr&gt; \"M002\", \"M002\", \"M002\", \"M002\", \"M002\", \"M002\", \"M002…\n$ group            &lt;fct&gt; Experienced, Experienced, Experienced, Experienced, E…\n$ nomt_corr        &lt;dbl&gt; 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 9…\n$ nomt_n           &lt;int&gt; 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108…\n$ nomt             &lt;dbl&gt; 0.9166667, 0.9166667, 0.9166667, 0.9166667, 0.9166667…\n\n\nWe can already see the columns that will be most important for us:\n\nblockType: Whether the block instructions emphasized Accuracy or Speed.\ncorrect_response: Whether the image on that trial was a Blast or Non-blast cell.\ndifficulty: Whether the image on that trial was Easy or Hard.\nrt: The response time (RT) in seconds.\nresponse: Whether the participant classified the image as a Blast or Non-blast cell.\nsubject: An identifier for each individual participant.\ngroup: Which of the three groups the participant came from (Experienced, Inexperienced, or Novice).\nnomt: The score on the Novel Object Memory Test (NOMT) for the participant on that trial.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A worked example</span>"
    ]
  },
  {
    "objectID": "blast_example.html#a-single-participant",
    "href": "blast_example.html#a-single-participant",
    "title": "8  A worked example",
    "section": "8.2 A single participant",
    "text": "8.2 A single participant\nIn the next section, we will fit a diffusion model to data from every participant. Before we do that, though, let’s see how to do it for a single participant. We will replicate this procedure for each individual participant in the next section.\n\n8.2.1 A single participant’s data\nI arbitrarily picked the participant with ID “M003” for us to examine. The code below uses the filter function to extract the data from just this participant:\n\n\nCode\nsubj_data &lt;- blast_data %&gt;%\n    filter(subject == \"M003\")\n\n\n\n\n8.2.2 Grouping the trials\nFor the next bit, make sure that you have sourced the wienr_fit_utils.r script:\n\n\nCode\nsource(\"wienr_fit_utils.r\")\n\n\nIf we omit the par argument, we can use the qp_fit function to get the observed response proportions and RT quantiles and make a quantile-probability plot of the observed data. However, to do this, we need to decide how to group the individual trials using the “indexing” trick we used in the last chapter. The way we do this will ultimately inform what diffusion model parameters we will estimate, so it is worth putting in the thought now.\nSpecifically, we need to think about what factors would influence the drift rate of the evidence being accumulated on each trial, what factors would influence how the participant sets their response boundaries on a given trial, and what factors might influence the residual time on each trial. Later, we will also consider how trial-by-trial variability in these three aspects of the model might or might not vary between conditions.\n\n8.2.2.1 What factors influence drift rates?\nThe “evidence” in this task arises from some kind of evaluation of how much the image looks like what the participant thinks of as a “blast” cell versus a “non-blast” cell. In other words, the “evidence” should depend on whether the image on that trial shows a blast or non-blast cell, just like how “evidence” in recognition memory depends on whether the test item is a target or foil. In addition, we would expect “hard” images to yield worse evidence than “easy” images, by definition. These two aspects of the data are reflected in the difficulty and correct_response columns. So we can specify a drift_index based on the interaction between these two factors.\nThe emphasis of the current block—Accuracy vs. Speed—could also impact drift rates (Rae et al., 2014), though exploring that possibility is left as an exercise for the reader.\n\n\n8.2.2.2 What factors influence response boundaries?\nThe response boundaries cannot be influenced by the type of image shown on a trial—if they were, then the participant would already know what kind of image they were seeing! On the other hand, it is reasonable to expect that participants would adjust their response boundaries depending on whether the current block emphasized speed or accuracy. This suggests that we can define a bound_index using the blockType column in the data.\n\n\n8.2.2.3 What factors influence residual time?\nIf residual time reflects only the processes involved in executing the motor response associated with a choice, then we might expect it to be unaffected by any experimental factors. On the other hand, it may be that participants are able to adjust their “response vigor” in light of speed/accuracy emphasis. In addition, it may be that participants can more quickly orient their attention to a stimulus if speed is emphasized. So we can specify a resid_index that also depends on blockType.\n\n\n8.2.2.4 Defining indices\nOn the basis of the considerations above, we will define three indices: one that specifies what conditions can have different drift rates (drift_index), one that specifies what conditions can have different response boundaries (bound_index), and one that specifies what conditions can have different residual time (resid_index):\n\n\nCode\nsubj_data &lt;- subj_data %&gt;%\n    mutate(\n        drift_index = as.numeric(interaction(difficulty, correct_response)),\n        bound_index = as.numeric(blockType),\n        resid_index = as.numeric(blockType)\n    )\n\n\nIt is important to keep in mind that the grouping defined above is not necessarily the “one true grouping”! It is merely meant to give a sense of the kind of things to think about when deciding how different model parameters will be assigned to different conditions.\n\n\n\n8.2.3 Plotting the observed data\nHaving defined our indices, we can pass them to the qp_fit function so that we can make a quantile-probability plot of this participant’s data. Note that I had to\n\n\nCode\nobs_qp &lt;- qp_fit(\n    rt = subj_data$rt,\n    response = subj_data$response,\n    drift_index = subj_data$drift_index,\n    bound_index = subj_data$bound_index,\n    resid_index = subj_data$resid_index\n)\n\n\nJoining with `by = join_by(drift_index, bound_index, resid_index, sv_index,\nsw_index, st0_index, response)`\n\n\nWhen making the plot, I found it helpful to “undo” the transformation of the different factors into numerical indices. That “undoing” is the purpose of the two mutate lines.\n\n\nCode\nobs_qp %&gt;%\n    mutate(item_type = factor(drift_index, levels = 1:4, labels = levels(interaction(blast_data$difficulty, blast_data$correct_response)))) %&gt;%\n    mutate(blockType = factor(bound_index, levels = 1:2, labels = levels(blast_data$blockType))) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = item_type, shape = response)) +\n    geom_point() +\n    expand_limits(x = c(0, 1)) +\n    labs(x = \"Response probability\", y = \"RT quantile\") +\n    facet_wrap(\"blockType\")\n\n\n\n\n\n\n\n\n\nIt is worth noting a few features of these data that are apparent from the quantile-probability plot. First, this participant was indeed faster in the Speed block than the Accuracy block. Even the faster RT’s (the 0.1 quantiles) are faster in the Speed block, supporting the idea that residual time could differ between blocks if residual time represents the minimal time needed to respond. It also looks like this participant was less accurate in the Speed block—at least for Blast images, they had nearly perfect accuracy in the Accuracy block but not in the speed block. This participant was not very good in either block at categorizing Non-blast images. It doesn’t look like difficulty (Easy vs. Hard) made a big difference for this participant in terms of their choice/RT behavior. Finally, it looks like this participant’s errors tended to be a bit slower than their correct responses, suggesting that the diffusion model will need to allow for trial-by-trial variability in drift rates to accommodate these data. This same consideration suggests that we don’t need to assume variability in boundaries (since that would produce fast errors instead).\n\n\n8.2.4 Fitting a diffusion model\nWith all the preliminaries out of the way, let’s try fitting a diffusion model to this participant’s data. This will look just like it did in the last chapter, only with real data instead of simulated data!\nWe have already decided how to assign parameters to trials using the indices we defined in the previous section. We also have good reason to believe that drift rates can vary from trial to trial. We can estimate \\(s_v\\), the standard deviation of the trial-by-trial distribution of drift rates, by including the argument fit_sv = TRUE to the fit_wienr function. We don’t have reason to assume variability in boundaries, which would be reflected in the \\(s_w\\) parameter, but we could do so if we passed fit_sw = TRUE to fit_wienr. Finally, we will allow for variability in residual time by including fit_st0 = TRUE in the function call to fit_wienr.\nFor present purposes, we will only estimate one value of \\(s_v\\) and one value of \\(s_{t_0}\\) parameter, and these values will apply to all trials. If we wanted to allow them to vary, we could pass a sv_index, sw_index, or st0_index vector to the fit_wienr function—these index vectors work just like the drift_index, bound_index, and resid_index vectors we defined above.\nPutting it all together, the code below fits our desired diffusion model to this participant’s choice and RT data.\n\n\nCode\nsubj_fit &lt;- fit_wienr(\n    rt = subj_data$rt,\n    response = subj_data$response,\n    fit_sv = TRUE,\n    fit_sw = FALSE,\n    fit_st0 = TRUE,\n    drift_index = subj_data$drift_index,\n    bound_index = subj_data$bound_index,\n    resid_index = subj_data$resid_index\n)\n\n\nLet’s have a look at the estimated parameter values:\n\n\nCode\nsubj_fit$par\n\n\n      a[1]       a[2]       v[1]       v[2]       v[3]       v[4]       w[1] \n 1.3875689  0.9074497 -0.9075545 -0.3305451  1.5610599  1.6110799  0.6583813 \n      w[2]      t0[1]      t0[2]      sv[1]     st0[1] \n 0.6743305  0.3969458  0.3926721  0.7117295  0.1218538 \n\n\nThe first two parameters are the response caution parameters, with a[1] corresponding to the Accuracy blocks and a[2] to the Speed blocks. As we might expect, the fact thata a[2] \\(&lt;\\) a[1] tells us that this participant was less cautious in the Speed blocks, being more willing to sacrifice accuracy for speed. Skipping ahead to w[1] and w[2], these parameters tell us that this participant was biased toward calling images “Blast” images in both Accuracy and Speed blocks (the response caution and response bias parameters have the same indices). Although we allowed for residual time to vary between Accuracy and Speed blocks, the estimates t0[1] and t0[2] look pretty similar to one another.\nThe drift rate parameters also make some sense: v[1], for easy non-blast images, is negative and has a greater magnitude than v[2], for hard non-blast images. The magnitudes of the drift rates for Blast images, v[3] and v[4], are greater than for the non-blast images and are not too different from one another, in accord with our observation that this participant was better at identifying blast images than non-blasts and that the difficulty of the blast image didn’t seem to matter much.\nFinally, we can see that the drift-rate variability parameter sv[1] and the residual time variability parameter st0[1] are both greater than zero. That said, we did not have strong theoretical reasons to expect these parameters to take any particular value—we just suspected they would be important to account for the data. We can verify that intuition by fitting a model without any trial-by-trial variability and seeing whether AIC and/or BIC still prefers the more complex model with both forms of variability.\n\n\nCode\nsubj_fit_novar &lt;- fit_wienr(\n    rt = subj_data$rt,\n    response = subj_data$response,\n    fit_sv = FALSE,\n    fit_sw = FALSE,\n    fit_st0 = FALSE,\n    drift_index = subj_data$drift_index,\n    bound_index = subj_data$bound_index,\n    resid_index = subj_data$resid_index\n)\n\naic_wvar &lt;- 2 * subj_fit$value + 2 * length(subj_fit$par)\naic_novar &lt;- 2 * subj_fit_novar$value + 2 * length(subj_fit_novar$par)\n\nbic_wvar &lt;- 2 * subj_fit$value + log(nrow(subj_data)) * length(subj_fit$par)\nbic_novar &lt;- 2 * subj_fit_novar$value + log(nrow(subj_data)) * length(subj_fit_novar$par)\n\nc(aic_wvar, aic_novar)\n\n\n[1]  8.878357 64.029238\n\n\nCode\nc(bic_wvar, bic_novar)\n\n\n[1]  56.68559 103.86860\n\n\nBoth AIC and BIC are lower for the model with trial-by-trial variability, suggesting that this additional complexity is warranted in light of the data.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A worked example</span>"
    ]
  },
  {
    "objectID": "blast_example.html#all-the-participants",
    "href": "blast_example.html#all-the-participants",
    "title": "8  A worked example",
    "section": "8.3 All the participants",
    "text": "8.3 All the participants\nHaving fit a diffusion model to one participant, we will now replicate that procedure for every participant. First, it will be convenient to define our three index vectors using the whole dataset:\n\n\nCode\nblast_data &lt;- blast_data %&gt;%\n    mutate(\n        drift_index = as.numeric(interaction(difficulty, correct_response, drop = TRUE)),\n        bound_index = as.numeric(blockType),\n        resid_index = as.numeric(blockType)\n    )\n\n\nNow comes the big stuff. We will write a for loop that does the following for each participant:\n\nExtracts that participant’s data from the complete dataset.\nFits a diffusion model to that participant’s data.\nExtracts the estimated parameters for that participant and saves them in a data frame called model_pars. This is so we can examine the estimated parameters later.\nComputes both observed and model-produced RT quantiles and response probabilities and saves them in a data frame called model_qp. This is so we can verify that the model is fitting the data.\n\nAll of that is accomplished with the following chunk of R code, which begins by using the unique function to extract all the unique participant ID’s in the dataset. Note that this is used to define what the for loop iterates over. This will take a while to run, but patience is a virtue!\n\n\nCode\nsubj_to_fit &lt;- unique(blast_data$subject)\n\nmodel_pars &lt;- c()\nmodel_qp &lt;- c()\n\nfor (id in subj_to_fit) {\n    this_subj_data &lt;- blast_data %&gt;%\n        filter(subject == id)\n    \n    this_fit &lt;- fit_wienr(\n        rt = this_subj_data$rt,\n        response = (this_subj_data$response == \"Blast\") + 1,\n        fit_sv = TRUE,\n        fit_sw = FALSE,\n        fit_st0 = TRUE,\n        drift_index = this_subj_data$drift_index,\n        bound_index = this_subj_data$bound_index,\n        resid_index = this_subj_data$resid_index\n    )\n    \n    model_pars &lt;- rbind(\n        model_pars,\n        tibble(subject = id, group = this_subj_data$group[1], par_name = names(this_fit$par), val = this_fit$par) %&gt;% extract(par_name, into = c(\"par\", \"index\"), regex = \"(.+)\\\\[(.+)\\\\]\")\n    )\n    \n    this_qp &lt;- qp_fit(\n        rt = this_subj_data$rt,\n        response = (this_subj_data$response == \"Blast\") + 1,\n        par = this_fit$par,\n        drift_index = this_subj_data$drift_index,\n        bound_index = this_subj_data$bound_index,\n        resid_index = this_subj_data$resid_index\n    ) %&gt;%\n        mutate(subject = id, group = this_subj_data$group[1])\n    \n    model_qp &lt;- rbind(\n        model_qp,\n        this_qp\n    )\n}\n\n\n\n8.3.1 Comparing parameters between groups\nOnce we have our parameter estimates safely stored in model_pars, we can visualize the resulting estimates using color to distinguish between the three groups. The plot below was made by using tiny, slightly faded points for each individual participant (note the alpha = 0.5, size = 0.5 settings in the geom_point line). Overlaid on those is a big point with error bars that shows the mean and 95% confidence interval for the mean, computed separately for each group.\n\n\nCode\nmodel_pars %&gt;%\n    ggplot(aes(x = index, y = val, color = group, shape = group)) +\n    geom_point(position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.4), alpha = 0.5, size = 0.5) +\n    stat_summary(geom = \"pointrange\", fun.data = mean_cl_boot, position = position_dodge(width = 0.4)) +\n    labs(x = \"Index\", y = \"Estimated value\", color = \"Group\") +\n    facet_wrap(\"par\", scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n8.3.1.1 Response caution\nLet’s focus first on a, the response caution parameter. As we know, a[1] corresponds to the Accuracy blocks while a[2] corresponds to the Speed blocks. It certainly looks like participants, on average, had lower response caution in the Speed blocks than in the Accuracy blocks. It also looks like the more experienced participants tended to have greater response caution in both block types.\nTo get some statistical evidence for differences between groups and between conditions, we can use our old friend, the Analysis of Variance (ANOVA). While you might normally think of applying ANOVA to observed values, like mean response time or accuracy, it can be applied just as well to estimated parameter values. In both cases, we have a single value for each participant in each condition and we are testing the null hypothesis that the parameter estimate does not differ, on average, between conditions/groups.\nTo do ANOVA, I’ll use the afex R package and make sure to run its set_sum_contrasts() function (by default, R uses “treatment” contrasts, which are not always appropriate).\n\n\nCode\nlibrary(afex)\n\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\n************\nWelcome to afex. For support visit: http://afex.singmann.science/\n\n\n- Functions for ANOVAs: aov_car(), aov_ez(), and aov_4()\n- Methods for calculating p-values with mixed(): 'S', 'KR', 'LRT', and 'PB'\n- 'afex_aov' and 'mixed' objects can be passed to emmeans() for follow-up tests\n- Get and set global package options with: afex_options()\n- Set sum-to-zero contrasts globally: set_sum_contrasts()\n- For example analyses see: browseVignettes(\"afex\")\n************\n\n\n\nAttaching package: 'afex'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nCode\nset_sum_contrasts()\n\n\nsetting contr.sum globally: options(contrasts=c('contr.sum', 'contr.poly'))\n\n\nNow, we can use the aov_ez function to do the ANOVA on the a parameter estimates.\n\n\nCode\naov_ez(\n    id = \"subject\",      # Specify the name of the column that identifies unique participants\n    dv = \"val\",          # Specify the name of the column that contains the values to be analyzed\n    data = model_pars %&gt;% filter(par == \"a\"), # The data for this ANOVA is stored in \"model_pars\", but we are only interested in the estimates of the \"a\" parameter\n    between = \"group\",   # Specify the name of the column that identifies between-subject comparisons\n    within = \"index\"     # Specify the name of the column that identifies within-subject comparisons\n)\n\n\nAnova Table (Type 3 tests)\n\nResponse: val\n       Effect    df  MSE          F  ges p.value\n1       group 2, 52 0.15  17.86 *** .264   &lt;.001\n2       index 1, 52 0.14 129.70 *** .544   &lt;.001\n3 group:index 2, 52 0.14   9.23 *** .145   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAs we can see, there is a main effect of “group”, consistent with our observation that more experienced participants had higher response caution. There is also a main effect of “index”, consistent with our observation that participants tended to set lower response caution in Speed blocks. Finally, there is a significant interaction between “group” and “index”, although it looks from the graph above that this is likely to be a “fan” interaction, with a bigger increase from Speed to Accuracy for the more experienced participants.\n\n\n8.3.1.2 Drift rates\nNow, let’s consider the drift rate parameters. Again, we will use ANOVA to look for statistical evidence of differences in drift rates between groups and between conditions. Things are a little more complicated, though, because drift rate was allowed to vary by both difficulty and image type (blast vs. non-blast). To properly specify the ANOVA, then, we should “undo” the drift rate indices back into those original two factors. That’s what the mutate lines in the data specification do in the code below.\n\n\nCode\naov_ez(\n    id = \"subject\",\n    dv = \"val\",\n    data = model_pars %&gt;%\n        filter(par == \"v\") %&gt;%\n        mutate(\n            difficulty = factor(index, levels = 1:4, labels = c(\"Easy\", \"Hard\", \"Easy\", \"Hard\")),\n            correct_response = factor(index, levels = 1:4, labels = c(\"Non-blast\", \"Non-blast\", \"Blast\", \"Blast\"))),\n    between = \"group\",\n    within = c(\"difficulty\", \"correct_response\")\n)\n\n\nAnova Table (Type 3 tests)\n\nResponse: val\n                             Effect    df  MSE          F   ges p.value\n1                             group 2, 52 1.65       0.02 &lt;.001    .980\n2                        difficulty 1, 52 0.25  68.87 ***  .052   &lt;.001\n3                  group:difficulty 2, 52 0.25     3.24 *  .005    .047\n4                  correct_response 1, 52 3.37 252.60 ***  .728   &lt;.001\n5            group:correct_response 2, 52 3.37  21.45 ***  .313   &lt;.001\n6       difficulty:correct_response 1, 52 0.83 164.02 ***  .300   &lt;.001\n7 group:difficulty:correct_response 2, 52 0.83  16.01 ***  .077   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nThe ANOVA finds evidence for significant differences for all but “group” on its own. However, this analysis is a bit misleading in that, as you’ll recall, drift rates for non-blast images tend to be negative while drift rates for blast images tend to be positive. We may be more interested in analyzing how drift rates toward the correct response boundary may or may not differ between groups/conditions.\nTo do this, we can add another mutate line that reverses the sign of the estimated drift rates for non-blast images:\n\n\nCode\naov_ez(\n    id = \"subject\",\n    dv = \"val\",\n    data = model_pars %&gt;%\n        filter(par == \"v\") %&gt;%\n        mutate(\n            difficulty = factor(index, levels = 1:4, labels = c(\"Easy\", \"Hard\", \"Easy\", \"Hard\")),\n            correct_response = factor(index, levels = 1:4, labels = c(\"Non-blast\", \"Non-blast\", \"Blast\", \"Blast\")),\n            val = if_else(correct_response == \"Blast\", val, -val)),\n    between = \"group\",\n    within = c(\"difficulty\", \"correct_response\")\n)\n\n\nAnova Table (Type 3 tests)\n\nResponse: val\n                             Effect    df  MSE          F   ges p.value\n1                             group 2, 52 3.37  21.45 ***  .313   &lt;.001\n2                        difficulty 1, 52 0.83 164.02 ***  .300   &lt;.001\n3                  group:difficulty 2, 52 0.83  16.01 ***  .077   &lt;.001\n4                  correct_response 1, 52 1.65       0.31  .002    .578\n5            group:correct_response 2, 52 1.65       0.02 &lt;.001    .980\n6       difficulty:correct_response 1, 52 0.25  68.87 ***  .052   &lt;.001\n7 group:difficulty:correct_response 2, 52 0.25     3.24 *  .005    .047\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nNow the ANOVA correctly detects a main effect of group that was obscured in the previous analysis, among other things.\n\n\n8.3.1.3 Individual differences\nFinally, recall that each participant also completed the “NOMT”, a test of general visual object processing ability. It would be reasonable to ask whether participants who have high NOMT scores also tend to have higher drift rates toward the correct response boundary. To analyze this, we need to first extract the NOMT scores for each participant, append them to the model parameter estimates, and include NOMT as a covariate in the ANOVA. For interpretability, I also “center” the NOMT scores by subtracting the group mean.\n\n\nCode\n# Extract NOMT scores and center them.\nnomt_scores &lt;- blast_data %&gt;%\n    group_by(group, subject) %&gt;%\n    summarize(nomt = first(nomt)) %&gt;%\n    mutate(nomt_centered = nomt - mean(nomt))\n\n\n`summarise()` has grouped output by 'group'. You can override using the\n`.groups` argument.\n\n\nCode\n# Append the NOMT scores to the parameter estimates\nmodel_pars_nomt &lt;- left_join(model_pars, nomt_scores)\n\n\nJoining with `by = join_by(group, subject)`\n\n\nCode\n# Run the same ANOVA as above, now including `nomt_centered` as a `covariate`\naov_ez(\n    id = \"subject\",\n    dv = \"val\",\n    data = model_pars_nomt %&gt;%\n        filter(par == \"v\") %&gt;%\n        mutate(\n            difficulty = factor(index, levels = 1:4, labels = c(\"Easy\", \"Hard\", \"Easy\", \"Hard\")),\n            correct_response = factor(index, levels = 1:4, labels = c(\"Non-blast\", \"Non-blast\", \"Blast\", \"Blast\")),\n            val = if_else(correct_response == \"Blast\", val, -val)),\n    between = \"group\",\n    within = c(\"difficulty\", \"correct_response\"),\n    covariate = \"nomt_centered\",\n    factorize = FALSE  # This last setting is necessary to ensure that \"nomt_centered\" isn't accidentally treated like a factor\n)\n\n\nAnova Table (Type 3 tests)\n\nResponse: val\n                                      Effect    df  MSE          F   ges\n1                                      group 2, 51 3.05  23.64 ***  .334\n2                              nomt_centered 1, 51 3.05     6.33 *  .063\n3                                 difficulty 1, 51 0.68 199.27 ***  .320\n4                           group:difficulty 2, 51 0.68  19.46 ***  .084\n5                   nomt_centered:difficulty 1, 51 0.68   12.18 **  .028\n6                           correct_response 1, 51 1.67       0.31  .002\n7                     group:correct_response 2, 51 1.67       0.02 &lt;.001\n8             nomt_centered:correct_response 1, 51 1.67       0.34  .002\n9                difficulty:correct_response 1, 51 0.24  71.48 ***  .057\n10         group:difficulty:correct_response 2, 51 0.24     3.36 *  .006\n11 nomt_centered:difficulty:correct_response 1, 51 0.24     2.97 +  .003\n   p.value\n1    &lt;.001\n2     .015\n3    &lt;.001\n4    &lt;.001\n5     .001\n6     .581\n7     .980\n8     .562\n9    &lt;.001\n10    .043\n11    .091\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nIn fact, it looks like NOMT not only has a main effect on drift rates, it also interacts with difficulty, suggesting that group differences alone do not account for individual differences in performance on this task—categorizing images of cells also seems to depend on general object processing ability.\n\n\n\n8.3.2 Visualizing model fit\nFinally, we come to the most challenging section: How to visualize the quality of the model fit. We could, of course, produce quantile-probability plots for each participant separately, but this would only be feasible with very few participants.\nInstead, the code below plots the observed and fitted RT quantiles and response probabilities averaged over the participants in each group. This is not meant to be the final word, but just a way to verify that the model is close to the data and that it is accurately reproducing the important aspects of the data.\n\n\nCode\nmodel_qp %&gt;%\n    mutate(\n        blockType = factor(bound_index, labels = levels(blast_data$blockType)),\n        item_type = factor(drift_index, labels = levels(interaction(blast_data$difficulty, blast_data$correct_response, sep = \" \", drop = T)))\n    ) %&gt;%\n    group_by(group, blockType, item_type, response, source, rt_p) %&gt;%\n    summarize(rt_q = mean(rt_q, na.rm = TRUE), p_resp = mean(p_resp, na.rm = TRUE)) %&gt;%\n    ggplot(aes(x = p_resp, y = rt_q, color = item_type)) +\n    geom_point(aes(shape = source), fill = \"white\") +\n    scale_linetype_manual(values = c(\"Observed\" = \"solid\", \"Fitted\" = \"dashed\")) +\n    scale_shape_manual(values = c(\"Observed\" = 16, \"Fitted\" = 21)) +\n    facet_grid(blockType ~ group, scales = \"free_y\")\n\n\n`summarise()` has grouped output by 'group', 'blockType', 'item_type',\n'response', 'source'. You can override using the `.groups` argument.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's linetype values.\n\n\n\n\n\n\n\n\n\nThe upshot is that it looke like the model is, at least on average, doing a very good job of capturing the response proportion and a pretty good one capturing the RT quantiles. That said, some of the misfits for the highest and lowest quantiles (see, e.g., the green points in the “Speed” conditions) may be due to sampling error, as discussed earlier.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A worked example</span>"
    ]
  },
  {
    "objectID": "blast_example.html#exercises",
    "href": "blast_example.html#exercises",
    "title": "8  A worked example",
    "section": "8.4 Exercises",
    "text": "8.4 Exercises\n\nRun an ANOVA analysis on other estimated model parameters, like bias (w), residual time (t0), and the two variability parameters (sv and st0). Do you find evidence for differences, on average, between groups or between conditions (for sv and st0, you can only compare between groups)?\nUsing the sv_index and st0_index parameters, modify the diffusion model we used above so that drift rate variability and residual time variability can also vary by block type. Does this more complex model provide a better account of the data, as scored by either AIC or BIC?\n\n\n\n\n\nRae, B., Heathcote, A., Donkin, C., Averell, L., & Brown, S. (2014). The hare and the tortoise: Emphasizing speed can change the evidence used to make decisions. Journal of Experimental Psychology: Learning, Memory, and Cognition, 40(5), 1226–1243.\n\n\nTrueblood, J. S., Holmes, W. R., Seegmiller, A. C., Douds, J., Compton, M., Szentirmai, E., Woodruff, M., Huang, W., Stratton, C., & Eichbaum, Q. (2018). The impact of speed and bias on the cognitive processes of experts and novices in medical image decision-making. Cognitive Research: Principles and Implications, 3, 1–14.",
    "crumbs": [
      "First foray",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A worked example</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Akaike, H. (1974). A new look at the\nstatistical model identification. IEEE Transactions on Automatic\nControl, 19(6), 716–723.\n\n\nArlot, S., & Celisse, A. (2010). A survey of cross-validation\nprocedures for model selection. Statistics Surveys, 4,\n40–79. https://doi.org/10.1214/09-SS054\n\n\nBlurton, S. P., Kesselmeier, M., & Gondan, M. (2012). Fast and\naccurate calculations for cumulative first-passage time distributions in\nwiener diffusion models. Journal of Mathematical Psychology,\n56(6), 470–475. https://doi.org/https://doi.org/10.1016/j.jmp.2012.09.002\n\n\nBroadbent, D. E. (1957). A mechanical model for human attention and\nimmediate memory. Psychological Review, 64(3),\n205–215.\n\n\nBrowne, M. W. (2000). Cross-validation methods. Journal of\nMathematical Psychology, 44(1), 108–132. https://doi.org/10.1006/jmps.1999.1279\n\n\nBusemeyer, J. R., & Townsend, J. T. (1993). Decision field theory: A\ndynamic–cognitive approach to decision making in an uncertain\nenvironment. Psychological Review, 100(3), 432–459.\n\n\nBusemeyer, J. R., & Wang, Y.-M. (2000). Model comparisons and model\nselections based on generalization criterion methodology. Journal of\nMathematical Psychology, 44(1), 171–189. https://doi.org/10.1006/jmps.1999.1282\n\n\nCox, G. E., & Shiffrin, R. M. (2024). Computational models of event\nmemory. In M. J. Kahana & A. Wagner (Eds.), Oxford handbook of\nhuman memory. Oxford University Press.\n\n\nDennett, D. (1980). The milk of human intentionality. Behavioral and\nBrain Sciences, 3(3), 428–430. https://doi.org/10.1017/s0140525x0000580x\n\n\nDiederich, A. (1997). Dynamic stochastic models for decision making\nunder time constraints. Journal of Mathematical Psychology,\n41(3), 260–274. https://doi.org/10.1006/jmps.1997.1167\n\n\nGelman, A., Hwang, J., & Vehtari, A. (2014). Understanding\npredictive information criteria for bayesian models. Statistics and\nComputing, 24(6), 997–1016. https://doi.org/10.1007/s11222-013-9416-2\n\n\nGillespie, N. F., & Cox, G. E. (2024). Perception and memory for\nnovel auditory stimuli: Similarity, serial position, and list\nhomogeneity. PsyArXiv. https://doi.org/10.31234/osf.io/n294a\n\n\nGondan, M., Blurton, S. P., & Kesselmeier, M. (2014). Even faster\nand even more accurate first-passage time densities and distributions\nfor the wiener diffusion model. Journal of Mathematical\nPsychology, 60, 20–22. https://doi.org/https://doi.org/10.1016/j.jmp.2014.05.002\n\n\nHartmann, R., & Klauer, K. C. (2021). Partial derivatives for the\nfirst-passage time distribution in wiener diffusion models. Journal\nof Mathematical Psychology, 103, 102550.\nhttps://doi.org/https://doi.org/10.1016/j.jmp.2021.102550\n\n\nHartmann, R., & Klauer, K. C. (2023). WienR: Derivatives of the\nfirst-passage time density and cumulative distribution function, and\nrandom sampling from the (truncated) first-passage time\ndistribution. https://CRAN.R-project.org/package=WienR\n\n\nNavarro, D. J. (2018). Between the devil and the deep blue sea: Tensions\nbetween scientific judgement and statistical model selection.\nComputational Brain & Behavior. https://doi.org/10.1007/s42113-018-0019-z\n\n\nNavarro, D. J., & Fuss, I. G. (2009). Fast and accurate calculations\nfor first-passage times in wiener diffusion models. Journal of\nMathematical Psychology, 53(4), 222–230.\nhttps://doi.org/https://doi.org/10.1016/j.jmp.2009.02.003\n\n\nNeisser, U. (1967). Cognitive psychology.\nAppleton-Century-Crofts.\n\n\nNosofsky, R. M. (1986). Attention, similarity, and the\nidentification-categorization relationship. Journal of Experimental\nPsychology: General, 115(1), 39–57.\n\n\nNosofsky, R. M. (1992). Similarity scaling and cognitive process models.\nAnnual Review of Psychology, 43, 25–53.\n\n\nNosofsky, R. M., Cox, G. E., Cao, R., & Shiffrin, R. M. (2014). An\nexemplar-familiarity model predicts short-term and long-term probe\nrecognition across diverse forms of memory search. Journal of\nExperimental Psychology: Learning, Memory, and Cognition,\n40(6), 1524–1539.\n\n\nNosofsky, R. M., Little, D. R., Donkin, C., & Fific, M. (2011).\nShort-term memory scanning viewed as exemplar-based categorization.\nPsychological Review, 118(2), 280–315.\n\n\nNosofsky, R. M., & Palmeri, T. J. (1997). An exemplar-based random\nwalk model of speeded classification. Psychological Review,\n104(2), 266–300.\n\n\nPiironen, J., & Vehtari, A. (2017). Comparison of bayesian\npredictive methods for model selection. Statistics and\nComputing, 27(3), 711–735. https://doi.org/10.1007/s11222-016-9649-y\n\n\nRae, B., Heathcote, A., Donkin, C., Averell, L., & Brown, S. (2014).\nThe hare and the tortoise: Emphasizing speed can change the evidence\nused to make decisions. Journal of Experimental Psychology:\nLearning, Memory, and Cognition, 40(5), 1226–1243.\n\n\nRaftery, A. E. (1995). Bayesian model selection in social research.\nSociological Methodology, 25, 111–163. https://doi.org/10.2307/271063\n\n\nRatcliff, R. (1978). A theory of memory retrieval. Psychological\nReview, 85(2), 59–108.\n\n\nRatcliff, R., & Rouder, J. N. (1998). Modeling response times for\ntwo-choice decisions. Psychological Science, 9(5),\n347–356.\n\n\nSchwarz, G. (1978). Estimating the dimension of a model. The Annals\nof Statistics, 6(2), 461–464.\n\n\nShepard, R. N. (1962a). The analysis of proximities:\nMultidimensional scaling with an unknown distance function.\nI. Psychometrika, 27(2), 125–140.\nhttps://doi.org/https://doi.org/10.1007/BF02289630\n\n\nShepard, R. N. (1962b). The analysis of proximities:\nMultidimensional scaling with an unknown distance function.\nII. Psychometrika, 27(3), 219–246.\nhttps://doi.org/https://doi.org/10.1007/BF02289621\n\n\nSingmann, H., Kellen, D., Cox, G. E., Chandramouli, S. H., Davis-Stober,\nC. P., Dunn, J. C., Gronau, Q. F., Kalish, M. L., McMullin, S. D.,\nNavarro, D. J., & Shiffrin, R. M. (2022). Statistics in the service\nof science: Don’t let the tail wag the dog. Computational Brain\n& Behavior.\n\n\nStone, M. (1977). An asymptotic equivalence of choice of model by\ncross-validation and Akaike’s criterion.\nJournal of the Royal Statistical Society. Series B\n(Methodological), 39(1), 44–47.\n\n\nTrueblood, J. S., Holmes, W. R., Seegmiller, A. C., Douds, J., Compton,\nM., Szentirmai, E., Woodruff, M., Huang, W., Stratton, C., &\nEichbaum, Q. (2018). The impact of speed and bias on the cognitive\nprocesses of experts and novices in medical image decision-making.\nCognitive Research: Principles and Implications, 3,\n1–14.\n\n\nTuerlincx, F. (2004). The efficient computation of the cumulative\ndistribution and probability density functions in the diffusion model.\nBehavior Research Methods, Instruments, & Computers,\n36(4), 702–716.\n\n\nVehtari, A., & Lampinen, J. (2002). Bayesian model assessment and\ncomparison using cross-validation predictive densities. Neural\nComputation, 14(10), 2439–2468. https://doi.org/10.1162/08997660260293292\n\n\nZucchini, W. (2000). An introduction to model selection. Journal of\nMathematical Psychology, 44(1), 41–61. https://doi.org/10.1006/jmps.1999.1276",
    "crumbs": [
      "References"
    ]
  }
]